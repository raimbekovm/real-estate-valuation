{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Bishkek Real Estate Price Prediction v2\n",
    "\n",
    "**Goal:** Predict apartment price per square meter ($/m²) in Bishkek, Kyrgyzstan\n",
    "\n",
    "**Key Improvements over v1:**\n",
    "- Temporal train/test split (no future leakage)\n",
    "- Proper target encoding with K-Fold CV\n",
    "- Outlier detection with IQR + domain rules\n",
    "- Feature selection (VIF + importance)\n",
    "- Ensemble model (XGB + LGBM + CatBoost)\n",
    "- Prediction intervals (quantile regression)\n",
    "- Comprehensive residual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (for Kaggle)\n",
    "!pip install -q optuna shap catboost lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import hashlib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import KFold, GroupKFold, cross_val_predict\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import optuna\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "CURRENT_YEAR = datetime.now().year\n",
    "\n",
    "print(f\"Current year: {CURRENT_YEAR}\")\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Load & Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "import sqlite3\nimport os\n\n# Load from SQLite database\nif os.path.exists('/kaggle/input/bishkek-real-estate-2025/bishkek.db'):\n    # Kaggle path\n    db_path = '/kaggle/input/bishkek-real-estate-2025/bishkek.db'\nelif os.path.exists('../data/databases/bishkek.db'):\n    # Local path\n    db_path = '../data/databases/bishkek.db'\nelse:\n    raise FileNotFoundError(\"Database not found!\")\n\nconn = sqlite3.connect(db_path)\n\ndf_raw = pd.read_sql('''\n    SELECT \n        a.*,\n        rc.name as jk_name,\n        rc.class as jk_class,\n        rc.status as jk_status,\n        rc.developer_name\n    FROM apartments a\n    LEFT JOIN residential_complexes rc ON a.residential_complex_id = rc.id\n    WHERE a.price_usd IS NOT NULL \n      AND a.area IS NOT NULL\n      AND a.price_per_m2 > 0\n''', conn)\nconn.close()\n\nprint(f\"Database: {db_path}\")\nprint(f\"Raw dataset: {len(df_raw)} rows\")\nprint(f\"Columns: {len(df_raw.columns)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# 1.1 Remove Duplicates\n",
    "# ===================\n",
    "# Same apartment can be listed multiple times with different prices\n",
    "# Keep the most recent listing\n",
    "\n",
    "df_raw['parsed_at'] = pd.to_datetime(df_raw['parsed_at'])\n",
    "\n",
    "# Create building signature for duplicate detection\n",
    "df_raw['building_signature'] = (\n",
    "    df_raw['address'].fillna('').str.lower() + '_' +\n",
    "    df_raw['floor'].fillna(0).astype(str) + '_' +\n",
    "    df_raw['area'].fillna(0).astype(str) + '_' +\n",
    "    df_raw['rooms'].fillna(0).astype(str)\n",
    ")\n",
    "\n",
    "# Sort by date and keep last (most recent)\n",
    "df_raw = df_raw.sort_values('parsed_at')\n",
    "duplicates_before = len(df_raw)\n",
    "df_raw = df_raw.drop_duplicates(subset=['building_signature'], keep='last')\n",
    "duplicates_removed = duplicates_before - len(df_raw)\n",
    "\n",
    "print(f\"Duplicates removed: {duplicates_removed}\")\n",
    "print(f\"Dataset after dedup: {len(df_raw)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# 1.2 Outlier Detection (IQR + Domain Rules)\n",
    "# ===================\n",
    "\n",
    "target = 'price_per_m2'\n",
    "\n",
    "def detect_outliers_iqr(series: pd.Series, k: float = 1.5) -> pd.Series:\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - k * IQR\n",
    "    upper = Q3 + k * IQR\n",
    "    return (series >= lower) & (series <= upper)\n",
    "\n",
    "# IQR-based outliers for price\n",
    "price_mask = detect_outliers_iqr(df_raw[target], k=2.0)\n",
    "\n",
    "# Domain-based rules\n",
    "domain_mask = (\n",
    "    (df_raw[target] >= 300) &      # Min $300/m² (very cheap even for Bishkek)\n",
    "    (df_raw[target] <= 5000) &     # Max $5000/m² (luxury segment)\n",
    "    (df_raw['area'] >= 15) &       # Min 15 m² (studio)\n",
    "    (df_raw['area'] <= 500) &      # Max 500 m² (mansion)\n",
    "    (df_raw['rooms'].fillna(1) <= 10) &  # Max 10 rooms\n",
    "    (df_raw['floor'].fillna(1) <= 50)    # Max 50 floors\n",
    ")\n",
    "\n",
    "# Combine masks\n",
    "valid_mask = price_mask & domain_mask\n",
    "outliers_removed = (~valid_mask).sum()\n",
    "\n",
    "print(f\"Outliers detected: {outliers_removed}\")\n",
    "print(f\"  - IQR price outliers: {(~price_mask).sum()}\")\n",
    "print(f\"  - Domain rule violations: {(~domain_mask).sum()}\")\n",
    "\n",
    "df = df_raw[valid_mask].copy()\n",
    "print(f\"\\nFinal dataset: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# 1.3 Data Quality Report\n",
    "# ===================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA QUALITY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nTarget variable ({target}):\")\n",
    "print(f\"  Mean:   ${df[target].mean():,.0f}/m²\")\n",
    "print(f\"  Median: ${df[target].median():,.0f}/m²\")\n",
    "print(f\"  Std:    ${df[target].std():,.0f}/m²\")\n",
    "print(f\"  Range:  ${df[target].min():,.0f} - ${df[target].max():,.0f}/m²\")\n",
    "\n",
    "print(f\"\\nMissing values (key columns):\")\n",
    "key_cols = ['rooms', 'area', 'floor', 'total_floors', 'year_built', \n",
    "            'latitude', 'longitude', 'district', 'jk_name', 'condition']\n",
    "for col in key_cols:\n",
    "    if col in df.columns:\n",
    "        missing = df[col].isna().sum()\n",
    "        pct = missing / len(df) * 100\n",
    "        print(f\"  {col:20s}: {missing:5d} ({pct:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nResidential complex coverage:\")\n",
    "print(f\"  With JK: {df['jk_name'].notna().sum()} ({df['jk_name'].notna().mean()*100:.1f}%)\")\n",
    "print(f\"  Unique JK: {df['jk_name'].nunique()}\")\n",
    "\n",
    "print(f\"\\nTemporal range:\")\n",
    "print(f\"  From: {df['parsed_at'].min()}\")\n",
    "print(f\"  To:   {df['parsed_at'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].hist(df[target], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(df[target].median(), color='red', linestyle='--', label=f'Median: ${df[target].median():,.0f}')\n",
    "axes[0].set_xlabel('Price per m² (USD)')\n",
    "axes[0].set_title('Price Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(np.log1p(df[target]), bins=50, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[1].set_xlabel('Log(Price per m²)')\n",
    "axes[1].set_title('Log-transformed (more normal)')\n",
    "\n",
    "# Box plot by district\n",
    "top_districts = df['district'].value_counts().head(8).index\n",
    "df_plot = df[df['district'].isin(top_districts)]\n",
    "df_plot.boxplot(column=target, by='district', ax=axes[2], rot=45)\n",
    "axes[2].set_title('Price by District')\n",
    "axes[2].set_xlabel('')\n",
    "plt.suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 2. Temporal Train/Test Split\n",
    "\n",
    "**Critical:** We split by time to avoid future leakage. The model should only see past data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# TEMPORAL SPLIT (not random!)\n",
    "# ===================\n",
    "\n",
    "# Sort by date\n",
    "df = df.sort_values('parsed_at').reset_index(drop=True)\n",
    "\n",
    "# Use last 20% by time as test set\n",
    "split_idx = int(len(df) * 0.8)\n",
    "split_date = df.iloc[split_idx]['parsed_at']\n",
    "\n",
    "train_df = df.iloc[:split_idx].copy()\n",
    "test_df = df.iloc[split_idx:].copy()\n",
    "\n",
    "print(f\"Temporal Split:\")\n",
    "print(f\"  Train: {len(train_df)} samples ({df['parsed_at'].min().date()} to {split_date.date()})\")\n",
    "print(f\"  Test:  {len(test_df)} samples ({split_date.date()} to {df['parsed_at'].max().date()})\")\n",
    "print(f\"\\nThis ensures no future data leakage!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "**Important:** All statistics (median, encodings) are computed on TRAIN set only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# POI (Points of Interest) - Bishkek coordinates (verified by user)\n",
    "# ===================\n",
    "\n",
    "POI = {\n",
    "    # Shopping malls\n",
    "    'dordoi_plaza': (42.8750, 74.6128),\n",
    "    'bishkek_park': (42.8741, 74.5888),\n",
    "    'tsum': (42.8746, 74.6031),\n",
    "    'vefa_center': (42.8668, 74.5931),\n",
    "    'asia_mall': (42.8489, 74.5672),\n",
    "    'karavan': (42.8562, 74.5686),\n",
    "    \n",
    "    # Key landmarks\n",
    "    'ala_too_square': (42.8746, 74.6030),\n",
    "    'philharmonic': (42.8749, 74.6108),\n",
    "    'white_house': (42.8760, 74.6097),\n",
    "    'victory_square': (42.8722, 74.5875),\n",
    "    \n",
    "    # Universities\n",
    "    'knu': (42.8778, 74.6027),\n",
    "    'auca': (42.8634, 74.6167),\n",
    "    'krsu': (42.8750, 74.5861),\n",
    "    \n",
    "    # Transport\n",
    "    'west_bus_station': (42.8628, 74.5294),\n",
    "    'east_bus_station': (42.8605, 74.6550),\n",
    "    'railway_station': (42.8588, 74.6339),\n",
    "    \n",
    "    # Markets\n",
    "    'osh_bazaar': (42.8722, 74.5761),\n",
    "    'dordoi_bazaar': (42.9453, 74.6494),\n",
    "    'ortosay_bazaar': (42.8478, 74.5542),\n",
    "    \n",
    "    # City center\n",
    "    'center': (42.8746, 74.5888),\n",
    "}\n",
    "\n",
    "PARKS = {\n",
    "    'dubovy_park': [(42.8749, 74.5875), (42.8780, 74.5875), (42.8780, 74.5930), (42.8749, 74.5930)],\n",
    "    'park_panfilova': [(42.8740, 74.6000), (42.8760, 74.6000), (42.8760, 74.6050), (42.8740, 74.6050)],\n",
    "    'park_ataturk': [(42.8690, 74.5950), (42.8720, 74.5950), (42.8720, 74.6000), (42.8690, 74.6000)],\n",
    "    'botanical_garden': [(42.8560, 74.5560), (42.8620, 74.5560), (42.8620, 74.5660), (42.8560, 74.5660)],\n",
    "}\n",
    "\n",
    "ALA_ARCHA_RIVER = [\n",
    "    (42.7800, 74.5700), (42.8100, 74.5650), (42.8400, 74.5600),\n",
    "    (42.8600, 74.5580), (42.8800, 74.5560), (42.9000, 74.5550),\n",
    "]\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate distance between two points in km\"\"\"\n",
    "    R = 6371\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat, dlon = lat2 - lat1, lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    return R * 2 * np.arcsin(np.sqrt(a))\n",
    "\n",
    "def distance_to_polyline(lat, lon, polyline):\n",
    "    \"\"\"Minimum distance to polyline\"\"\"\n",
    "    distances = [haversine_distance(lat, lon, p[0], p[1]) for p in polyline]\n",
    "    return min(distances)\n",
    "\n",
    "print(f\"POI: {len(POI)}, Parks: {len(PARKS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "    \"\"\"\n",
    "    Feature engineering with proper train/test separation.\n",
    "    All statistics are computed on train set only.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fitted = False\n",
    "        self.medians = {}\n",
    "        self.target_encodings = {}\n",
    "        \n",
    "    def fit(self, df: pd.DataFrame, target_col: str):\n",
    "        \"\"\"Compute all statistics from training data\"\"\"\n",
    "        # Store medians for imputation\n",
    "        self.medians = {\n",
    "            'floor': df['floor'].median(),\n",
    "            'total_floors': df['total_floors'].median(),\n",
    "            'year_built': df['year_built'].median(),\n",
    "            'rooms': df['rooms'].median(),\n",
    "            'kitchen_ratio': (df['kitchen_area'] / df['area']).median(),\n",
    "            'ceiling_height': self._parse_ceiling_series(df['ceiling_height']).median(),\n",
    "            'latitude': df['latitude'].median(),\n",
    "            'longitude': df['longitude'].median(),\n",
    "        }\n",
    "        \n",
    "        # Target encoding will be done separately with CV\n",
    "        self.global_target_mean = df[target_col].mean()\n",
    "        self.fitted = True\n",
    "        return self\n",
    "    \n",
    "    def _parse_ceiling_series(self, series: pd.Series) -> pd.Series:\n",
    "        \"\"\"Parse ceiling height values\"\"\"\n",
    "        def parse_one(val):\n",
    "            if pd.isna(val): return np.nan\n",
    "            val = str(val).replace('м', '').replace(',', '.').strip()\n",
    "            try: return float(val)\n",
    "            except: return np.nan\n",
    "        return series.apply(parse_one)\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Apply feature engineering using fitted statistics\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Must call fit() first!\")\n",
    "        \n",
    "        df = df.copy()\n",
    "        \n",
    "        # === Impute missing values with TRAIN medians ===\n",
    "        df['floor'] = df['floor'].fillna(self.medians['floor'])\n",
    "        df['total_floors'] = df['total_floors'].fillna(self.medians['total_floors'])\n",
    "        df['year_built'] = df['year_built'].fillna(self.medians['year_built'])\n",
    "        df['rooms'] = df['rooms'].fillna(self.medians['rooms'])\n",
    "        df['latitude'] = df['latitude'].fillna(self.medians['latitude'])\n",
    "        df['longitude'] = df['longitude'].fillna(self.medians['longitude'])\n",
    "        \n",
    "        # === Floor features ===\n",
    "        df['floor_ratio'] = df['floor'] / df['total_floors'].replace(0, 1)\n",
    "        df['is_first_floor'] = (df['floor'] == 1).astype(int)\n",
    "        df['is_last_floor'] = (df['floor'] == df['total_floors']).astype(int)\n",
    "        \n",
    "        # === Building features ===\n",
    "        df['building_age'] = CURRENT_YEAR - df['year_built']\n",
    "        df['is_new_building'] = (df['year_built'] >= CURRENT_YEAR - 7).astype(int)\n",
    "        df['is_soviet'] = (df['year_built'] < 1991).astype(int)\n",
    "        df['is_highrise'] = (df['total_floors'] >= 9).astype(int)\n",
    "        \n",
    "        # === Area features ===\n",
    "        df['area_per_room'] = df['area'] / df['rooms'].replace(0, 1)\n",
    "        df['is_large'] = (df['area'] >= 100).astype(int)\n",
    "        df['is_studio'] = (df['rooms'] <= 1).astype(int)\n",
    "        \n",
    "        # Kitchen\n",
    "        df['kitchen_area_num'] = pd.to_numeric(df['kitchen_area'], errors='coerce')\n",
    "        df['kitchen_ratio'] = (df['kitchen_area_num'] / df['area']).clip(0, 0.5)\n",
    "        df['kitchen_missing'] = df['kitchen_ratio'].isna().astype(int)\n",
    "        df['kitchen_ratio'] = df['kitchen_ratio'].fillna(self.medians['kitchen_ratio'])\n",
    "        \n",
    "        # Ceiling\n",
    "        df['ceiling_height_m'] = self._parse_ceiling_series(df['ceiling_height'])\n",
    "        df['ceiling_missing'] = df['ceiling_height_m'].isna().astype(int)\n",
    "        df['ceiling_height_m'] = df['ceiling_height_m'].fillna(self.medians['ceiling_height'])\n",
    "        df['high_ceiling'] = (df['ceiling_height_m'] >= 3.0).astype(int)\n",
    "        \n",
    "        # === Condition ===\n",
    "        condition_map = {'евроремонт': 4, 'хороший': 3, 'средний': 2, \n",
    "                        'черновая отделка': 1, 'требует ремонта': 0}\n",
    "        df['condition_score'] = df['condition'].map(condition_map).fillna(2)\n",
    "        df['condition_missing'] = (~df['condition'].isin(condition_map.keys())).astype(int)\n",
    "        \n",
    "        # === Categorical binary features ===\n",
    "        df['has_separate_bathroom'] = df['bathroom'].str.contains('раздельн', na=False).astype(int)\n",
    "        df['has_balcony'] = df['balcony'].notna().astype(int)\n",
    "        df['has_parking'] = df['parking'].notna().astype(int)\n",
    "        df['has_furniture'] = df['furniture'].notna().astype(int)\n",
    "        df['is_parquet'] = df['floor_type'].str.contains('паркет', na=False).astype(int)\n",
    "        \n",
    "        # Security score\n",
    "        df['security_score'] = (\n",
    "            df['security'].str.contains('охран', na=False).astype(int) * 2 +\n",
    "            df['security'].str.contains('видео', na=False).astype(int) +\n",
    "            df['security'].str.contains('домофон', na=False).astype(int)\n",
    "        )\n",
    "        \n",
    "        # House type\n",
    "        df['is_monolith'] = (df['house_type'] == 'монолитный').astype(int)\n",
    "        df['is_brick'] = (df['house_type'] == 'кирпичный').astype(int)\n",
    "        df['is_panel'] = (df['house_type'] == 'панельный').astype(int)\n",
    "        \n",
    "        # === JK features ===\n",
    "        df['has_jk'] = df['jk_name'].notna().astype(int)\n",
    "        jk_class_map = {'эконом': 1, 'комфорт': 2, 'бизнес': 3, 'премиум': 4, 'элит': 4}\n",
    "        df['jk_class_score'] = df['jk_class'].map(jk_class_map).fillna(0)\n",
    "        df['jk_completed'] = (df['jk_status'] == 'completed').astype(int)\n",
    "        \n",
    "        # === Geo features ===\n",
    "        lats = df['latitude'].values\n",
    "        lons = df['longitude'].values\n",
    "        \n",
    "        # POI distances\n",
    "        for poi_name, (poi_lat, poi_lon) in POI.items():\n",
    "            df[f'dist_{poi_name}'] = haversine_distance(lats, lons, poi_lat, poi_lon)\n",
    "        \n",
    "        # Aggregated distances\n",
    "        mall_cols = [f'dist_{p}' for p in ['dordoi_plaza', 'bishkek_park', 'tsum', 'vefa_center', 'asia_mall', 'karavan']]\n",
    "        df['dist_nearest_mall'] = df[mall_cols].min(axis=1)\n",
    "        \n",
    "        transport_cols = [f'dist_{p}' for p in ['west_bus_station', 'east_bus_station', 'railway_station']]\n",
    "        df['dist_nearest_transport'] = df[transport_cols].min(axis=1)\n",
    "        \n",
    "        bazaar_cols = [f'dist_{p}' for p in ['osh_bazaar', 'dordoi_bazaar', 'ortosay_bazaar']]\n",
    "        df['dist_nearest_bazaar'] = df[bazaar_cols].min(axis=1)\n",
    "        \n",
    "        # Parks\n",
    "        for park_name, polygon in PARKS.items():\n",
    "            centroid = (np.mean([p[0] for p in polygon]), np.mean([p[1] for p in polygon]))\n",
    "            df[f'dist_{park_name}'] = haversine_distance(lats, lons, centroid[0], centroid[1])\n",
    "        \n",
    "        park_cols = [f'dist_{p}' for p in PARKS.keys()]\n",
    "        df['dist_nearest_park'] = df[park_cols].min(axis=1)\n",
    "        \n",
    "        # River distance\n",
    "        df['dist_river'] = df.apply(\n",
    "            lambda r: distance_to_polyline(r['latitude'], r['longitude'], ALA_ARCHA_RIVER), axis=1\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def fit_transform(self, df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n",
    "        return self.fit(df, target_col).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# Target Encoding with K-Fold CV (no leakage)\n",
    "# ===================\n",
    "\n",
    "class TargetEncoderCV:\n",
    "    \"\"\"\n",
    "    Target encoding with K-Fold cross-validation to prevent leakage.\n",
    "    For training data: use out-of-fold predictions\n",
    "    For test data: use full training set statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cols: List[str], min_samples: int = 30, n_folds: int = 5):\n",
    "        self.cols = cols\n",
    "        self.min_samples = min_samples\n",
    "        self.n_folds = n_folds\n",
    "        self.encodings = {}\n",
    "        self.global_mean = None\n",
    "        \n",
    "    def fit_transform(self, df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n",
    "        \"\"\"Fit on training data and transform with CV (no leakage)\"\"\"\n",
    "        df = df.copy()\n",
    "        self.global_mean = df[target_col].mean()\n",
    "        \n",
    "        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "        \n",
    "        for col in self.cols:\n",
    "            new_col = f'{col}_target_enc'\n",
    "            df[new_col] = np.nan\n",
    "            \n",
    "            # Fill with out-of-fold encoding\n",
    "            for train_idx, val_idx in kf.split(df):\n",
    "                train_fold = df.iloc[train_idx]\n",
    "                val_fold = df.iloc[val_idx]\n",
    "                \n",
    "                # Compute encoding from train fold\n",
    "                encoding = self._compute_encoding(train_fold, col, target_col)\n",
    "                \n",
    "                # Apply to validation fold\n",
    "                df.iloc[val_idx, df.columns.get_loc(new_col)] = (\n",
    "                    val_fold[col].map(encoding).fillna(self.global_mean)\n",
    "                )\n",
    "            \n",
    "            # Store full encoding for test set\n",
    "            self.encodings[col] = self._compute_encoding(df, col, target_col)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Transform test data using full training encodings\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        for col in self.cols:\n",
    "            new_col = f'{col}_target_enc'\n",
    "            df[new_col] = df[col].map(self.encodings[col]).fillna(self.global_mean)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _compute_encoding(self, df: pd.DataFrame, col: str, target_col: str) -> Dict:\n",
    "        \"\"\"Compute smoothed target encoding\"\"\"\n",
    "        agg = df.groupby(col)[target_col].agg(['mean', 'count'])\n",
    "        \n",
    "        # Smoothing formula: more samples = more trust in category mean\n",
    "        smoothing = agg['count'] / (agg['count'] + self.min_samples)\n",
    "        encoding = smoothing * agg['mean'] + (1 - smoothing) * self.global_mean\n",
    "        \n",
    "        return encoding.to_dict()\n",
    "\n",
    "print(\"Target Encoder with CV defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# Apply Feature Engineering\n",
    "# ===================\n",
    "\n",
    "# 1. Basic features (fit on train only)\n",
    "feature_engineer = FeatureEngineer()\n",
    "train_df = feature_engineer.fit_transform(train_df, target)\n",
    "test_df = feature_engineer.transform(test_df)\n",
    "\n",
    "print(f\"Basic features created: {train_df.shape[1]} columns\")\n",
    "\n",
    "# 2. Target encoding (with CV on train, direct on test)\n",
    "target_encoder = TargetEncoderCV(\n",
    "    cols=['district', 'jk_name'], \n",
    "    min_samples=30,  # Higher threshold for better smoothing\n",
    "    n_folds=5\n",
    ")\n",
    "train_df = target_encoder.fit_transform(train_df, target)\n",
    "test_df = target_encoder.transform(test_df)\n",
    "\n",
    "print(f\"Target encoding applied\")\n",
    "print(f\"Final train shape: {train_df.shape}\")\n",
    "print(f\"Final test shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 4. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# Define Feature Groups\n",
    "# ===================\n",
    "\n",
    "# Core features (no multicollinearity)\n",
    "FEATURE_GROUPS = {\n",
    "    'core': ['rooms', 'area', 'floor_ratio', 'building_age'],\n",
    "    \n",
    "    'building': ['is_new_building', 'is_soviet', 'is_highrise', 'total_floors'],\n",
    "    \n",
    "    'apartment': [\n",
    "        'area_per_room', 'is_large', 'is_studio',\n",
    "        'kitchen_ratio', 'kitchen_missing',\n",
    "        'ceiling_height_m', 'ceiling_missing', 'high_ceiling',\n",
    "        'condition_score', 'condition_missing',\n",
    "    ],\n",
    "    \n",
    "    'amenities': [\n",
    "        'has_separate_bathroom', 'has_balcony', 'has_parking',\n",
    "        'has_furniture', 'is_parquet', 'security_score',\n",
    "    ],\n",
    "    \n",
    "    'house_type': ['is_monolith', 'is_brick', 'is_panel'],\n",
    "    \n",
    "    'jk': ['has_jk', 'jk_class_score', 'jk_completed'],\n",
    "    \n",
    "    'target_encoding': ['district_target_enc', 'jk_name_target_enc'],\n",
    "    \n",
    "    'location': [\n",
    "        'dist_center', 'dist_nearest_mall', 'dist_nearest_transport',\n",
    "        'dist_nearest_bazaar', 'dist_nearest_park', 'dist_river',\n",
    "        'latitude', 'longitude',\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Flatten to get all features\n",
    "ALL_FEATURES = [f for group in FEATURE_GROUPS.values() for f in group]\n",
    "\n",
    "# Check availability\n",
    "available_features = [f for f in ALL_FEATURES if f in train_df.columns]\n",
    "missing_features = [f for f in ALL_FEATURES if f not in train_df.columns]\n",
    "\n",
    "print(f\"Features defined: {len(ALL_FEATURES)}\")\n",
    "print(f\"Features available: {len(available_features)}\")\n",
    "if missing_features:\n",
    "    print(f\"Missing: {missing_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# Check Multicollinearity (VIF)\n",
    "# ===================\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calculate_vif(df: pd.DataFrame, features: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Calculate Variance Inflation Factor for features\"\"\"\n",
    "    X = df[features].fillna(0).values\n",
    "    vif_data = []\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        try:\n",
    "            vif = variance_inflation_factor(X, i)\n",
    "            vif_data.append({'feature': feature, 'VIF': vif})\n",
    "        except:\n",
    "            vif_data.append({'feature': feature, 'VIF': np.inf})\n",
    "    \n",
    "    return pd.DataFrame(vif_data).sort_values('VIF', ascending=False)\n",
    "\n",
    "# Check VIF for numeric features\n",
    "numeric_features = [f for f in available_features if train_df[f].dtype in ['float64', 'int64']]\n",
    "vif_df = calculate_vif(train_df, numeric_features[:20])  # Check top 20\n",
    "\n",
    "print(\"Top features by VIF (>5 indicates multicollinearity):\")\n",
    "print(vif_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# Remove High VIF Features\n",
    "# ===================\n",
    "\n",
    "# Remove features with VIF > 10 (keeping the most important variant)\n",
    "high_vif_features = vif_df[vif_df['VIF'] > 10]['feature'].tolist()\n",
    "\n",
    "# Manual selection: keep the most informative from correlated pairs\n",
    "features_to_remove = []\n",
    "# year_built and building_age are correlated - keep building_age\n",
    "if 'year_built' in available_features:\n",
    "    features_to_remove.append('year_built')\n",
    "# floor and floor_ratio - keep floor_ratio\n",
    "if 'floor' in available_features and 'floor_ratio' in available_features:\n",
    "    features_to_remove.append('floor')\n",
    "\n",
    "# Final feature list\n",
    "final_features = [f for f in available_features if f not in features_to_remove]\n",
    "\n",
    "print(f\"Features removed due to multicollinearity: {features_to_remove}\")\n",
    "print(f\"Final feature count: {len(final_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# Prepare Final Data\n",
    "# ===================\n",
    "\n",
    "X_train = train_df[final_features].fillna(0).values\n",
    "X_test = test_df[final_features].fillna(0).values\n",
    "y_train = train_df[target].values\n",
    "y_test = test_df[target].values\n",
    "\n",
    "# Create groups for CV (by building location)\n",
    "# Hash of (lat, lon) rounded to create spatial groups\n",
    "def create_spatial_groups(df: pd.DataFrame, precision: int = 3) -> np.ndarray:\n",
    "    \"\"\"Create groups based on spatial proximity\"\"\"\n",
    "    lat_round = df['latitude'].round(precision).astype(str)\n",
    "    lon_round = df['longitude'].round(precision).astype(str)\n",
    "    group_str = lat_round + '_' + lon_round\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    return le.fit_transform(group_str)\n",
    "\n",
    "train_groups = create_spatial_groups(train_df)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"Spatial groups for CV: {len(np.unique(train_groups))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 5. Model Training & Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# Evaluation Metrics\n",
    "# ===================\n",
    "\n",
    "def evaluate_predictions(y_true: np.ndarray, y_pred: np.ndarray, name: str = \"\") -> Dict:\n",
    "    \"\"\"Comprehensive evaluation metrics\"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Median Absolute Error (robust to outliers)\n",
    "    medae = np.median(np.abs(y_true - y_pred))\n",
    "    \n",
    "    # Median Absolute Percentage Error (better than MAPE)\n",
    "    mape = np.median(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    # Percentage within 10% error\n",
    "    within_10pct = np.mean(np.abs((y_true - y_pred) / y_true) <= 0.10) * 100\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R²': r2,\n",
    "        'MedAE': medae,\n",
    "        'MedAPE%': mape,\n",
    "        'Within10%': within_10pct,\n",
    "    }\n",
    "\n",
    "def print_metrics(metrics: Dict):\n",
    "    print(f\"  MAE:       ${metrics['MAE']:,.0f}/m²\")\n",
    "    print(f\"  RMSE:      ${metrics['RMSE']:,.0f}/m²\")\n",
    "    print(f\"  R²:        {metrics['R²']:.4f}\")\n",
    "    print(f\"  MedAE:     ${metrics['MedAE']:,.0f}/m²\")\n",
    "    print(f\"  MedAPE:    {metrics['MedAPE%']:.1f}%\")\n",
    "    print(f\"  Within 10%: {metrics['Within10%']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# Train Base Models\n",
    "# ===================\n",
    "\n",
    "print(\"Training base models...\\n\")\n",
    "\n",
    "base_models = {\n",
    "    'XGBoost': XGBRegressor(\n",
    "        n_estimators=300, max_depth=8, learning_rate=0.05,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        random_state=RANDOM_STATE, n_jobs=-1\n",
    "    ),\n",
    "    'LightGBM': LGBMRegressor(\n",
    "        n_estimators=300, max_depth=8, learning_rate=0.05,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        random_state=RANDOM_STATE, verbose=-1, n_jobs=-1\n",
    "    ),\n",
    "    'CatBoost': CatBoostRegressor(\n",
    "        n_estimators=300, max_depth=8, learning_rate=0.05,\n",
    "        random_state=RANDOM_STATE, verbose=0\n",
    "    ),\n",
    "}\n",
    "\n",
    "results = []\n",
    "trained_models = {}\n",
    "oof_predictions = {}  # Out-of-fold predictions for stacking\n",
    "\n",
    "for name, model in base_models.items():\n",
    "    print(f\"{name}:\")\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # OOF predictions using GroupKFold\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    oof = cross_val_predict(model, X_train, y_train, cv=gkf, groups=train_groups, n_jobs=-1)\n",
    "    oof_predictions[name] = oof\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = evaluate_predictions(y_test, y_pred_test, name)\n",
    "    results.append(metrics)\n",
    "    print_metrics(metrics)\n",
    "    print()\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('MAE')\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASE MODELS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# Stacking Ensemble\n",
    "# ===================\n",
    "\n",
    "print(\"\\nTraining Stacking Ensemble...\")\n",
    "\n",
    "# Prepare stacking features (OOF predictions)\n",
    "stack_train = np.column_stack([oof_predictions[name] for name in base_models.keys()])\n",
    "stack_test = np.column_stack([trained_models[name].predict(X_test) for name in base_models.keys()])\n",
    "\n",
    "# Meta-learner (Ridge regression - simple and effective)\n",
    "meta_model = Ridge(alpha=1.0)\n",
    "meta_model.fit(stack_train, y_train)\n",
    "\n",
    "# Ensemble prediction\n",
    "y_pred_ensemble = meta_model.predict(stack_test)\n",
    "\n",
    "# Evaluate ensemble\n",
    "ensemble_metrics = evaluate_predictions(y_test, y_pred_ensemble, 'Stacking Ensemble')\n",
    "print(\"\\nStacking Ensemble Results:\")\n",
    "print_metrics(ensemble_metrics)\n",
    "\n",
    "# Print weights\n",
    "print(f\"\\nMeta-learner weights:\")\n",
    "for name, weight in zip(base_models.keys(), meta_model.coef_):\n",
    "    print(f\"  {name}: {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# Simple Average Ensemble (often works better)\n",
    "# ===================\n",
    "\n",
    "y_pred_avg = np.mean([\n",
    "    trained_models[name].predict(X_test) for name in base_models.keys()\n",
    "], axis=0)\n",
    "\n",
    "avg_metrics = evaluate_predictions(y_test, y_pred_avg, 'Average Ensemble')\n",
    "print(\"\\nSimple Average Ensemble Results:\")\n",
    "print_metrics(avg_metrics)\n",
    "\n",
    "# Choose best ensemble\n",
    "if avg_metrics['MAE'] < ensemble_metrics['MAE']:\n",
    "    print(\"\\n>>> Using Simple Average (better performance)\")\n",
    "    final_predictions = y_pred_avg\n",
    "    final_method = 'average'\n",
    "else:\n",
    "    print(\"\\n>>> Using Stacking Ensemble (better performance)\")\n",
    "    final_predictions = y_pred_ensemble\n",
    "    final_method = 'stacking'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 6. Prediction Intervals (Quantile Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# Train Quantile Models for Prediction Intervals\n",
    "# ===================\n",
    "\n",
    "print(\"Training quantile models for prediction intervals...\")\n",
    "\n",
    "# LightGBM supports quantile regression natively\n",
    "quantile_models = {}\n",
    "\n",
    "for q, name in [(0.10, 'q10'), (0.50, 'q50'), (0.90, 'q90')]:\n",
    "    model = LGBMRegressor(\n",
    "        objective='quantile',\n",
    "        alpha=q,\n",
    "        n_estimators=200,\n",
    "        max_depth=8,\n",
    "        learning_rate=0.05,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    quantile_models[name] = model\n",
    "    print(f\"  {name} trained\")\n",
    "\n",
    "# Get prediction intervals\n",
    "pred_q10 = quantile_models['q10'].predict(X_test)\n",
    "pred_q50 = quantile_models['q50'].predict(X_test)\n",
    "pred_q90 = quantile_models['q90'].predict(X_test)\n",
    "\n",
    "# Check coverage (should be ~80% for 10-90 interval)\n",
    "coverage = np.mean((y_test >= pred_q10) & (y_test <= pred_q90)) * 100\n",
    "avg_interval_width = np.mean(pred_q90 - pred_q10)\n",
    "\n",
    "print(f\"\\nPrediction Intervals (80% CI):\")\n",
    "print(f\"  Coverage: {coverage:.1f}% (target: 80%)\")\n",
    "print(f\"  Average interval width: ${avg_interval_width:,.0f}/m²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## 7. Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# Comprehensive Residual Analysis\n",
    "# ===================\n",
    "\n",
    "residuals = y_test - final_predictions\n",
    "std_residuals = (residuals - residuals.mean()) / residuals.std()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# 1. Residuals vs Predicted (check heteroscedasticity)\n",
    "axes[0, 0].scatter(final_predictions, residuals, alpha=0.3, s=10)\n",
    "axes[0, 0].axhline(0, color='red', linestyle='--')\n",
    "axes[0, 0].set_xlabel('Predicted')\n",
    "axes[0, 0].set_ylabel('Residual')\n",
    "axes[0, 0].set_title('Residuals vs Predicted\\n(check for patterns)')\n",
    "\n",
    "# 2. Residuals distribution\n",
    "axes[0, 1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].axvline(0, color='red', linestyle='--')\n",
    "axes[0, 1].axvline(residuals.mean(), color='green', linestyle='--', label=f'Mean: ${residuals.mean():,.0f}')\n",
    "axes[0, 1].set_xlabel('Residual')\n",
    "axes[0, 1].set_title('Residuals Distribution')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. QQ Plot (check normality)\n",
    "from scipy import stats\n",
    "stats.probplot(std_residuals, dist=\"norm\", plot=axes[0, 2])\n",
    "axes[0, 2].set_title('Q-Q Plot\\n(should be linear if normal)')\n",
    "\n",
    "# 4. Actual vs Predicted\n",
    "axes[1, 0].scatter(y_test, final_predictions, alpha=0.3, s=10)\n",
    "axes[1, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[1, 0].set_xlabel('Actual')\n",
    "axes[1, 0].set_ylabel('Predicted')\n",
    "axes[1, 0].set_title('Actual vs Predicted')\n",
    "\n",
    "# 5. Residuals vs Actual (check for bias at different price levels)\n",
    "axes[1, 1].scatter(y_test, residuals, alpha=0.3, s=10)\n",
    "axes[1, 1].axhline(0, color='red', linestyle='--')\n",
    "axes[1, 1].set_xlabel('Actual Price')\n",
    "axes[1, 1].set_ylabel('Residual')\n",
    "axes[1, 1].set_title('Residuals vs Actual\\n(check for bias)')\n",
    "\n",
    "# 6. Prediction intervals\n",
    "sample_idx = np.random.choice(len(y_test), min(100, len(y_test)), replace=False)\n",
    "sample_idx = np.sort(sample_idx)\n",
    "\n",
    "axes[1, 2].fill_between(range(len(sample_idx)), \n",
    "                        pred_q10[sample_idx], pred_q90[sample_idx], \n",
    "                        alpha=0.3, label='80% CI')\n",
    "axes[1, 2].scatter(range(len(sample_idx)), y_test[sample_idx], s=20, c='red', label='Actual')\n",
    "axes[1, 2].plot(range(len(sample_idx)), pred_q50[sample_idx], 'b-', label='Median pred')\n",
    "axes[1, 2].set_xlabel('Sample')\n",
    "axes[1, 2].set_ylabel('Price/m²')\n",
    "axes[1, 2].set_title('Prediction Intervals (sample)')\n",
    "axes[1, 2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# Error Analysis by Segment\n",
    "# ===================\n",
    "\n",
    "test_analysis = test_df.copy()\n",
    "test_analysis['predicted'] = final_predictions\n",
    "test_analysis['residual'] = residuals\n",
    "test_analysis['abs_error'] = np.abs(residuals)\n",
    "test_analysis['pct_error'] = test_analysis['abs_error'] / test_analysis[target] * 100\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ERROR ANALYSIS BY SEGMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# By district\n",
    "print(\"\\nBy District:\")\n",
    "district_error = test_analysis.groupby('district').agg({\n",
    "    'abs_error': ['mean', 'median'],\n",
    "    'pct_error': 'median',\n",
    "    target: 'count'\n",
    "}).round(1)\n",
    "district_error.columns = ['MAE', 'MedAE', 'MedAPE%', 'Count']\n",
    "district_error = district_error.sort_values('MedAE')\n",
    "print(district_error.head(10))\n",
    "\n",
    "# By JK presence\n",
    "print(\"\\nBy Residential Complex:\")\n",
    "jk_error = test_analysis.groupby('has_jk').agg({\n",
    "    'abs_error': ['mean', 'median'],\n",
    "    'pct_error': 'median',\n",
    "    target: 'count'\n",
    "}).round(1)\n",
    "jk_error.columns = ['MAE', 'MedAE', 'MedAPE%', 'Count']\n",
    "jk_error.index = ['No JK', 'Has JK']\n",
    "print(jk_error)\n",
    "\n",
    "# By price segment\n",
    "print(\"\\nBy Price Segment:\")\n",
    "test_analysis['price_segment'] = pd.cut(\n",
    "    test_analysis[target], \n",
    "    bins=[0, 1000, 1500, 2000, 3000, 10000],\n",
    "    labels=['<$1k', '$1-1.5k', '$1.5-2k', '$2-3k', '>$3k']\n",
    ")\n",
    "segment_error = test_analysis.groupby('price_segment').agg({\n",
    "    'abs_error': ['mean', 'median'],\n",
    "    'pct_error': 'median',\n",
    "    target: 'count'\n",
    "}).round(1)\n",
    "segment_error.columns = ['MAE', 'MedAE', 'MedAPE%', 'Count']\n",
    "print(segment_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## 8. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# Permutation Importance (more reliable than built-in)\n",
    "# ===================\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "print(\"Computing permutation importance...\")\n",
    "\n",
    "# Use XGBoost for importance analysis\n",
    "perm_importance = permutation_importance(\n",
    "    trained_models['XGBoost'], X_test, y_test,\n",
    "    n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': final_features,\n",
    "    'importance_mean': perm_importance.importances_mean,\n",
    "    'importance_std': perm_importance.importances_std,\n",
    "}).sort_values('importance_mean', ascending=False)\n",
    "\n",
    "# Normalize to percentage\n",
    "importance_df['importance_pct'] = importance_df['importance_mean'] / importance_df['importance_mean'].sum() * 100\n",
    "\n",
    "print(\"\\nTop 15 Features (Permutation Importance):\")\n",
    "print(importance_df.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize importance with error bars\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "top_n = 15\n",
    "top_features = importance_df.head(top_n)\n",
    "\n",
    "y_pos = np.arange(top_n)\n",
    "ax.barh(y_pos, top_features['importance_mean'], \n",
    "        xerr=top_features['importance_std'], \n",
    "        align='center', alpha=0.8)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(top_features['feature'])\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Permutation Importance (MAE increase)')\n",
    "ax.set_title('Top 15 Feature Importances with Std Dev')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "## 9. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "\n",
    "# Save all components\n",
    "model_artifacts = {\n",
    "    'base_models': trained_models,\n",
    "    'meta_model': meta_model if final_method == 'stacking' else None,\n",
    "    'ensemble_method': final_method,\n",
    "    'quantile_models': quantile_models,\n",
    "    'feature_engineer': feature_engineer,\n",
    "    'target_encoder': target_encoder,\n",
    "    'features': final_features,\n",
    "}\n",
    "\n",
    "joblib.dump(model_artifacts, 'bishkek_model_v2.joblib')\n",
    "print(\"Model artifacts saved to: bishkek_model_v2.joblib\")\n",
    "\n",
    "# Save config\n",
    "final_metrics = evaluate_predictions(y_test, final_predictions, 'Final')\n",
    "\n",
    "config = {\n",
    "    'version': 2,\n",
    "    'features': final_features,\n",
    "    'ensemble_method': final_method,\n",
    "    'metrics': {\n",
    "        'mae': float(final_metrics['MAE']),\n",
    "        'rmse': float(final_metrics['RMSE']),\n",
    "        'r2': float(final_metrics['R²']),\n",
    "        'medae': float(final_metrics['MedAE']),\n",
    "        'medape': float(final_metrics['MedAPE%']),\n",
    "        'within_10pct': float(final_metrics['Within10%']),\n",
    "        'ci_coverage': float(coverage),\n",
    "    },\n",
    "    'data_info': {\n",
    "        'train_size': len(train_df),\n",
    "        'test_size': len(test_df),\n",
    "        'train_date_range': f\"{train_df['parsed_at'].min().date()} to {train_df['parsed_at'].max().date()}\",\n",
    "        'test_date_range': f\"{test_df['parsed_at'].min().date()} to {test_df['parsed_at'].max().date()}\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('bishkek_model_v2_config.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Config saved to: bishkek_model_v2_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"BISHKEK REAL ESTATE PRICE PREDICTION v2 - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n[DATA]\")\n",
    "print(f\"  Total samples: {len(df):,}\")\n",
    "print(f\"  Train: {len(train_df):,} | Test: {len(test_df):,}\")\n",
    "print(f\"  Split method: Temporal (last 20% by date)\")\n",
    "print(f\"  JK coverage: {df['jk_name'].notna().mean()*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n[FEATURES]\")\n",
    "print(f\"  Total features: {len(final_features)}\")\n",
    "print(f\"  Target encoding: CV-based (no leakage)\")\n",
    "print(f\"  Top 3: {', '.join(importance_df['feature'].head(3).tolist())}\")\n",
    "\n",
    "print(f\"\\n[MODEL]\")\n",
    "print(f\"  Ensemble: {final_method.title()} (XGBoost + LightGBM + CatBoost)\")\n",
    "print(f\"  Prediction intervals: 80% CI (Quantile Regression)\")\n",
    "\n",
    "print(f\"\\n[PERFORMANCE]\")\n",
    "print(f\"  MAE:        ${final_metrics['MAE']:,.0f}/m²\")\n",
    "print(f\"  MedAE:      ${final_metrics['MedAE']:,.0f}/m²\")\n",
    "print(f\"  R²:         {final_metrics['R²']:.3f}\")\n",
    "print(f\"  MedAPE:     {final_metrics['MedAPE%']:.1f}%\")\n",
    "print(f\"  Within 10%: {final_metrics['Within10%']:.1f}%\")\n",
    "print(f\"  CI Coverage: {coverage:.1f}% (target: 80%)\")\n",
    "\n",
    "print(f\"\\n[INTERPRETATION]\")\n",
    "median_price = df[target].median()\n",
    "print(f\"  Median price: ${median_price:,.0f}/m²\")\n",
    "print(f\"  Typical error: ${final_metrics['MedAE']:,.0f}/m² ({final_metrics['MedAE']/median_price*100:.1f}%)\")\n",
    "print(f\"  For 60m² apartment (~${median_price*60:,.0f}):\")\n",
    "print(f\"    Expected error: ~${final_metrics['MedAE']*60:,.0f}\")\n",
    "\n",
    "print(f\"\\n[IMPROVEMENTS OVER v1]\")\n",
    "print(f\"  + Temporal split (no future leakage)\")\n",
    "print(f\"  + Target encoding with CV (no train leakage)\")\n",
    "print(f\"  + Proper outlier detection (IQR + domain rules)\")\n",
    "print(f\"  + Duplicate removal\")\n",
    "print(f\"  + Ensemble model (3 algorithms)\")\n",
    "print(f\"  + Prediction intervals (quantile regression)\")\n",
    "print(f\"  + Comprehensive residual analysis\")\n",
    "print(f\"  + Permutation importance (more reliable)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}