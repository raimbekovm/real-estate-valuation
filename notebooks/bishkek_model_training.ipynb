{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Bishkek Real Estate Price Prediction\n",
    "\n",
    "**Goal:** Predict apartment price per square meter ($/m²) in Bishkek, Kyrgyzstan\n",
    "\n",
    "**Dataset:** 8,821 apartment listings from house.kg (January 2025)\n",
    "\n",
    "**Approach:**\n",
    "1. Feature Engineering\n",
    "2. Baseline Models Comparison\n",
    "3. Hyperparameter Tuning (Optuna)\n",
    "4. Final Evaluation & SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (for Kaggle)\n",
    "!pip install -q optuna shap catboost lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GroupKFold\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import optuna\n",
    "import shap\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Load from SQLite database\n",
    "# For Kaggle: change to CSV path\n",
    "try:\n",
    "    # Local path\n",
    "    conn = sqlite3.connect('../data/databases/bishkek.db')\n",
    "except:\n",
    "    # Kaggle path\n",
    "    conn = sqlite3.connect('/kaggle/input/bishkek-real-estate-2025/bishkek.db')\n",
    "\n",
    "df = pd.read_sql('''\n",
    "    SELECT \n",
    "        a.*,\n",
    "        rc.name as jk_name,\n",
    "        rc.class as jk_class,\n",
    "        rc.status as jk_status,\n",
    "        rc.developer_name\n",
    "    FROM apartments a\n",
    "    LEFT JOIN residential_complexes rc ON a.residential_complex_id = rc.id\n",
    "    WHERE a.price_usd IS NOT NULL \n",
    "      AND a.area IS NOT NULL\n",
    "      AND a.price_per_m2 > 0\n",
    "      AND a.price_per_m2 < 10000  -- Filter outliers (> $10k/m2 is unrealistic)\n",
    "''', conn)\n",
    "conn.close()\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick overview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable statistics\n",
    "target = 'price_per_m2'\n",
    "\n",
    "print(\"Target variable statistics:\")\n",
    "print(f\"  Mean:   ${df[target].mean():,.0f}/m²\")\n",
    "print(f\"  Median: ${df[target].median():,.0f}/m²\")\n",
    "print(f\"  Std:    ${df[target].std():,.0f}/m²\")\n",
    "print(f\"  Min:    ${df[target].min():,.0f}/m²\")\n",
    "print(f\"  Max:    ${df[target].max():,.0f}/m²\")\n",
    "\n",
    "print(f\"\\nResidential complex coverage:\")\n",
    "print(f\"  With JK: {df['jk_name'].notna().sum()} ({df['jk_name'].notna().mean()*100:.1f}%)\")\n",
    "print(f\"  Unique JK: {df['jk_name'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(df[target], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Price per m² (USD)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Price Distribution')\n",
    "axes[0].axvline(df[target].median(), color='red', linestyle='--', label=f'Median: ${df[target].median():,.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(np.log1p(df[target]), bins=50, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[1].set_xlabel('Log(Price per m²)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Log-transformed Price Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# POI (Points of Interest) - Bishkek coordinates\n",
    "# ===================\n",
    "\n",
    "POI = {\n",
    "    # Shopping malls\n",
    "    'dordoi_plaza': (42.8750, 74.6128),\n",
    "    'bishkek_park': (42.8741, 74.5888),\n",
    "    'tsum': (42.8746, 74.6031),\n",
    "    'vefa_center': (42.8668, 74.5931),\n",
    "    'asia_mall': (42.8489, 74.5672),\n",
    "    'karavan': (42.8562, 74.5686),\n",
    "    \n",
    "    # Key landmarks\n",
    "    'ala_too_square': (42.8746, 74.6030),\n",
    "    'philharmonic': (42.8749, 74.6108),\n",
    "    'white_house': (42.8760, 74.6097),\n",
    "    'victory_square': (42.8722, 74.5875),\n",
    "    \n",
    "    # Universities\n",
    "    'knu': (42.8778, 74.6027),  # Kyrgyz National University\n",
    "    'auca': (42.8634, 74.6167),  # American University of Central Asia\n",
    "    'krsu': (42.8750, 74.5861),  # Kyrgyz-Russian Slavic University\n",
    "    \n",
    "    # Transport\n",
    "    'west_bus_station': (42.8628, 74.5294),\n",
    "    'east_bus_station': (42.8605, 74.6550),\n",
    "    'railway_station': (42.8588, 74.6339),\n",
    "    \n",
    "    # Markets\n",
    "    'osh_bazaar': (42.8722, 74.5761),\n",
    "    'dordoi_bazaar': (42.9453, 74.6494),\n",
    "    'ortosay_bazaar': (42.8478, 74.5542),\n",
    "    \n",
    "    # City center\n",
    "    'center': (42.8746, 74.5888),\n",
    "}\n",
    "\n",
    "# ===================\n",
    "# Parks in Bishkek\n",
    "# ===================\n",
    "\n",
    "PARKS = {\n",
    "    'dubovy_park': [\n",
    "        (42.8749, 74.5875), (42.8780, 74.5875), (42.8780, 74.5930), (42.8749, 74.5930)\n",
    "    ],\n",
    "    'park_panfilova': [\n",
    "        (42.8740, 74.6000), (42.8760, 74.6000), (42.8760, 74.6050), (42.8740, 74.6050)\n",
    "    ],\n",
    "    'park_atatürk': [\n",
    "        (42.8690, 74.5950), (42.8720, 74.5950), (42.8720, 74.6000), (42.8690, 74.6000)\n",
    "    ],\n",
    "    'botanical_garden': [\n",
    "        (42.8560, 74.5560), (42.8620, 74.5560), (42.8620, 74.5660), (42.8560, 74.5660)\n",
    "    ],\n",
    "    'youth_park': [\n",
    "        (42.8650, 74.5700), (42.8700, 74.5700), (42.8700, 74.5800), (42.8650, 74.5800)\n",
    "    ],\n",
    "}\n",
    "\n",
    "# ===================\n",
    "# Ala-Archa River (through Bishkek)\n",
    "# ===================\n",
    "ALA_ARCHA_RIVER = [\n",
    "    (42.7800, 74.5700), (42.8100, 74.5650), (42.8400, 74.5600),\n",
    "    (42.8600, 74.5580), (42.8800, 74.5560), (42.9000, 74.5550),\n",
    "    (42.9200, 74.5540), (42.9400, 74.5520)\n",
    "]\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate distance between two points in km using Haversine formula\"\"\"\n",
    "    R = 6371  # Earth's radius in km\n",
    "    \n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    return R * c\n",
    "\n",
    "def distance_to_polyline(lat, lon, polyline):\n",
    "    \"\"\"Calculate minimum distance from point to polyline\"\"\"\n",
    "    min_dist = float('inf')\n",
    "    \n",
    "    for i in range(len(polyline) - 1):\n",
    "        p1_lat, p1_lon = polyline[i]\n",
    "        p2_lat, p2_lon = polyline[i + 1]\n",
    "        \n",
    "        d1 = haversine_distance(lat, lon, p1_lat, p1_lon)\n",
    "        d2 = haversine_distance(lat, lon, p2_lat, p2_lon)\n",
    "        mid_lat = (p1_lat + p2_lat) / 2\n",
    "        mid_lon = (p1_lon + p2_lon) / 2\n",
    "        d_mid = haversine_distance(lat, lon, mid_lat, mid_lon)\n",
    "        \n",
    "        min_dist = min(min_dist, d1, d2, d_mid)\n",
    "    \n",
    "    return min_dist\n",
    "\n",
    "def point_in_polygon(lat, lon, polygon):\n",
    "    \"\"\"Check if point is inside polygon using ray casting\"\"\"\n",
    "    n = len(polygon)\n",
    "    inside = False\n",
    "    \n",
    "    p1_lat, p1_lon = polygon[0]\n",
    "    for i in range(1, n + 1):\n",
    "        p2_lat, p2_lon = polygon[i % n]\n",
    "        if lon > min(p1_lon, p2_lon):\n",
    "            if lon <= max(p1_lon, p2_lon):\n",
    "                if lat <= max(p1_lat, p2_lat):\n",
    "                    if p1_lon != p2_lon:\n",
    "                        lat_inters = (lon - p1_lon) * (p2_lat - p1_lat) / (p2_lon - p1_lon) + p1_lat\n",
    "                    if p1_lat == p2_lat or lat <= lat_inters:\n",
    "                        inside = not inside\n",
    "        p1_lat, p1_lon = p2_lat, p2_lon\n",
    "    \n",
    "    return inside\n",
    "\n",
    "def point_in_any_park(lat, lon):\n",
    "    \"\"\"Check if point is inside any park\"\"\"\n",
    "    for park_name, polygon in PARKS.items():\n",
    "        if point_in_polygon(lat, lon, polygon):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "print(f\"Loaded {len(POI)} POI locations\")\n",
    "print(f\"Loaded {len(PARKS)} park polygons\")\n",
    "print(f\"Loaded Ala-Archa river with {len(ALA_ARCHA_RIVER)} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, fit_encoders=None, target_col=None):\n",
    "    \"\"\"\n",
    "    Create all features for the model.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    encoders = fit_encoders or {}\n",
    "    \n",
    "    # ===================\n",
    "    # Floor features\n",
    "    # ===================\n",
    "    df['total_floors'] = df['total_floors'].fillna(df['total_floors'].median())\n",
    "    df['floor'] = df['floor'].fillna(1)\n",
    "    df['floor_ratio'] = df['floor'] / df['total_floors'].replace(0, 1)\n",
    "    df['is_first_floor'] = (df['floor'] == 1).astype(int)\n",
    "    df['is_last_floor'] = (df['floor'] == df['total_floors']).astype(int)\n",
    "    \n",
    "    # ===================\n",
    "    # Building features\n",
    "    # ===================\n",
    "    df['year_built'] = df['year_built'].fillna(df['year_built'].median())\n",
    "    df['building_age'] = 2025 - df['year_built']\n",
    "    df['is_new_building'] = (df['year_built'] >= 2018).astype(int)\n",
    "    df['is_highrise'] = (df['total_floors'] >= 9).astype(int)\n",
    "    \n",
    "    # ===================\n",
    "    # Area features\n",
    "    # ===================\n",
    "    df['rooms'] = df['rooms'].fillna(df['rooms'].median())\n",
    "    df['area_per_room'] = df['area'] / df['rooms'].replace(0, 1)\n",
    "    df['is_large_apartment'] = (df['area'] >= 100).astype(int)\n",
    "    \n",
    "    # Kitchen\n",
    "    df['kitchen_area'] = pd.to_numeric(df['kitchen_area'], errors='coerce')\n",
    "    df['kitchen_ratio'] = df['kitchen_area'] / df['area']\n",
    "    df['kitchen_ratio'] = df['kitchen_ratio'].clip(0, 0.5)\n",
    "    df['kitchen_missing'] = df['kitchen_ratio'].isna().astype(int)\n",
    "    kitchen_median = df['kitchen_ratio'].median() if fit_encoders is None else encoders.get('kitchen_median', 0.12)\n",
    "    df['kitchen_ratio'] = df['kitchen_ratio'].fillna(kitchen_median)\n",
    "    if fit_encoders is None:\n",
    "        encoders['kitchen_median'] = kitchen_median\n",
    "    \n",
    "    # ===================\n",
    "    # Ceiling height\n",
    "    # ===================\n",
    "    def parse_ceiling(val):\n",
    "        if pd.isna(val):\n",
    "            return np.nan\n",
    "        val = str(val).replace('м', '').replace(',', '.').strip()\n",
    "        try:\n",
    "            return float(val)\n",
    "        except:\n",
    "            return np.nan\n",
    "    \n",
    "    df['ceiling_height_m'] = df['ceiling_height'].apply(parse_ceiling)\n",
    "    df['ceiling_missing'] = df['ceiling_height_m'].isna().astype(int)\n",
    "    ceiling_median = df['ceiling_height_m'].median() if fit_encoders is None else encoders.get('ceiling_median', 2.7)\n",
    "    df['ceiling_height_m'] = df['ceiling_height_m'].fillna(ceiling_median)\n",
    "    if fit_encoders is None:\n",
    "        encoders['ceiling_median'] = ceiling_median\n",
    "    \n",
    "    # ===================\n",
    "    # Condition\n",
    "    # ===================\n",
    "    condition_map = {\n",
    "        'евроремонт': 4,\n",
    "        'хороший': 3,\n",
    "        'средний': 2,\n",
    "        'черновая отделка': 1,\n",
    "        'требует ремонта': 0\n",
    "    }\n",
    "    df['condition_score'] = df['condition'].map(condition_map)\n",
    "    df['condition_missing'] = df['condition_score'].isna().astype(int)\n",
    "    df['condition_score'] = df['condition_score'].fillna(2)\n",
    "    \n",
    "    # ===================\n",
    "    # Bathroom\n",
    "    # ===================\n",
    "    df['has_separate_bathroom'] = df['bathroom'].str.contains('раздельн', na=False).astype(int)\n",
    "    df['has_2plus_bathrooms'] = df['bathroom'].str.contains('2|два', na=False, case=False).astype(int)\n",
    "    \n",
    "    # ===================\n",
    "    # Balcony\n",
    "    # ===================\n",
    "    df['has_balcony'] = df['balcony'].notna().astype(int)\n",
    "    df['has_loggia'] = df['balcony'].str.contains('лоджия', na=False).astype(int)\n",
    "    \n",
    "    # ===================\n",
    "    # Parking\n",
    "    # ===================\n",
    "    df['has_parking'] = df['parking'].notna().astype(int)\n",
    "    \n",
    "    # ===================\n",
    "    # Floor type\n",
    "    # ===================\n",
    "    df['is_parquet'] = df['floor_type'].str.contains('паркет', na=False).astype(int)\n",
    "    df['is_laminate'] = df['floor_type'].str.contains('ламинат', na=False).astype(int)\n",
    "    \n",
    "    # ===================\n",
    "    # Security\n",
    "    # ===================\n",
    "    df['security_score'] = (\n",
    "        df['security'].str.contains('охран', na=False).astype(int) * 2 +\n",
    "        df['security'].str.contains('видео', na=False).astype(int) +\n",
    "        df['security'].str.contains('домофон', na=False).astype(int) +\n",
    "        df['security'].str.contains('консьерж', na=False).astype(int)\n",
    "    )\n",
    "    \n",
    "    # ===================\n",
    "    # Furniture\n",
    "    # ===================\n",
    "    df['has_furniture'] = df['furniture'].notna().astype(int)\n",
    "    \n",
    "    # ===================\n",
    "    # House type\n",
    "    # ===================\n",
    "    df['is_monolith'] = (df['house_type'] == 'монолитный').astype(int)\n",
    "    df['is_brick'] = (df['house_type'] == 'кирпичный').astype(int)\n",
    "    df['is_panel'] = (df['house_type'] == 'панельный').astype(int)\n",
    "    \n",
    "    # ===================\n",
    "    # Residential complex features\n",
    "    # ===================\n",
    "    df['has_jk'] = df['jk_name'].notna().astype(int)\n",
    "    \n",
    "    # JK class (эконом/комфорт/бизнес/премиум)\n",
    "    jk_class_map = {\n",
    "        'эконом': 1, 'комфорт': 2, 'бизнес': 3, 'премиум': 4, 'элит': 4\n",
    "    }\n",
    "    df['jk_class_score'] = df['jk_class'].map(jk_class_map).fillna(0)\n",
    "    \n",
    "    # JK status (строится/сдан)\n",
    "    df['jk_is_completed'] = (df['jk_status'] == 'completed').astype(int)\n",
    "    \n",
    "    # ===================\n",
    "    # Location features\n",
    "    # ===================\n",
    "    # District encoding will be done via target encoding\n",
    "    \n",
    "    # Fill missing coordinates with city center\n",
    "    df['latitude'] = df['latitude'].fillna(42.8746)\n",
    "    df['longitude'] = df['longitude'].fillna(74.5888)\n",
    "    \n",
    "    # Distance to river\n",
    "    df['dist_river'] = df.apply(\n",
    "        lambda row: distance_to_polyline(row['latitude'], row['longitude'], ALA_ARCHA_RIVER),\n",
    "        axis=1\n",
    "    )\n",
    "    df['near_river'] = (df['dist_river'] <= 0.5).astype(int)\n",
    "    \n",
    "    # ===================\n",
    "    # POI distances\n",
    "    # ===================\n",
    "    lats = df['latitude'].values\n",
    "    lons = df['longitude'].values\n",
    "    \n",
    "    for poi_name, (poi_lat, poi_lon) in POI.items():\n",
    "        df[f'dist_{poi_name}'] = haversine_distance(lats, lons, poi_lat, poi_lon)\n",
    "    \n",
    "    # Aggregated distances\n",
    "    mall_pois = ['dordoi_plaza', 'bishkek_park', 'tsum', 'vefa_center', 'asia_mall', 'karavan']\n",
    "    df['dist_nearest_mall'] = df[[f'dist_{p}' for p in mall_pois]].min(axis=1)\n",
    "    \n",
    "    transport_pois = ['west_bus_station', 'east_bus_station', 'railway_station']\n",
    "    df['dist_nearest_station'] = df[[f'dist_{p}' for p in transport_pois]].min(axis=1)\n",
    "    \n",
    "    market_pois = ['osh_bazaar', 'dordoi_bazaar', 'ortosay_bazaar']\n",
    "    df['dist_nearest_bazaar'] = df[[f'dist_{p}' for p in market_pois]].min(axis=1)\n",
    "    \n",
    "    df['dist_center'] = df['dist_center']\n",
    "    \n",
    "    # ===================\n",
    "    # Park features\n",
    "    # ===================\n",
    "    df['near_park'] = df.apply(\n",
    "        lambda row: point_in_any_park(row['latitude'], row['longitude']), \n",
    "        axis=1\n",
    "    ).astype(int)\n",
    "    \n",
    "    for park_name, polygon in PARKS.items():\n",
    "        centroid_lat = np.mean([p[0] for p in polygon])\n",
    "        centroid_lon = np.mean([p[1] for p in polygon])\n",
    "        df[f'dist_{park_name}'] = haversine_distance(lats, lons, centroid_lat, centroid_lon)\n",
    "    \n",
    "    park_dist_cols = [f'dist_{p}' for p in PARKS.keys()]\n",
    "    df['dist_nearest_park'] = df[park_dist_cols].min(axis=1)\n",
    "    \n",
    "    return df, encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering\n",
    "df_features, feature_encoders = create_features(df)\n",
    "print(f\"Features created. New shape: {df_features.shape}\")\n",
    "\n",
    "# Show distance feature statistics\n",
    "poi_dist_cols = [c for c in df_features.columns if c.startswith('dist_')][:5]\n",
    "print(f\"\\nDistance features sample:\")\n",
    "print(df_features[poi_dist_cols].describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_encoders(train_df, cols, target_col, min_samples=5):\n",
    "    \"\"\"\n",
    "    Create target encoders from training data only (no leakage).\n",
    "    Returns dict of {col: {category: encoded_value}}\n",
    "    \"\"\"\n",
    "    encoders = {}\n",
    "    global_mean = train_df[target_col].mean()\n",
    "    \n",
    "    for col in cols:\n",
    "        # Handle NaN values by converting to string\n",
    "        temp_col = train_df[col].fillna('__MISSING__')\n",
    "        temp_target = train_df[target_col]\n",
    "        \n",
    "        agg = pd.DataFrame({'val': temp_col, 'target': temp_target}).groupby('val')['target'].agg(['mean', 'count'])\n",
    "        \n",
    "        # Smoothing: blend with global mean based on sample size\n",
    "        smoothing_factor = agg['count'] / (agg['count'] + min_samples)\n",
    "        smoothed_mean = smoothing_factor * agg['mean'] + (1 - smoothing_factor) * global_mean\n",
    "        \n",
    "        encoders[col] = {\n",
    "            'mapping': smoothed_mean.to_dict(),\n",
    "            'global_mean': global_mean\n",
    "        }\n",
    "    \n",
    "    return encoders\n",
    "\n",
    "\n",
    "def apply_target_encoding(df, encoders):\n",
    "    \"\"\"Apply target encoding using pre-fitted encoders (safe for train/test)\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    for col, encoder in encoders.items():\n",
    "        col_mean = f'{col}_price_mean'\n",
    "        # Handle NaN by mapping to '__MISSING__'\n",
    "        temp_col = df[col].fillna('__MISSING__')\n",
    "        df[col_mean] = temp_col.map(encoder['mapping'])\n",
    "        df[col_mean] = df[col_mean].fillna(encoder['global_mean'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"Target encoding functions defined (will apply after train/test split)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# FEATURE LIST\n",
    "# ===================\n",
    "\n",
    "# Core numeric\n",
    "numeric_features = [\n",
    "    'rooms', 'area', 'floor', 'total_floors', 'year_built',\n",
    "    'latitude', 'longitude'\n",
    "]\n",
    "\n",
    "# Floor & Building\n",
    "floor_building_features = [\n",
    "    'floor_ratio', 'is_first_floor', 'is_last_floor',\n",
    "    'building_age', 'is_new_building', 'is_highrise',\n",
    "]\n",
    "\n",
    "# Area & Kitchen\n",
    "area_features = [\n",
    "    'area_per_room', 'is_large_apartment',\n",
    "    'kitchen_ratio', 'kitchen_missing',\n",
    "]\n",
    "\n",
    "# Ceiling\n",
    "ceiling_features = [\n",
    "    'ceiling_height_m', 'ceiling_missing',\n",
    "]\n",
    "\n",
    "# Condition\n",
    "condition_features = [\n",
    "    'condition_score', 'condition_missing',\n",
    "]\n",
    "\n",
    "# Bathroom & Balcony\n",
    "bathroom_balcony_features = [\n",
    "    'has_separate_bathroom', 'has_2plus_bathrooms',\n",
    "    'has_balcony', 'has_loggia',\n",
    "]\n",
    "\n",
    "# Other amenities\n",
    "amenity_features = [\n",
    "    'has_parking', 'is_parquet', 'is_laminate',\n",
    "    'security_score', 'has_furniture',\n",
    "]\n",
    "\n",
    "# House type\n",
    "house_type_features = ['is_monolith', 'is_brick', 'is_panel']\n",
    "\n",
    "# Residential complex\n",
    "jk_features = [\n",
    "    'has_jk', 'jk_class_score', 'jk_is_completed',\n",
    "]\n",
    "\n",
    "# Target encoding\n",
    "target_encoding_features = [\n",
    "    'district_price_mean',\n",
    "    'jk_name_price_mean',  # Target encoding by residential complex\n",
    "]\n",
    "\n",
    "# Location & River\n",
    "location_features = [\n",
    "    'dist_river', 'near_river',\n",
    "]\n",
    "\n",
    "# POI distances\n",
    "poi_features = [f'dist_{poi}' for poi in POI.keys()] + [\n",
    "    'dist_nearest_mall', 'dist_nearest_station', 'dist_nearest_bazaar', 'dist_center',\n",
    "]\n",
    "\n",
    "# Park features\n",
    "park_features = [f'dist_{park}' for park in PARKS.keys()] + [\n",
    "    'dist_nearest_park', 'near_park',\n",
    "]\n",
    "\n",
    "# Combine all features\n",
    "all_features = (\n",
    "    numeric_features + \n",
    "    floor_building_features + \n",
    "    area_features +\n",
    "    ceiling_features +\n",
    "    condition_features +\n",
    "    bathroom_balcony_features +\n",
    "    amenity_features +\n",
    "    house_type_features +\n",
    "    jk_features +\n",
    "    target_encoding_features +\n",
    "    location_features +\n",
    "    poi_features + \n",
    "    park_features\n",
    ")\n",
    "\n",
    "print(f\"TOTAL FEATURES: {len(all_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# TRAIN/TEST SPLIT FIRST (before target encoding!)\n",
    "# ===================\n",
    "target = 'price_per_m2'\n",
    "\n",
    "# Split indices\n",
    "train_idx, test_idx = train_test_split(\n",
    "    df_features.index, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_df = df_features.loc[train_idx].copy()\n",
    "test_df = df_features.loc[test_idx].copy()\n",
    "\n",
    "print(f\"Train set: {len(train_df)} samples\")\n",
    "print(f\"Test set:  {len(test_df)} samples\")\n",
    "\n",
    "# ===================\n",
    "# TARGET ENCODING (fit on train only!)\n",
    "# ===================\n",
    "target_cols = ['district', 'jk_name']\n",
    "target_encoders = create_target_encoders(\n",
    "    train_df, target_cols, target, min_samples=10\n",
    ")\n",
    "\n",
    "# Apply to both sets\n",
    "train_df = apply_target_encoding(train_df, target_encoders)\n",
    "test_df = apply_target_encoding(test_df, target_encoders)\n",
    "\n",
    "print(\"\\nTarget encoding applied (fitted on train only - no leakage)\")\n",
    "\n",
    "# Show JK price stats\n",
    "print(\"\\nTop 10 JK by average price (from training data):\")\n",
    "jk_stats = train_df[train_df['jk_name'].notna()].groupby('jk_name')[target].agg(['mean', 'count'])\n",
    "jk_stats = jk_stats[jk_stats['count'] >= 5].sort_values('mean', ascending=False)\n",
    "print(jk_stats.head(10).round(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# PREPARE FINAL FEATURES\n",
    "# ===================\n",
    "\n",
    "# Check which features are available\n",
    "available_features = [f for f in all_features if f in train_df.columns]\n",
    "missing_features = [f for f in all_features if f not in train_df.columns]\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"Warning: {len(missing_features)} features not found:\")\n",
    "    print(f\"  {missing_features[:10]}...\")\n",
    "\n",
    "print(f\"\\nUsing {len(available_features)} features\")\n",
    "\n",
    "# Create X and y\n",
    "X_train = train_df[available_features].values\n",
    "X_test = test_df[available_features].values\n",
    "y_train = train_df[target].values\n",
    "y_test = test_df[target].values\n",
    "\n",
    "# Replace any remaining NaN with 0\n",
    "X_train = np.nan_to_num(X_train, nan=0.0)\n",
    "X_test = np.nan_to_num(X_test, nan=0.0)\n",
    "\n",
    "print(f\"\\nX_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape:  {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# CREATE GROUP LABELS for GroupKFold\n",
    "# ===================\n",
    "# Apartments in the same residential complex should not be split across train/validation\n",
    "\n",
    "complex_col = 'jk_name'\n",
    "train_groups = train_df[complex_col].fillna('unknown_' + train_df.index.astype(str)).copy()\n",
    "\n",
    "# Convert to numeric group IDs\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "group_encoder = LabelEncoder()\n",
    "train_group_ids = group_encoder.fit_transform(train_groups.values)\n",
    "\n",
    "n_complexes = len(np.unique(train_group_ids))\n",
    "print(f\"Created {n_complexes} unique groups for GroupKFold\")\n",
    "print(f\"  - Named complexes: {train_df[complex_col].notna().sum()}\")\n",
    "print(f\"  - Unnamed (individual): {train_df[complex_col].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 3. Baseline Models Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"Train and evaluate a model, return metrics\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Train MAE': mean_absolute_error(y_train, y_pred_train),\n",
    "        'Test MAE': mean_absolute_error(y_test, y_pred_test),\n",
    "        'Train R²': r2_score(y_train, y_pred_train),\n",
    "        'Test R²': r2_score(y_test, y_pred_test),\n",
    "        'MAPE (%)': np.mean(np.abs((y_test - y_pred_test) / y_test)) * 100\n",
    "    }\n",
    "    \n",
    "    return metrics, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline models\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(\n",
    "        n_estimators=200, max_depth=15, random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    'XGBoost': XGBRegressor(\n",
    "        n_estimators=300, max_depth=10, learning_rate=0.05, random_state=42\n",
    "    ),\n",
    "    'LightGBM': LGBMRegressor(\n",
    "        n_estimators=300, max_depth=10, learning_rate=0.05, random_state=42, verbose=-1\n",
    "    ),\n",
    "    'CatBoost': CatBoostRegressor(\n",
    "        n_estimators=300, max_depth=10, learning_rate=0.05, random_state=42, verbose=0\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate all models\n",
    "results = []\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    metrics, trained_model = evaluate_model(model, X_train, X_test, y_train, y_test, name)\n",
    "    results.append(metrics)\n",
    "    trained_models[name] = trained_model\n",
    "    print(f\"  Test MAE: ${metrics['Test MAE']:,.0f}/m² | Test R²: {metrics['Test R²']:.3f} | MAPE: {metrics['MAPE (%)']:.1f}%\")\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results comparison\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('Test MAE')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BASELINE MODELS COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "colors = ['#2ecc71' if m == results_df['Test MAE'].min() else '#3498db' for m in results_df['Test MAE']]\n",
    "axes[0].barh(results_df['Model'], results_df['Test MAE'], color=colors)\n",
    "axes[0].set_xlabel('MAE ($/m²)')\n",
    "axes[0].set_title('Test MAE by Model (lower is better)')\n",
    "\n",
    "colors = ['#2ecc71' if r == results_df['Test R²'].max() else '#3498db' for r in results_df['Test R²']]\n",
    "axes[1].barh(results_df['Model'], results_df['Test R²'], color=colors)\n",
    "axes[1].set_xlabel('R²')\n",
    "axes[1].set_title('Test R² by Model (higher is better)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning (Optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"Optuna objective function for XGBoost with GroupKFold\"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 0.9),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.9),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-4, 1.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-4, 10.0, log=True),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    model = XGBRegressor(**params)\n",
    "    \n",
    "    # GroupKFold - keeps same residential complex in same fold (no leakage)\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    \n",
    "    scores = []\n",
    "    for train_idx, val_idx in gkf.split(X_train, y_train, groups=train_group_ids):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "        scores.append(mean_absolute_error(y_val, y_pred))\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "print(\"Objective function defined with GroupKFold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Optuna optimization\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest trial:\")\n",
    "print(f\"  MAE (CV): ${study.best_trial.value:,.0f}/m²\")\n",
    "print(f\"\\nBest parameters:\")\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "trials = [t.value for t in study.trials]\n",
    "best_so_far = [min(trials[:i+1]) for i in range(len(trials))]\n",
    "axes[0].plot(trials, 'o-', alpha=0.5, label='Trial MAE')\n",
    "axes[0].plot(best_so_far, 'r-', linewidth=2, label='Best so far')\n",
    "axes[0].set_xlabel('Trial')\n",
    "axes[0].set_ylabel('MAE ($/m²)')\n",
    "axes[0].set_title('Optimization History')\n",
    "axes[0].legend()\n",
    "\n",
    "try:\n",
    "    importance = optuna.importance.get_param_importances(study)\n",
    "    params = list(importance.keys())\n",
    "    values = list(importance.values())\n",
    "    axes[1].barh(params, values, color='steelblue')\n",
    "    axes[1].set_xlabel('Importance')\n",
    "    axes[1].set_title('Hyperparameter Importance')\n",
    "except Exception as e:\n",
    "    axes[1].text(0.5, 0.5, f'Could not compute importance', ha='center', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## 5. Final Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best parameters\n",
    "best_params = study.best_trial.params\n",
    "best_params['random_state'] = 42\n",
    "\n",
    "final_model = XGBRegressor(**best_params)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = final_model.predict(X_train)\n",
    "y_pred_test = final_model.predict(X_test)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL MODEL RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTrain Set:\")\n",
    "print(f\"  MAE:  ${mean_absolute_error(y_train, y_pred_train):,.0f}/m²\")\n",
    "print(f\"  R²:   {r2_score(y_train, y_pred_train):.4f}\")\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  MAE:  ${mean_absolute_error(y_test, y_pred_test):,.0f}/m²\")\n",
    "print(f\"  R²:   {r2_score(y_test, y_pred_test):.4f}\")\n",
    "print(f\"  MAPE: {np.mean(np.abs((y_test - y_pred_test) / y_test)) * 100:.2f}%\")\n",
    "print(f\"  RMSE: ${np.sqrt(mean_squared_error(y_test, y_pred_test)):,.0f}/m²\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction vs Actual plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].scatter(y_test, y_pred_test, alpha=0.3, s=10)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)\n",
    "axes[0].set_xlabel('Actual Price ($/m²)')\n",
    "axes[0].set_ylabel('Predicted Price ($/m²)')\n",
    "axes[0].set_title('Predicted vs Actual')\n",
    "\n",
    "residuals = y_test - y_pred_test\n",
    "axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(0, color='red', linestyle='--')\n",
    "axes[1].set_xlabel('Residual ($/m²)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title(f'Residuals Distribution (Mean: ${residuals.mean():,.0f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "## 6. Feature Importance & SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': available_features,\n",
    "    'importance': final_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_n = 15\n",
    "plt.barh(importance_df['feature'][:top_n][::-1], \n",
    "         importance_df['importance'][:top_n][::-1], \n",
    "         color='steelblue')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title(f'Top {top_n} Feature Importances (XGBoost)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 15 features:\")\n",
    "for idx, row in importance_df.head(15).iterrows():\n",
    "    print(f\"  {row['feature']:30s}: {row['importance']:.4f} ({row['importance']*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP values\n",
    "print(\"Computing SHAP values...\")\n",
    "try:\n",
    "    explainer = shap.TreeExplainer(final_model)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    print(\"Done!\")\n",
    "    shap_computed = True\n",
    "except Exception as e:\n",
    "    print(f\"SHAP computation failed: {e}\")\n",
    "    shap_computed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP summary plot\n",
    "if shap_computed:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values, X_test, feature_names=available_features, show=False)\n",
    "    plt.title('SHAP Feature Impact on Price Prediction')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "## 7. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataframe with predictions\n",
    "test_analysis = test_df.copy()\n",
    "test_analysis['predicted'] = y_pred_test\n",
    "test_analysis['error'] = test_analysis[target] - test_analysis['predicted']\n",
    "test_analysis['abs_error'] = np.abs(test_analysis['error'])\n",
    "test_analysis['pct_error'] = test_analysis['abs_error'] / test_analysis[target] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error by district\n",
    "district_error = test_analysis.groupby('district').agg({\n",
    "    'abs_error': 'mean',\n",
    "    'pct_error': 'mean',\n",
    "    target: 'count'\n",
    "}).rename(columns={target: 'count'})\n",
    "district_error = district_error.sort_values('abs_error')\n",
    "\n",
    "print(\"Error by District:\")\n",
    "print(district_error.round(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error by JK presence\n",
    "jk_error = test_analysis.groupby('has_jk').agg({\n",
    "    'abs_error': 'mean',\n",
    "    'pct_error': 'mean',\n",
    "    target: 'count'\n",
    "}).rename(columns={target: 'count'})\n",
    "\n",
    "print(\"\\nError by Residential Complex Presence:\")\n",
    "print(jk_error.round(1))\n",
    "print(\"\\n0 = No JK, 1 = Has JK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error by room count\n",
    "room_error = test_analysis.groupby('rooms').agg({\n",
    "    'abs_error': 'mean',\n",
    "    'pct_error': 'mean',\n",
    "    target: 'count'\n",
    "}).rename(columns={target: 'count'})\n",
    "\n",
    "print(\"\\nError by Room Count:\")\n",
    "print(room_error.round(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-39",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "\n",
    "# Save model\n",
    "joblib.dump(final_model, 'bishkek_price_model.joblib')\n",
    "print(\"Model saved to: bishkek_price_model.joblib\")\n",
    "\n",
    "# Save feature list and encoders\n",
    "with open('bishkek_model_config.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump({\n",
    "        'features': available_features,\n",
    "        'best_params': best_params,\n",
    "        'target_encoders': {\n",
    "            col: {\n",
    "                'mapping': {str(k): float(v) for k, v in enc['mapping'].items()},\n",
    "                'global_mean': float(enc['global_mean'])\n",
    "            } for col, enc in target_encoders.items()\n",
    "        },\n",
    "        'feature_encoders': {\n",
    "            'kitchen_median': float(feature_encoders.get('kitchen_median', 0.12)),\n",
    "            'ceiling_median': float(feature_encoders.get('ceiling_median', 2.7))\n",
    "        },\n",
    "        'metrics': {\n",
    "            'test_mae': float(mean_absolute_error(y_test, y_pred_test)),\n",
    "            'test_r2': float(r2_score(y_test, y_pred_test)),\n",
    "            'test_mape': float(np.mean(np.abs((y_test - y_pred_test) / y_test)) * 100)\n",
    "        }\n",
    "    }, f, indent=2, ensure_ascii=False)\n",
    "print(\"Model config saved to: bishkek_model_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-41",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"BISHKEK REAL ESTATE PRICE PREDICTION - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDataset: {len(df):,} apartments\")\n",
    "print(f\"Features: {len(available_features)}\")\n",
    "print(f\"Train/Test split: 80/20\")\n",
    "print(f\"\\nResidential Complex Coverage:\")\n",
    "print(f\"  With JK: {df['jk_name'].notna().sum()} ({df['jk_name'].notna().mean()*100:.1f}%)\")\n",
    "print(f\"  Unique JK: {df['jk_name'].nunique()}\")\n",
    "print(f\"\\nBest Model: XGBoost (tuned with Optuna, 50 trials)\")\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"  MAE:  ${mean_absolute_error(y_test, y_pred_test):,.0f}/m²\")\n",
    "print(f\"  MAPE: {np.mean(np.abs((y_test - y_pred_test) / y_test)) * 100:.1f}%\")\n",
    "print(f\"  R²:   {r2_score(y_test, y_pred_test):.3f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  For an average apartment (60m², ~$95,000):\")\n",
    "print(f\"  Expected prediction error: ~${mean_absolute_error(y_test, y_pred_test) * 60:,.0f}\")\n",
    "print(f\"\\nTop 5 Most Important Features:\")\n",
    "for idx, row in importance_df.head(5).iterrows():\n",
    "    print(f\"  - {row['feature']} ({row['importance']*100:.1f}%)\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
