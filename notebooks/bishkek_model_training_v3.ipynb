{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bishkek Real Estate Price Prediction v3\n",
    "\n",
    "**Goal:** Predict apartment price per square meter ($/m²) in Bishkek, Kyrgyzstan\n",
    "\n",
    "**v3 Improvements (Phase 1 - Research-based):**\n",
    "- **Spatial Lag Features** - neighbor price statistics (Zillow-inspired)\n",
    "- **H3 Geographic Tiles** - Uber H3 hexagonal indexing at multiple resolutions\n",
    "- **Market Trend Features** - rolling price statistics by district\n",
    "- **Density Features** - listing density as desirability proxy\n",
    "\n",
    "**References:**\n",
    "- [Zillow Neural Zestimate](https://www.zillow.com/tech/building-the-neural-zestimate/)\n",
    "- [Multi-Head Gated Attention Paper](https://arxiv.org/abs/2405.07456)\n",
    "- [Spatial ML Methods](https://www.mdpi.com/2071-1050/14/15/9056)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (for Kaggle)\n",
    "!pip install -q optuna catboost lightgbm h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import hashlib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import KFold, GroupKFold, cross_val_predict\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neighbors import BallTree\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import h3\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "CURRENT_YEAR = datetime.now().year\n",
    "\n",
    "print(f\"Current year: {CURRENT_YEAR}\")\n",
    "print(f\"H3 version: {h3.__version__}\")\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load & Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Load from SQLite database\n",
    "if os.path.exists('/kaggle/input/bishkek-real-estate-2025/bishkek.db'):\n",
    "    db_path = '/kaggle/input/bishkek-real-estate-2025/bishkek.db'\n",
    "elif os.path.exists('../data/databases/bishkek.db'):\n",
    "    db_path = '../data/databases/bishkek.db'\n",
    "else:\n",
    "    raise FileNotFoundError(\"Database not found!\")\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "df_raw = pd.read_sql('''\n",
    "    SELECT \n",
    "        a.*,\n",
    "        rc.name as jk_name,\n",
    "        rc.class as jk_class,\n",
    "        rc.status as jk_status,\n",
    "        rc.developer_name\n",
    "    FROM apartments a\n",
    "    LEFT JOIN residential_complexes rc ON a.residential_complex_id = rc.id\n",
    "    WHERE a.price_usd IS NOT NULL \n",
    "      AND a.area IS NOT NULL\n",
    "      AND a.price_per_m2 > 0\n",
    "''', conn)\n",
    "conn.close()\n",
    "\n",
    "print(f\"Database: {db_path}\")\n",
    "print(f\"Raw dataset: {len(df_raw)} rows\")\n",
    "print(f\"Columns: {len(df_raw.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# 1.1 Remove Duplicates\n",
    "# ===================\n",
    "\n",
    "df_raw['parsed_at'] = pd.to_datetime(df_raw['parsed_at'])\n",
    "\n",
    "# Create building signature for duplicate detection\n",
    "df_raw['building_signature'] = (\n",
    "    df_raw['address'].fillna('').str.lower() + '_' +\n",
    "    df_raw['floor'].fillna(0).astype(str) + '_' +\n",
    "    df_raw['area'].fillna(0).astype(str) + '_' +\n",
    "    df_raw['rooms'].fillna(0).astype(str)\n",
    ")\n",
    "\n",
    "df_raw = df_raw.sort_values('parsed_at')\n",
    "duplicates_before = len(df_raw)\n",
    "df_raw = df_raw.drop_duplicates(subset=['building_signature'], keep='last')\n",
    "duplicates_removed = duplicates_before - len(df_raw)\n",
    "\n",
    "print(f\"Duplicates removed: {duplicates_removed}\")\n",
    "print(f\"Dataset after dedup: {len(df_raw)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# 1.2 Outlier Detection (IQR + Domain Rules)\n",
    "# ===================\n",
    "\n",
    "target = 'price_per_m2'\n",
    "\n",
    "def detect_outliers_iqr(series: pd.Series, k: float = 1.5) -> pd.Series:\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - k * IQR\n",
    "    upper = Q3 + k * IQR\n",
    "    return (series >= lower) & (series <= upper)\n",
    "\n",
    "price_mask = detect_outliers_iqr(df_raw[target], k=2.0)\n",
    "\n",
    "domain_mask = (\n",
    "    (df_raw[target] >= 300) &\n",
    "    (df_raw[target] <= 5000) &\n",
    "    (df_raw['area'] >= 15) &\n",
    "    (df_raw['area'] <= 500) &\n",
    "    (df_raw['rooms'].fillna(1) <= 10) &\n",
    "    (df_raw['floor'].fillna(1) <= 50)\n",
    ")\n",
    "\n",
    "valid_mask = price_mask & domain_mask\n",
    "outliers_removed = (~valid_mask).sum()\n",
    "\n",
    "print(f\"Outliers detected: {outliers_removed}\")\n",
    "\n",
    "df = df_raw[valid_mask].copy()\n",
    "print(f\"Final dataset: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Temporal Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORAL SPLIT (not random!)\n",
    "df = df.sort_values('parsed_at').reset_index(drop=True)\n",
    "\n",
    "split_idx = int(len(df) * 0.8)\n",
    "split_date = df.iloc[split_idx]['parsed_at']\n",
    "\n",
    "train_df = df.iloc[:split_idx].copy()\n",
    "test_df = df.iloc[split_idx:].copy()\n",
    "\n",
    "print(f\"Temporal Split:\")\n",
    "print(f\"  Train: {len(train_df)} samples ({df['parsed_at'].min().date()} to {split_date.date()})\")\n",
    "print(f\"  Test:  {len(test_df)} samples ({split_date.date()} to {df['parsed_at'].max().date()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering (v3 - with Advanced Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# POI (Points of Interest) - Bishkek coordinates\n",
    "# ===================\n",
    "\n",
    "POI = {\n",
    "    'dordoi_plaza': (42.8750, 74.6128),\n",
    "    'bishkek_park': (42.8741, 74.5888),\n",
    "    'tsum': (42.8746, 74.6031),\n",
    "    'vefa_center': (42.8668, 74.5931),\n",
    "    'asia_mall': (42.8489, 74.5672),\n",
    "    'karavan': (42.8562, 74.5686),\n",
    "    'ala_too_square': (42.8746, 74.6030),\n",
    "    'philharmonic': (42.8749, 74.6108),\n",
    "    'white_house': (42.8760, 74.6097),\n",
    "    'victory_square': (42.8722, 74.5875),\n",
    "    'knu': (42.8778, 74.6027),\n",
    "    'auca': (42.8634, 74.6167),\n",
    "    'krsu': (42.8750, 74.5861),\n",
    "    'west_bus_station': (42.8628, 74.5294),\n",
    "    'east_bus_station': (42.8605, 74.6550),\n",
    "    'railway_station': (42.8588, 74.6339),\n",
    "    'osh_bazaar': (42.8722, 74.5761),\n",
    "    'dordoi_bazaar': (42.9453, 74.6494),\n",
    "    'ortosay_bazaar': (42.8478, 74.5542),\n",
    "    'center': (42.8746, 74.5888),\n",
    "}\n",
    "\n",
    "PARKS = {\n",
    "    'dubovy_park': [(42.8749, 74.5875), (42.8780, 74.5875), (42.8780, 74.5930), (42.8749, 74.5930)],\n",
    "    'park_panfilova': [(42.8740, 74.6000), (42.8760, 74.6000), (42.8760, 74.6050), (42.8740, 74.6050)],\n",
    "    'park_ataturk': [(42.8690, 74.5950), (42.8720, 74.5950), (42.8720, 74.6000), (42.8690, 74.6000)],\n",
    "    'botanical_garden': [(42.8560, 74.5560), (42.8620, 74.5560), (42.8620, 74.5660), (42.8560, 74.5660)],\n",
    "}\n",
    "\n",
    "ALA_ARCHA_RIVER = [\n",
    "    (42.7800, 74.5700), (42.8100, 74.5650), (42.8400, 74.5600),\n",
    "    (42.8600, 74.5580), (42.8800, 74.5560), (42.9000, 74.5550),\n",
    "]\n",
    "\n",
    "EARTH_RADIUS_KM = 6371.0\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat, dlon = lat2 - lat1, lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    return R * 2 * np.arcsin(np.sqrt(a))\n",
    "\n",
    "def distance_to_polyline(lat, lon, polyline):\n",
    "    distances = [haversine_distance(lat, lon, p[0], p[1]) for p in polyline]\n",
    "    return min(distances)\n",
    "\n",
    "print(f\"POI: {len(POI)}, Parks: {len(PARKS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# NEW: Spatial Lag Features (v3)\n",
    "# ===================\n",
    "\n",
    "class SpatialLagFeatures:\n",
    "    \"\"\"\n",
    "    Calculate spatial lag features - neighbor price statistics.\n",
    "    This is one of the most impactful features (used by Zillow).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, radius_km: float = 0.5, min_neighbors: int = 3):\n",
    "        self.radius_km = radius_km\n",
    "        self.min_neighbors = min_neighbors\n",
    "        self.tree = None\n",
    "        self.train_prices = None\n",
    "        self.train_indices = None\n",
    "        self._fitted = False\n",
    "    \n",
    "    def fit(self, df: pd.DataFrame, price_col: str = 'price_per_m2'):\n",
    "        coords = np.radians(df[['latitude', 'longitude']].values)\n",
    "        self.tree = BallTree(coords, metric='haversine')\n",
    "        self.train_prices = df[price_col].values\n",
    "        self.train_indices = df.index.values\n",
    "        self._fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if not self._fitted:\n",
    "            raise ValueError(\"Must call fit() first!\")\n",
    "        \n",
    "        df = df.copy()\n",
    "        coords = np.radians(df[['latitude', 'longitude']].values)\n",
    "        radius_rad = self.radius_km / EARTH_RADIUS_KM\n",
    "        \n",
    "        indices_list = self.tree.query_radius(coords, r=radius_rad)\n",
    "        is_train = set(df.index.values) == set(self.train_indices)\n",
    "        \n",
    "        means, medians, stds, counts = [], [], [], []\n",
    "        \n",
    "        for i, neighbor_idx in enumerate(indices_list):\n",
    "            if is_train and len(neighbor_idx) > 0:\n",
    "                current_idx = df.index[i]\n",
    "                mask = self.train_indices[neighbor_idx] != current_idx\n",
    "                neighbor_idx = neighbor_idx[mask]\n",
    "            \n",
    "            neighbor_prices = self.train_prices[neighbor_idx]\n",
    "            \n",
    "            if len(neighbor_prices) >= self.min_neighbors:\n",
    "                means.append(np.mean(neighbor_prices))\n",
    "                medians.append(np.median(neighbor_prices))\n",
    "                stds.append(np.std(neighbor_prices))\n",
    "            else:\n",
    "                means.append(np.nan)\n",
    "                medians.append(np.nan)\n",
    "                stds.append(np.nan)\n",
    "            counts.append(len(neighbor_idx))\n",
    "        \n",
    "        df['neighbor_price_mean'] = means\n",
    "        df['neighbor_price_median'] = medians\n",
    "        df['neighbor_price_std'] = stds\n",
    "        df['neighbor_count'] = counts\n",
    "        \n",
    "        # Fill NaN with global mean\n",
    "        global_mean = np.mean(self.train_prices)\n",
    "        df['neighbor_price_mean'] = df['neighbor_price_mean'].fillna(global_mean)\n",
    "        df['neighbor_price_median'] = df['neighbor_price_median'].fillna(global_mean)\n",
    "        df['neighbor_price_std'] = df['neighbor_price_std'].fillna(0)\n",
    "        \n",
    "        return df\n",
    "\n",
    "print(\"SpatialLagFeatures defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# NEW: H3 Geographic Tiles (v3)\n",
    "# ===================\n",
    "\n",
    "class H3Features:\n",
    "    \"\"\"\n",
    "    Generate Uber H3 hexagonal tile features at multiple resolutions.\n",
    "    Used by Zillow Neural Zestimate for geographic embeddings.\n",
    "    \n",
    "    Resolution guide:\n",
    "    - 7: ~5.16 km² (district level)\n",
    "    - 8: ~0.74 km² (neighborhood level)  \n",
    "    - 9: ~0.11 km² (block level)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, resolutions: List[int] = [7, 8, 9]):\n",
    "        self.resolutions = resolutions\n",
    "        self.encoders = {}\n",
    "        self._fitted = False\n",
    "    \n",
    "    def fit(self, df: pd.DataFrame):\n",
    "        for res in self.resolutions:\n",
    "            h3_indices = df.apply(\n",
    "                lambda r: h3.latlng_to_cell(r['latitude'], r['longitude'], res), axis=1\n",
    "            )\n",
    "            self.encoders[res] = {idx: i for i, idx in enumerate(h3_indices.unique())}\n",
    "        self._fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if not self._fitted:\n",
    "            raise ValueError(\"Must call fit() first!\")\n",
    "        \n",
    "        df = df.copy()\n",
    "        \n",
    "        for res in self.resolutions:\n",
    "            col = f'h3_res{res}'\n",
    "            df[col] = df.apply(\n",
    "                lambda r: h3.latlng_to_cell(r['latitude'], r['longitude'], res), axis=1\n",
    "            )\n",
    "            df[f'{col}_encoded'] = df[col].map(lambda x: self.encoders[res].get(x, -1))\n",
    "        \n",
    "        return df\n",
    "\n",
    "print(\"H3Features defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===================\n# NEW: Market Trend Features (v3) - FIXED (no leakage)\n# ===================\n\nclass MarketTrendFeatures:\n    \"\"\"\n    Calculate market trend features - rolling price statistics.\n    Captures market dynamics over time.\n    \n    NOTE: We do NOT include price_vs_district_zscore as it would use target!\n    \"\"\"\n    \n    def __init__(self, windows: List[int] = [30, 60, 90]):\n        self.windows = windows\n        self.district_stats = {}\n        self.global_mean = 0\n        self.first_date = None\n        self._fitted = False\n    \n    def fit(self, df: pd.DataFrame, price_col: str = 'price_per_m2'):\n        self.global_mean = df[price_col].mean()\n        self.first_date = df['parsed_at'].min()\n        \n        # Store district means for reference (from train only)\n        for district in df['district'].dropna().unique():\n            district_df = df[df['district'] == district]\n            self.district_stats[district] = {\n                'mean': district_df[price_col].mean(),\n                'count': len(district_df),\n            }\n        self._fitted = True\n        return self\n    \n    def transform(self, df: pd.DataFrame, price_col: str = 'price_per_m2') -> pd.DataFrame:\n        if not self._fitted:\n            raise ValueError(\"Must call fit() first!\")\n        \n        df = df.copy().sort_values('parsed_at')\n        \n        # Rolling statistics by district (uses PAST prices only due to sorting)\n        for days in self.windows:\n            col_mean = f'district_price_{days}d_mean'\n            # shift(1) ensures we only use past data, not current row\n            df[col_mean] = df.groupby('district')[price_col].transform(\n                lambda x: x.shift(1).rolling(window=days, min_periods=1).mean()\n            )\n            df[col_mean] = df[col_mean].fillna(self.global_mean)\n        \n        # Days since first listing (no leakage - just temporal feature)\n        df['days_on_market'] = (df['parsed_at'] - self.first_date).dt.days\n        \n        # District activity (count of listings - no price info)\n        df['district_listing_count'] = df['district'].map(\n            lambda d: self.district_stats.get(d, {}).get('count', 0)\n        )\n        \n        return df\n\nprint(\"MarketTrendFeatures defined (FIXED - no leakage)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# NEW: Density Features (v3)\n",
    "# ===================\n",
    "\n",
    "class DensityFeatures:\n",
    "    \"\"\"\n",
    "    Calculate listing density - proxy for market activity and desirability.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, radii_km: List[float] = [0.5, 1.0]):\n",
    "        self.radii_km = radii_km\n",
    "        self.tree = None\n",
    "        self.avg_density = {}\n",
    "        self._fitted = False\n",
    "    \n",
    "    def fit(self, df: pd.DataFrame):\n",
    "        coords = np.radians(df[['latitude', 'longitude']].values)\n",
    "        self.tree = BallTree(coords, metric='haversine')\n",
    "        \n",
    "        for radius in self.radii_km:\n",
    "            radius_rad = radius / EARTH_RADIUS_KM\n",
    "            counts = self.tree.query_radius(coords, r=radius_rad, count_only=True)\n",
    "            self.avg_density[radius] = np.mean(counts)\n",
    "        \n",
    "        self._fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if not self._fitted:\n",
    "            raise ValueError(\"Must call fit() first!\")\n",
    "        \n",
    "        df = df.copy()\n",
    "        coords = np.radians(df[['latitude', 'longitude']].values)\n",
    "        \n",
    "        for radius in self.radii_km:\n",
    "            radius_rad = radius / EARTH_RADIUS_KM\n",
    "            counts = self.tree.query_radius(coords, r=radius_rad, count_only=True)\n",
    "            \n",
    "            col = f'listings_{int(radius * 1000)}m'\n",
    "            df[col] = counts\n",
    "            \n",
    "            avg = max(self.avg_density[radius], 1)\n",
    "            df[f'{col}_ratio'] = counts / avg\n",
    "        \n",
    "        return df\n",
    "\n",
    "print(\"DensityFeatures defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# Basic Feature Engineer (from v2)\n",
    "# ===================\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fitted = False\n",
    "        self.medians = {}\n",
    "        \n",
    "    def fit(self, df: pd.DataFrame, target_col: str):\n",
    "        self.medians = {\n",
    "            'floor': df['floor'].median(),\n",
    "            'total_floors': df['total_floors'].median(),\n",
    "            'year_built': df['year_built'].median(),\n",
    "            'rooms': df['rooms'].median(),\n",
    "            'kitchen_ratio': (df['kitchen_area'] / df['area']).median(),\n",
    "            'ceiling_height': self._parse_ceiling_series(df['ceiling_height']).median(),\n",
    "            'latitude': df['latitude'].median(),\n",
    "            'longitude': df['longitude'].median(),\n",
    "        }\n",
    "        self.global_target_mean = df[target_col].mean()\n",
    "        self.fitted = True\n",
    "        return self\n",
    "    \n",
    "    def _parse_ceiling_series(self, series: pd.Series) -> pd.Series:\n",
    "        def parse_one(val):\n",
    "            if pd.isna(val): return np.nan\n",
    "            val = str(val).replace('м', '').replace(',', '.').strip()\n",
    "            try: return float(val)\n",
    "            except: return np.nan\n",
    "        return series.apply(parse_one)\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Must call fit() first!\")\n",
    "        \n",
    "        df = df.copy()\n",
    "        \n",
    "        # Impute missing values\n",
    "        df['floor'] = df['floor'].fillna(self.medians['floor'])\n",
    "        df['total_floors'] = df['total_floors'].fillna(self.medians['total_floors'])\n",
    "        df['year_built'] = df['year_built'].fillna(self.medians['year_built'])\n",
    "        df['rooms'] = df['rooms'].fillna(self.medians['rooms'])\n",
    "        df['latitude'] = df['latitude'].fillna(self.medians['latitude'])\n",
    "        df['longitude'] = df['longitude'].fillna(self.medians['longitude'])\n",
    "        \n",
    "        # Floor features\n",
    "        df['floor_ratio'] = df['floor'] / df['total_floors'].replace(0, 1)\n",
    "        df['is_first_floor'] = (df['floor'] == 1).astype(int)\n",
    "        df['is_last_floor'] = (df['floor'] == df['total_floors']).astype(int)\n",
    "        \n",
    "        # Building features\n",
    "        df['building_age'] = CURRENT_YEAR - df['year_built']\n",
    "        df['is_new_building'] = (df['year_built'] >= CURRENT_YEAR - 7).astype(int)\n",
    "        df['is_soviet'] = (df['year_built'] < 1991).astype(int)\n",
    "        df['is_highrise'] = (df['total_floors'] >= 9).astype(int)\n",
    "        \n",
    "        # Area features\n",
    "        df['area_per_room'] = df['area'] / df['rooms'].replace(0, 1)\n",
    "        df['is_large'] = (df['area'] >= 100).astype(int)\n",
    "        df['is_studio'] = (df['rooms'] <= 1).astype(int)\n",
    "        \n",
    "        # Kitchen\n",
    "        df['kitchen_area_num'] = pd.to_numeric(df['kitchen_area'], errors='coerce')\n",
    "        df['kitchen_ratio'] = (df['kitchen_area_num'] / df['area']).clip(0, 0.5)\n",
    "        df['kitchen_missing'] = df['kitchen_ratio'].isna().astype(int)\n",
    "        df['kitchen_ratio'] = df['kitchen_ratio'].fillna(self.medians['kitchen_ratio'])\n",
    "        \n",
    "        # Ceiling\n",
    "        df['ceiling_height_m'] = self._parse_ceiling_series(df['ceiling_height'])\n",
    "        df['ceiling_missing'] = df['ceiling_height_m'].isna().astype(int)\n",
    "        df['ceiling_height_m'] = df['ceiling_height_m'].fillna(self.medians['ceiling_height'])\n",
    "        df['high_ceiling'] = (df['ceiling_height_m'] >= 3.0).astype(int)\n",
    "        \n",
    "        # Condition\n",
    "        condition_map = {'евроремонт': 4, 'хороший': 3, 'средний': 2, \n",
    "                        'черновая отделка': 1, 'требует ремонта': 0}\n",
    "        df['condition_score'] = df['condition'].map(condition_map).fillna(2)\n",
    "        df['condition_missing'] = (~df['condition'].isin(condition_map.keys())).astype(int)\n",
    "        \n",
    "        # Categorical binary features\n",
    "        df['has_separate_bathroom'] = df['bathroom'].str.contains('раздельн', na=False).astype(int)\n",
    "        df['has_balcony'] = df['balcony'].notna().astype(int)\n",
    "        df['has_parking'] = df['parking'].notna().astype(int)\n",
    "        df['has_furniture'] = df['furniture'].notna().astype(int)\n",
    "        df['is_parquet'] = df['floor_type'].str.contains('паркет', na=False).astype(int)\n",
    "        \n",
    "        # Security\n",
    "        df['security_score'] = (\n",
    "            df['security'].str.contains('охран', na=False).astype(int) * 2 +\n",
    "            df['security'].str.contains('видео', na=False).astype(int) +\n",
    "            df['security'].str.contains('домофон', na=False).astype(int)\n",
    "        )\n",
    "        \n",
    "        # House type\n",
    "        df['is_monolith'] = (df['house_type'] == 'монолитный').astype(int)\n",
    "        df['is_brick'] = (df['house_type'] == 'кирпичный').astype(int)\n",
    "        df['is_panel'] = (df['house_type'] == 'панельный').astype(int)\n",
    "        \n",
    "        # JK features\n",
    "        df['has_jk'] = df['jk_name'].notna().astype(int)\n",
    "        jk_class_map = {'эконом': 1, 'комфорт': 2, 'бизнес': 3, 'премиум': 4, 'элит': 4}\n",
    "        df['jk_class_score'] = df['jk_class'].map(jk_class_map).fillna(0)\n",
    "        df['jk_completed'] = (df['jk_status'] == 'completed').astype(int)\n",
    "        \n",
    "        # Geo features (POI distances)\n",
    "        lats = df['latitude'].values\n",
    "        lons = df['longitude'].values\n",
    "        \n",
    "        for poi_name, (poi_lat, poi_lon) in POI.items():\n",
    "            df[f'dist_{poi_name}'] = haversine_distance(lats, lons, poi_lat, poi_lon)\n",
    "        \n",
    "        # Aggregated distances\n",
    "        mall_cols = [f'dist_{p}' for p in ['dordoi_plaza', 'bishkek_park', 'tsum', 'vefa_center', 'asia_mall', 'karavan']]\n",
    "        df['dist_nearest_mall'] = df[mall_cols].min(axis=1)\n",
    "        \n",
    "        transport_cols = [f'dist_{p}' for p in ['west_bus_station', 'east_bus_station', 'railway_station']]\n",
    "        df['dist_nearest_transport'] = df[transport_cols].min(axis=1)\n",
    "        \n",
    "        bazaar_cols = [f'dist_{p}' for p in ['osh_bazaar', 'dordoi_bazaar', 'ortosay_bazaar']]\n",
    "        df['dist_nearest_bazaar'] = df[bazaar_cols].min(axis=1)\n",
    "        \n",
    "        # Parks\n",
    "        for park_name, polygon in PARKS.items():\n",
    "            centroid = (np.mean([p[0] for p in polygon]), np.mean([p[1] for p in polygon]))\n",
    "            df[f'dist_{park_name}'] = haversine_distance(lats, lons, centroid[0], centroid[1])\n",
    "        \n",
    "        park_cols = [f'dist_{p}' for p in PARKS.keys()]\n",
    "        df['dist_nearest_park'] = df[park_cols].min(axis=1)\n",
    "        \n",
    "        # River distance\n",
    "        df['dist_river'] = df.apply(\n",
    "            lambda r: distance_to_polyline(r['latitude'], r['longitude'], ALA_ARCHA_RIVER), axis=1\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def fit_transform(self, df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n",
    "        return self.fit(df, target_col).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# Target Encoding with K-Fold CV\n",
    "# ===================\n",
    "\n",
    "class TargetEncoderCV:\n",
    "    \n",
    "    def __init__(self, cols: List[str], min_samples: int = 30, n_folds: int = 5):\n",
    "        self.cols = cols\n",
    "        self.min_samples = min_samples\n",
    "        self.n_folds = n_folds\n",
    "        self.encodings = {}\n",
    "        self.global_mean = None\n",
    "        \n",
    "    def fit_transform(self, df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        self.global_mean = df[target_col].mean()\n",
    "        \n",
    "        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "        \n",
    "        for col in self.cols:\n",
    "            new_col = f'{col}_target_enc'\n",
    "            df[new_col] = np.nan\n",
    "            \n",
    "            for train_idx, val_idx in kf.split(df):\n",
    "                train_fold = df.iloc[train_idx]\n",
    "                encoding = self._compute_encoding(train_fold, col, target_col)\n",
    "                df.iloc[val_idx, df.columns.get_loc(new_col)] = (\n",
    "                    df.iloc[val_idx][col].map(encoding).fillna(self.global_mean)\n",
    "                )\n",
    "            \n",
    "            self.encodings[col] = self._compute_encoding(df, col, target_col)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        for col in self.cols:\n",
    "            new_col = f'{col}_target_enc'\n",
    "            df[new_col] = df[col].map(self.encodings[col]).fillna(self.global_mean)\n",
    "        return df\n",
    "    \n",
    "    def _compute_encoding(self, df: pd.DataFrame, col: str, target_col: str) -> Dict:\n",
    "        agg = df.groupby(col)[target_col].agg(['mean', 'count'])\n",
    "        smoothing = agg['count'] / (agg['count'] + self.min_samples)\n",
    "        encoding = smoothing * agg['mean'] + (1 - smoothing) * self.global_mean\n",
    "        return encoding.to_dict()\n",
    "\n",
    "print(\"TargetEncoderCV defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# Apply ALL Feature Engineering\n",
    "# ===================\n",
    "\n",
    "print(\"Applying feature engineering pipeline...\\n\")\n",
    "\n",
    "# 1. Basic features\n",
    "print(\"1. Basic features...\")\n",
    "feature_engineer = FeatureEngineer()\n",
    "train_df = feature_engineer.fit_transform(train_df, target)\n",
    "test_df = feature_engineer.transform(test_df)\n",
    "print(f\"   Columns: {train_df.shape[1]}\")\n",
    "\n",
    "# 2. Spatial Lag (NEW v3)\n",
    "print(\"2. Spatial lag features (neighbor prices)...\")\n",
    "spatial_lag = SpatialLagFeatures(radius_km=0.5, min_neighbors=3)\n",
    "spatial_lag.fit(train_df, target)\n",
    "train_df = spatial_lag.transform(train_df)\n",
    "test_df = spatial_lag.transform(test_df)\n",
    "print(f\"   Added: neighbor_price_mean, neighbor_price_median, neighbor_price_std, neighbor_count\")\n",
    "\n",
    "# 3. H3 Geographic Tiles (NEW v3)\n",
    "print(\"3. H3 geographic tiles...\")\n",
    "h3_features = H3Features(resolutions=[7, 8, 9])\n",
    "h3_features.fit(train_df)\n",
    "train_df = h3_features.transform(train_df)\n",
    "test_df = h3_features.transform(test_df)\n",
    "print(f\"   Added: h3_res7_encoded, h3_res8_encoded, h3_res9_encoded\")\n",
    "\n",
    "# 4. Market Trends (NEW v3)\n",
    "print(\"4. Market trend features...\")\n",
    "market_trends = MarketTrendFeatures(windows=[30, 60, 90])\n",
    "market_trends.fit(train_df, target)\n",
    "train_df = market_trends.transform(train_df, target)\n",
    "test_df = market_trends.transform(test_df, target)\n",
    "print(f\"   Added: district_price_30/60/90d_mean, price_vs_district_zscore, days_on_market\")\n",
    "\n",
    "# 5. Density (NEW v3)\n",
    "print(\"5. Density features...\")\n",
    "density = DensityFeatures(radii_km=[0.5, 1.0])\n",
    "density.fit(train_df)\n",
    "train_df = density.transform(train_df)\n",
    "test_df = density.transform(test_df)\n",
    "print(f\"   Added: listings_500m, listings_1000m, listings_500m_ratio, listings_1000m_ratio\")\n",
    "\n",
    "# 6. Target encoding\n",
    "print(\"6. Target encoding (CV-based)...\")\n",
    "target_encoder = TargetEncoderCV(cols=['district', 'jk_name'], min_samples=30, n_folds=5)\n",
    "train_df = target_encoder.fit_transform(train_df, target)\n",
    "test_df = target_encoder.transform(test_df)\n",
    "print(f\"   Added: district_target_enc, jk_name_target_enc\")\n",
    "\n",
    "print(f\"\\nFinal train shape: {train_df.shape}\")\n",
    "print(f\"Final test shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===================\n# Define Feature Groups (v3 - with new features, NO LEAKAGE)\n# ===================\n\nFEATURE_GROUPS = {\n    'core': ['rooms', 'area', 'floor_ratio', 'building_age'],\n    \n    'building': ['is_new_building', 'is_soviet', 'is_highrise', 'total_floors'],\n    \n    'apartment': [\n        'area_per_room', 'is_large', 'is_studio',\n        'kitchen_ratio', 'kitchen_missing',\n        'ceiling_height_m', 'ceiling_missing', 'high_ceiling',\n        'condition_score', 'condition_missing',\n    ],\n    \n    'amenities': [\n        'has_separate_bathroom', 'has_balcony', 'has_parking',\n        'has_furniture', 'is_parquet', 'security_score',\n    ],\n    \n    'house_type': ['is_monolith', 'is_brick', 'is_panel'],\n    \n    'jk': ['has_jk', 'jk_class_score', 'jk_completed'],\n    \n    'target_encoding': ['district_target_enc', 'jk_name_target_enc'],\n    \n    'location': [\n        'dist_center', 'dist_nearest_mall', 'dist_nearest_transport',\n        'dist_nearest_bazaar', 'dist_nearest_park', 'dist_river',\n        'latitude', 'longitude',\n    ],\n    \n    # NEW v3 features (NO LEAKAGE)\n    'spatial_lag': [\n        'neighbor_price_mean', 'neighbor_price_median', \n        'neighbor_price_std', 'neighbor_count',\n    ],\n    \n    'h3_tiles': [\n        'h3_res7_encoded', 'h3_res8_encoded', 'h3_res9_encoded',\n    ],\n    \n    'market_trends': [\n        'district_price_30d_mean', 'district_price_60d_mean', 'district_price_90d_mean',\n        'days_on_market', 'district_listing_count',\n        # REMOVED: 'price_vs_district_zscore' - this was leaking target!\n    ],\n    \n    'density': [\n        'listings_500m', 'listings_1000m',\n        'listings_500m_ratio', 'listings_1000m_ratio',\n    ],\n}\n\nALL_FEATURES = [f for group in FEATURE_GROUPS.values() for f in group]\navailable_features = [f for f in ALL_FEATURES if f in train_df.columns]\n\nprint(f\"Features defined: {len(ALL_FEATURES)}\")\nprint(f\"Features available: {len(available_features)}\")\nprint(f\"\\nNEW v3 features: {len(FEATURE_GROUPS['spatial_lag']) + len(FEATURE_GROUPS['h3_tiles']) + len(FEATURE_GROUPS['market_trends']) + len(FEATURE_GROUPS['density'])}\")\nprint(\"NOTE: price_vs_district_zscore REMOVED (was leaking target)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# Prepare Final Data\n",
    "# ===================\n",
    "\n",
    "final_features = available_features\n",
    "\n",
    "X_train = train_df[final_features].fillna(0).values\n",
    "X_test = test_df[final_features].fillna(0).values\n",
    "y_train = train_df[target].values\n",
    "y_test = test_df[target].values\n",
    "\n",
    "# Spatial groups for CV\n",
    "def create_spatial_groups(df: pd.DataFrame, precision: int = 3) -> np.ndarray:\n",
    "    lat_round = df['latitude'].round(precision).astype(str)\n",
    "    lon_round = df['longitude'].round(precision).astype(str)\n",
    "    group_str = lat_round + '_' + lon_round\n",
    "    le = LabelEncoder()\n",
    "    return le.fit_transform(group_str.fillna('unknown'))\n",
    "\n",
    "train_groups = create_spatial_groups(train_df)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"Features: {len(final_features)}\")\n",
    "print(f\"Spatial groups for CV: {len(np.unique(train_groups))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training & Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# Evaluation Metrics\n",
    "# ===================\n",
    "\n",
    "def evaluate_predictions(y_true: np.ndarray, y_pred: np.ndarray, name: str = \"\") -> Dict:\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    medae = np.median(np.abs(y_true - y_pred))\n",
    "    mape = np.median(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    within_10pct = np.mean(np.abs((y_true - y_pred) / y_true) <= 0.10) * 100\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R²': r2,\n",
    "        'MedAE': medae,\n",
    "        'MedAPE%': mape,\n",
    "        'Within10%': within_10pct,\n",
    "    }\n",
    "\n",
    "def print_metrics(metrics: Dict):\n",
    "    print(f\"  MAE:       ${metrics['MAE']:,.0f}/m²\")\n",
    "    print(f\"  RMSE:      ${metrics['RMSE']:,.0f}/m²\")\n",
    "    print(f\"  R²:        {metrics['R²']:.4f}\")\n",
    "    print(f\"  MedAE:     ${metrics['MedAE']:,.0f}/m²\")\n",
    "    print(f\"  MedAPE:    {metrics['MedAPE%']:.1f}%\")\n",
    "    print(f\"  Within 10%: {metrics['Within10%']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# Train Base Models\n",
    "# ===================\n",
    "\n",
    "print(\"Training base models...\\n\")\n",
    "\n",
    "base_models = {\n",
    "    'XGBoost': XGBRegressor(\n",
    "        n_estimators=300, max_depth=8, learning_rate=0.05,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        random_state=RANDOM_STATE, n_jobs=-1\n",
    "    ),\n",
    "    'LightGBM': LGBMRegressor(\n",
    "        n_estimators=300, max_depth=8, learning_rate=0.05,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        random_state=RANDOM_STATE, verbose=-1, n_jobs=-1\n",
    "    ),\n",
    "    'CatBoost': CatBoostRegressor(\n",
    "        n_estimators=300, max_depth=8, learning_rate=0.05,\n",
    "        random_state=RANDOM_STATE, verbose=0\n",
    "    ),\n",
    "}\n",
    "\n",
    "results = []\n",
    "trained_models = {}\n",
    "oof_predictions = {}\n",
    "\n",
    "for name, model in base_models.items():\n",
    "    print(f\"{name}:\")\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # OOF predictions\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    oof = cross_val_predict(model, X_train, y_train, cv=gkf, groups=train_groups, n_jobs=-1)\n",
    "    oof_predictions[name] = oof\n",
    "    \n",
    "    metrics = evaluate_predictions(y_test, y_pred_test, name)\n",
    "    results.append(metrics)\n",
    "    print_metrics(metrics)\n",
    "    print()\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('MAE')\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASE MODELS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# Stacking Ensemble\n",
    "# ===================\n",
    "\n",
    "print(\"\\nTraining Stacking Ensemble...\")\n",
    "\n",
    "stack_train = np.column_stack([oof_predictions[name] for name in base_models.keys()])\n",
    "stack_test = np.column_stack([trained_models[name].predict(X_test) for name in base_models.keys()])\n",
    "\n",
    "meta_model = Ridge(alpha=1.0)\n",
    "meta_model.fit(stack_train, y_train)\n",
    "\n",
    "y_pred_ensemble = meta_model.predict(stack_test)\n",
    "\n",
    "ensemble_metrics = evaluate_predictions(y_test, y_pred_ensemble, 'Stacking Ensemble')\n",
    "print(\"\\nStacking Ensemble Results:\")\n",
    "print_metrics(ensemble_metrics)\n",
    "\n",
    "# Simple average\n",
    "y_pred_avg = np.mean([trained_models[name].predict(X_test) for name in base_models.keys()], axis=0)\n",
    "avg_metrics = evaluate_predictions(y_test, y_pred_avg, 'Average Ensemble')\n",
    "print(\"\\nSimple Average Ensemble Results:\")\n",
    "print_metrics(avg_metrics)\n",
    "\n",
    "# Choose best\n",
    "if avg_metrics['MAE'] < ensemble_metrics['MAE']:\n",
    "    print(\"\\n>>> Using Simple Average (better performance)\")\n",
    "    final_predictions = y_pred_avg\n",
    "    final_method = 'average'\n",
    "else:\n",
    "    print(\"\\n>>> Using Stacking Ensemble (better performance)\")\n",
    "    final_predictions = y_pred_ensemble\n",
    "    final_method = 'stacking'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prediction Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# Quantile Models for Prediction Intervals\n",
    "# ===================\n",
    "\n",
    "print(\"Training quantile models...\")\n",
    "\n",
    "quantile_models = {}\n",
    "\n",
    "for q, name in [(0.10, 'q10'), (0.50, 'q50'), (0.90, 'q90')]:\n",
    "    model = LGBMRegressor(\n",
    "        objective='quantile', alpha=q,\n",
    "        n_estimators=200, max_depth=8, learning_rate=0.05,\n",
    "        random_state=RANDOM_STATE, verbose=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    quantile_models[name] = model\n",
    "\n",
    "pred_q10 = quantile_models['q10'].predict(X_test)\n",
    "pred_q50 = quantile_models['q50'].predict(X_test)\n",
    "pred_q90 = quantile_models['q90'].predict(X_test)\n",
    "\n",
    "coverage = np.mean((y_test >= pred_q10) & (y_test <= pred_q90)) * 100\n",
    "avg_interval = np.mean(pred_q90 - pred_q10)\n",
    "\n",
    "print(f\"\\nPrediction Intervals (80% CI):\")\n",
    "print(f\"  Coverage: {coverage:.1f}% (target: 80%)\")\n",
    "print(f\"  Avg width: ${avg_interval:,.0f}/m²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# Permutation Importance\n",
    "# ===================\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "print(\"Computing permutation importance...\")\n",
    "\n",
    "perm_importance = permutation_importance(\n",
    "    trained_models['XGBoost'], X_test, y_test,\n",
    "    n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': final_features,\n",
    "    'importance_mean': perm_importance.importances_mean,\n",
    "    'importance_std': perm_importance.importances_std,\n",
    "}).sort_values('importance_mean', ascending=False)\n",
    "\n",
    "# Mark new v3 features\n",
    "new_features = (\n",
    "    FEATURE_GROUPS['spatial_lag'] + \n",
    "    FEATURE_GROUPS['h3_tiles'] + \n",
    "    FEATURE_GROUPS['market_trends'] + \n",
    "    FEATURE_GROUPS['density']\n",
    ")\n",
    "importance_df['is_v3_new'] = importance_df['feature'].isin(new_features)\n",
    "\n",
    "print(\"\\nTop 20 Features:\")\n",
    "print(importance_df.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# NEW v3 Features Impact Analysis\n",
    "# ===================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEW v3 FEATURES ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "v3_importance = importance_df[importance_df['is_v3_new']].sort_values('importance_mean', ascending=False)\n",
    "v3_total_importance = v3_importance['importance_mean'].sum()\n",
    "total_importance = importance_df['importance_mean'].sum()\n",
    "\n",
    "print(f\"\\nv3 features contribution: {v3_total_importance/total_importance*100:.1f}% of total importance\")\n",
    "print(f\"\\nTop v3 features:\")\n",
    "print(v3_importance[['feature', 'importance_mean']].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Top 15 overall\n",
    "top15 = importance_df.head(15)\n",
    "colors = ['#ff7f0e' if is_new else '#1f77b4' for is_new in top15['is_v3_new']]\n",
    "axes[0].barh(range(15), top15['importance_mean'], color=colors)\n",
    "axes[0].set_yticks(range(15))\n",
    "axes[0].set_yticklabels(top15['feature'])\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].set_title('Top 15 Features (orange = NEW v3)')\n",
    "\n",
    "# v3 features only\n",
    "v3_top = v3_importance.head(10)\n",
    "axes[1].barh(range(len(v3_top)), v3_top['importance_mean'], color='#ff7f0e')\n",
    "axes[1].set_yticks(range(len(v3_top)))\n",
    "axes[1].set_yticklabels(v3_top['feature'])\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_title('NEW v3 Features Only')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. v2 vs v3 Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# FINAL COMPARISON: v2 vs v3\n",
    "# ===================\n",
    "\n",
    "final_metrics = evaluate_predictions(y_test, final_predictions, 'v3 Final')\n",
    "\n",
    "# v2 baseline (from previous run)\n",
    "v2_metrics = {\n",
    "    'MAE': 144,\n",
    "    'MedAE': 103,\n",
    "    'R²': 0.668,\n",
    "    'MedAPE%': 7.0,\n",
    "    'Within10%': 64.9,\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL COMPARISON: v2 vs v3\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Metric':<15} {'v2':<15} {'v3':<15} {'Change':<15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'MAE':<15} ${v2_metrics['MAE']:<14,.0f} ${final_metrics['MAE']:<14,.0f} {(final_metrics['MAE']-v2_metrics['MAE'])/v2_metrics['MAE']*100:+.1f}%\")\n",
    "print(f\"{'MedAE':<15} ${v2_metrics['MedAE']:<14,.0f} ${final_metrics['MedAE']:<14,.0f} {(final_metrics['MedAE']-v2_metrics['MedAE'])/v2_metrics['MedAE']*100:+.1f}%\")\n",
    "print(f\"{'R²':<15} {v2_metrics['R²']:<15.3f} {final_metrics['R²']:<15.3f} {(final_metrics['R²']-v2_metrics['R²'])/v2_metrics['R²']*100:+.1f}%\")\n",
    "print(f\"{'MedAPE%':<15} {v2_metrics['MedAPE%']:<15.1f} {final_metrics['MedAPE%']:<15.1f} {(final_metrics['MedAPE%']-v2_metrics['MedAPE%'])/v2_metrics['MedAPE%']*100:+.1f}%\")\n",
    "print(f\"{'Within 10%':<15} {v2_metrics['Within10%']:<15.1f} {final_metrics['Within10%']:<15.1f} {(final_metrics['Within10%']-v2_metrics['Within10%'])/v2_metrics['Within10%']*100:+.1f}%\")\n",
    "print(f\"{'CI Coverage':<15} {'72.9%':<15} {coverage:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# Save Model\n",
    "# ===================\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "model_artifacts = {\n",
    "    'version': 3,\n",
    "    'base_models': trained_models,\n",
    "    'meta_model': meta_model if final_method == 'stacking' else None,\n",
    "    'ensemble_method': final_method,\n",
    "    'quantile_models': quantile_models,\n",
    "    'feature_engineer': feature_engineer,\n",
    "    'spatial_lag': spatial_lag,\n",
    "    'h3_features': h3_features,\n",
    "    'market_trends': market_trends,\n",
    "    'density': density,\n",
    "    'target_encoder': target_encoder,\n",
    "    'features': final_features,\n",
    "}\n",
    "\n",
    "joblib.dump(model_artifacts, 'bishkek_model_v3.joblib')\n",
    "print(\"Model saved: bishkek_model_v3.joblib\")\n",
    "\n",
    "config = {\n",
    "    'version': 3,\n",
    "    'features': final_features,\n",
    "    'new_v3_features': new_features,\n",
    "    'ensemble_method': final_method,\n",
    "    'metrics': {\n",
    "        'mae': float(final_metrics['MAE']),\n",
    "        'rmse': float(final_metrics['RMSE']),\n",
    "        'r2': float(final_metrics['R²']),\n",
    "        'medae': float(final_metrics['MedAE']),\n",
    "        'medape': float(final_metrics['MedAPE%']),\n",
    "        'within_10pct': float(final_metrics['Within10%']),\n",
    "        'ci_coverage': float(coverage),\n",
    "    },\n",
    "    'v2_comparison': {\n",
    "        'mae_improvement': f\"{(v2_metrics['MAE']-final_metrics['MAE'])/v2_metrics['MAE']*100:.1f}%\",\n",
    "        'medape_improvement': f\"{(v2_metrics['MedAPE%']-final_metrics['MedAPE%'])/v2_metrics['MedAPE%']*100:.1f}%\",\n",
    "        'r2_improvement': f\"{(final_metrics['R²']-v2_metrics['R²'])/v2_metrics['R²']*100:.1f}%\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('bishkek_model_v3_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(\"Config saved: bishkek_model_v3_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# FINAL SUMMARY\n",
    "# ===================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BISHKEK REAL ESTATE PRICE PREDICTION v3 - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n[DATA]\")\n",
    "print(f\"  Total samples: {len(df):,}\")\n",
    "print(f\"  Train: {len(train_df):,} | Test: {len(test_df):,}\")\n",
    "\n",
    "print(f\"\\n[NEW v3 FEATURES]\")\n",
    "print(f\"  Spatial Lag: neighbor_price_mean/median/std, neighbor_count\")\n",
    "print(f\"  H3 Tiles: h3_res7/8/9_encoded (Uber H3)\")\n",
    "print(f\"  Market Trends: district_price_30/60/90d_mean, price_vs_district_zscore\")\n",
    "print(f\"  Density: listings_500m/1000m, listings_ratio\")\n",
    "print(f\"  Total new features: {len(new_features)}\")\n",
    "\n",
    "print(f\"\\n[PERFORMANCE]\")\n",
    "print(f\"  MAE:        ${final_metrics['MAE']:,.0f}/m²\")\n",
    "print(f\"  MedAE:      ${final_metrics['MedAE']:,.0f}/m²\")\n",
    "print(f\"  R²:         {final_metrics['R²']:.3f}\")\n",
    "print(f\"  MedAPE:     {final_metrics['MedAPE%']:.1f}%\")\n",
    "print(f\"  Within 10%: {final_metrics['Within10%']:.1f}%\")\n",
    "\n",
    "print(f\"\\n[IMPROVEMENT vs v2]\")\n",
    "mae_change = (v2_metrics['MAE'] - final_metrics['MAE']) / v2_metrics['MAE'] * 100\n",
    "mape_change = (v2_metrics['MedAPE%'] - final_metrics['MedAPE%']) / v2_metrics['MedAPE%'] * 100\n",
    "r2_change = (final_metrics['R²'] - v2_metrics['R²']) / v2_metrics['R²'] * 100\n",
    "print(f\"  MAE:    {mae_change:+.1f}% {'(better)' if mae_change > 0 else ''}\")\n",
    "print(f\"  MedAPE: {mape_change:+.1f}% {'(better)' if mape_change > 0 else ''}\")\n",
    "print(f\"  R²:     {r2_change:+.1f}% {'(better)' if r2_change > 0 else ''}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}