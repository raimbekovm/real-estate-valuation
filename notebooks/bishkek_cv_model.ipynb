{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14448775,"sourceType":"datasetVersion","datasetId":9203618},{"sourceId":14453297,"sourceType":"datasetVersion","datasetId":9231639}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Bishkek Real Estate: v3 + Computer Vision\n#\nThis notebook combines:\n- **v3 Baseline**: 39 features, POI distances, Optuna tuning (MAE $122, R² 0.76)\n- **CV Embeddings**: ResNet-50 image features (64 PCA components)\n#\nBased on research:\n- [MHPP (arXiv 2024)](https://arxiv.org/abs/2409.05335): +21-26% MAE improvement with images\n- [PLOS One 2025](https://pmc.ncbi.nlm.nih.gov/articles/PMC12088074/): ResNet-101 + t-SNE\n#\nExpected improvement: MAE $122 → $100-110 (-10% to -18%)\n\n","metadata":{}},{"cell_type":"code","source":"# Install dependencies\nimport subprocess\nimport sys\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"optuna\"])\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport warnings\nimport gc\nimport shutil\nwarnings.filterwarnings('ignore')\n\n# ML\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.linear_model import Ridge\n\n# Boosting models\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\n# Optuna for hyperparameter tuning\ntry:\n    import optuna\n    from optuna.integration import XGBoostPruningCallback, LightGBMPruningCallback\n    OPTUNA_AVAILABLE = True\n    print(\"Optuna available for hyperparameter tuning\")\nexcept ImportError:\n    OPTUNA_AVAILABLE = False\n    print(\"Optuna not available\")\n\n# For image processing\ntry:\n    import torch\n    import torchvision.models as models\n    import torchvision.transforms as transforms\n    from PIL import Image\n    TORCH_AVAILABLE = True\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"PyTorch available, device: {DEVICE}\")\n    if torch.cuda.is_available():\n        print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\nexcept ImportError:\n    TORCH_AVAILABLE = False\n    DEVICE = None\n    print(\"PyTorch not available\")\n\nprint(f\"\\nSetup complete!\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T18:47:43.048230Z","iopub.execute_input":"2026-01-10T18:47:43.048492Z","iopub.status.idle":"2026-01-10T18:48:04.535732Z","shell.execute_reply.started":"2026-01-10T18:47:43.048469Z","shell.execute_reply":"2026-01-10T18:48:04.535014Z"}},"outputs":[{"name":"stdout","text":"Optuna available for hyperparameter tuning\nPyTorch available, device: cuda\n  GPU: Tesla T4\n\nSetup complete!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## POI (Points of Interest)\n\n","metadata":{}},{"cell_type":"code","source":"from math import radians, sin, cos, sqrt, atan2\n\n# POI Bishkek - key locations by category\nBISHKEK_POI = {\n    'bazaars': [\n        ('osh_bazaar', 42.874823, 74.569599),\n        ('dordoi_bazaar', 42.939732, 74.620613),\n        ('ortosay_bazaar', 42.836209, 74.615931),\n        ('alamedin_bazaar', 42.88683, 74.637305),\n    ],\n    'parks': [\n        ('dubovy_park', 42.877681, 74.606759),\n        ('ataturk_park', 42.839587, 74.595725),\n        ('karagach_grove', 42.900362, 74.619652),\n        ('victory_park', 42.872456, 74.615523),\n        ('botanical_garden', 42.829413, 74.616985),\n    ],\n    'malls': [\n        ('bishkek_park', 42.871234, 74.593345),\n        ('dordoi_plaza', 42.878456, 74.618234),\n        ('vefa_center', 42.854123, 74.612345),\n        ('tsum', 42.874234, 74.600123),\n    ],\n    'universities': [\n        ('auca', 42.824891, 74.618307),\n        ('krsu', 42.873917, 74.595389),\n        ('bgu', 42.875423, 74.613456),\n        ('knu', 42.872345, 74.603456),\n    ],\n    'hospitals': [\n        ('national_hospital', 42.873456, 74.621234),\n        ('city_hospital_1', 42.856789, 74.598765),\n        ('republican_hospital', 42.869012, 74.615678),\n    ],\n    'transport': [\n        ('west_bus_station', 42.874567, 74.554321),\n        ('east_bus_station', 42.873456, 74.650123),\n        ('railway_station', 42.871234, 74.573456),\n    ],\n    'admin': [\n        ('jogorku_kenesh', 42.874567, 74.604321),\n        ('erkindik', 42.872345, 74.599876),\n    ],\n}\n\nBISHKEK_PREMIUM_ZONES = {\n    'erkindik_south': (42.867, 74.600),\n    'center': (42.874, 74.600),\n    'filarmonia': (42.872, 74.616),\n    'tsum_area': (42.876, 74.602),\n    'vefa_area': (42.862, 74.618),\n}\n\nBISHKEK_CENTER = (42.874621, 74.569199)\n\ndef haversine_distance(lat1, lon1, lat2, lon2):\n    \"\"\"Calculate distance between two points in km\"\"\"\n    R = 6371\n    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n    c = 2 * atan2(sqrt(a), sqrt(1-a))\n    return R * c\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T18:48:04.537733Z","iopub.execute_input":"2026-01-10T18:48:04.538372Z","iopub.status.idle":"2026-01-10T18:48:04.546666Z","shell.execute_reply.started":"2026-01-10T18:48:04.538343Z","shell.execute_reply":"2026-01-10T18:48:04.545998Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Load Data\n\n","metadata":{}},{"cell_type":"code","source":"# Load data from Kaggle\ndf = pd.read_csv('/kaggle/input/bishkek-real-estate-2025/bishkek_apartments.csv')\n\nprint(f\"Dataset: {len(df)} apartments, {len(df.columns)} columns\")\nprint(f\"year_built filled: {df['year_built'].notna().mean()*100:.1f}%\")\nprint(f\"JK linked: {df['jk_name'].notna().mean()*100:.1f}%\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T18:48:04.547537Z","iopub.execute_input":"2026-01-10T18:48:04.547790Z","iopub.status.idle":"2026-01-10T18:48:04.872858Z","shell.execute_reply.started":"2026-01-10T18:48:04.547768Z","shell.execute_reply":"2026-01-10T18:48:04.872134Z"}},"outputs":[{"name":"stdout","text":"Dataset: 8821 apartments, 60 columns\nyear_built filled: 73.1%\nJK linked: 48.9%\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Feature Engineering (Same as v3)\n\n","metadata":{}},{"cell_type":"code","source":"class FeatureEngineer:\n    \"\"\"\n    Feature engineering pipeline for real estate data.\n    Handles imputation, encoding, POI distances, and derived features.\n\n    IMPORTANT: All transformations are fit on train data only to prevent data leakage.\n    \"\"\"\n\n    def __init__(self, n_district_clusters=30, include_poi=True):\n        self.n_district_clusters = n_district_clusters\n        self.include_poi = include_poi\n        self.year_medians_by_series = None\n        self.year_medians_by_type_floor = None\n        self.global_year_median = None\n        self.district_kmeans = None\n        self.target_encodings = {}\n        self.label_encoders = {}\n        self.is_fitted = False\n\n    def fit(self, df, y=None):\n        \"\"\"Fit all transformers on training data\"\"\"\n        df = df.copy()\n\n        # 1. Year built imputation medians\n        df['floor_group'] = pd.cut(\n            df['total_floors'].fillna(9),\n            bins=[0, 5, 9, 12, 16, 100],\n            labels=['1-5', '6-9', '10-12', '13-16', '17+']\n        )\n\n        filled = df[df['year_built'].notna()]\n        self.year_medians_by_series = filled.groupby('building_series')['year_built'].median().to_dict()\n        self.year_medians_by_type_floor = filled.groupby(['house_type', 'floor_group'])['year_built'].median().to_dict()\n        self.global_year_median = filled['year_built'].median()\n\n        # 2. District clustering (K-means on coordinates)\n        coords = df[['latitude', 'longitude']].dropna()\n        if len(coords) > self.n_district_clusters:\n            self.district_kmeans = KMeans(\n                n_clusters=self.n_district_clusters,\n                random_state=42,\n                n_init=10\n            )\n            self.district_kmeans.fit(coords)\n\n        # 3. Target encoding for categorical variables (if y provided)\n        if y is not None:\n            df_with_target = df.copy()\n            df_with_target['target'] = y\n\n            for col in ['jk_name', 'district', 'building_series', 'house_type']:\n                if col in df.columns:\n                    means = df_with_target.groupby(col)['target'].mean()\n                    counts = df_with_target.groupby(col)['target'].count()\n\n                    # Smoothing with global mean\n                    global_mean = y.mean()\n                    smoothing = 10\n\n                    smoothed = (means * counts + global_mean * smoothing) / (counts + smoothing)\n                    self.target_encodings[col] = smoothed.to_dict()\n\n        # 4. Label encoders for remaining categoricals\n        for col in ['condition', 'heating', 'bathroom']:\n            if col in df.columns:\n                le = LabelEncoder()\n                valid_values = df[col].dropna().unique()\n                le.fit(list(valid_values) + ['unknown'])\n                self.label_encoders[col] = le\n\n        self.is_fitted = True\n        return self\n\n    def transform(self, df):\n        \"\"\"Transform data using fitted transformers\"\"\"\n        if not self.is_fitted:\n            raise ValueError(\"FeatureEngineer must be fitted before transform\")\n\n        df = df.copy()\n\n        # 1. Impute year_built\n        df['floor_group'] = pd.cut(\n            df['total_floors'].fillna(9),\n            bins=[0, 5, 9, 12, 16, 100],\n            labels=['1-5', '6-9', '10-12', '13-16', '17+']\n        )\n\n        mask = df['year_built'].isna()\n        for idx in df[mask].index:\n            series = df.loc[idx, 'building_series']\n            htype = df.loc[idx, 'house_type']\n            fgroup = df.loc[idx, 'floor_group']\n\n            if series in self.year_medians_by_series:\n                df.loc[idx, 'year_built'] = self.year_medians_by_series[series]\n            elif (htype, fgroup) in self.year_medians_by_type_floor:\n                df.loc[idx, 'year_built'] = self.year_medians_by_type_floor[(htype, fgroup)]\n            else:\n                df.loc[idx, 'year_built'] = self.global_year_median\n\n        # 2. District clusters\n        if self.district_kmeans is not None:\n            coords = df[['latitude', 'longitude']].fillna(df[['latitude', 'longitude']].median())\n            df['district_cluster'] = self.district_kmeans.predict(coords)\n\n        # 3. Derived features\n        df['floor_ratio'] = df['floor'] / df['total_floors'].replace(0, 1)\n        df['is_first_floor'] = (df['floor'] == 1).astype(int)\n        df['is_last_floor'] = (df['floor'] == df['total_floors']).astype(int)\n        df['building_age'] = 2025 - df['year_built']\n        df['is_new_building'] = (df['year_built'] >= 2020).astype(int)\n        df['area_per_room'] = df['area'] / df['rooms'].replace(0, 1)\n        df['is_highrise'] = (df['total_floors'] >= 12).astype(int)\n\n        # 4. Binary amenity features\n        df['has_balcony'] = df['balcony'].notna().astype(int)\n        df['has_parking'] = df['parking'].notna().astype(int)\n        df['has_furniture'] = df['furniture'].notna().astype(int)\n\n        # 5. Target encoding\n        for col, encoding in self.target_encodings.items():\n            if col in df.columns:\n                global_mean = np.mean(list(encoding.values()))\n                df[f'{col}_encoded'] = df[col].map(encoding).fillna(global_mean)\n\n        # 6. Label encoding\n        for col, le in self.label_encoders.items():\n            if col in df.columns:\n                df[f'{col}_encoded'] = df[col].fillna('unknown').apply(\n                    lambda x: le.transform([x])[0] if x in le.classes_ else le.transform(['unknown'])[0]\n                )\n\n        # 7. House type one-hot\n        if 'house_type' in df.columns:\n            df['is_monolith'] = (df['house_type'] == 'монолит').astype(int)\n            df['is_brick'] = (df['house_type'] == 'кирпич').astype(int)\n            df['is_panel'] = (df['house_type'] == 'панель').astype(int)\n\n        # 8. POI distances\n        if self.include_poi:\n            df = self._add_poi_features(df)\n\n        return df\n\n    def _add_poi_features(self, df):\n        \"\"\"Add POI distance features\"\"\"\n        df = df.copy()\n\n        # Distance to city center\n        df['dist_to_center'] = df.apply(\n            lambda row: haversine_distance(\n                row['latitude'], row['longitude'],\n                BISHKEK_CENTER[0], BISHKEK_CENTER[1]\n            ) if pd.notna(row['latitude']) and pd.notna(row['longitude']) else np.nan,\n            axis=1\n        )\n\n        # Distance to each POI category (minimum distance to nearest POI in category)\n        for category, pois in BISHKEK_POI.items():\n            col_name = f'dist_to_{category}'\n            df[col_name] = df.apply(\n                lambda row: self._min_distance_to_pois(row, pois),\n                axis=1\n            )\n\n        # Distance to premium zones (minimum)\n        df['dist_to_premium'] = df.apply(\n            lambda row: self._min_distance_to_premium(row),\n            axis=1\n        )\n\n        # Binary: is in premium zone (within 1km)\n        df['is_premium_zone'] = (df['dist_to_premium'] <= 1.0).astype(int)\n\n        return df\n\n    def _min_distance_to_pois(self, row, pois):\n        \"\"\"Calculate minimum distance to a list of POIs\"\"\"\n        if pd.isna(row['latitude']) or pd.isna(row['longitude']):\n            return np.nan\n        distances = [\n            haversine_distance(row['latitude'], row['longitude'], lat, lon)\n            for name, lat, lon in pois\n        ]\n        return min(distances) if distances else np.nan\n\n    def _min_distance_to_premium(self, row):\n        \"\"\"Calculate minimum distance to premium zones\"\"\"\n        if pd.isna(row['latitude']) or pd.isna(row['longitude']):\n            return np.nan\n        distances = [\n            haversine_distance(row['latitude'], row['longitude'], lat, lon)\n            for name, (lat, lon) in BISHKEK_PREMIUM_ZONES.items()\n        ]\n        return min(distances) if distances else np.nan\n\n    def fit_transform(self, df, y=None):\n        \"\"\"Fit and transform in one step\"\"\"\n        self.fit(df, y)\n        return self.transform(df)\n\n    def get_feature_columns(self):\n        \"\"\"Return list of feature columns for model\"\"\"\n        base_features = [\n            # Core numeric\n            'latitude', 'longitude', 'area', 'rooms', 'floor', 'total_floors',\n            'year_built', 'ceiling_height',\n\n            # Derived\n            'floor_ratio', 'is_first_floor', 'is_last_floor',\n            'building_age', 'is_new_building', 'area_per_room', 'is_highrise',\n\n            # Binary\n            'has_balcony', 'has_parking', 'has_furniture',\n\n            # House type\n            'is_monolith', 'is_brick', 'is_panel',\n\n            # Cluster\n            'district_cluster',\n        ]\n\n        # POI distances\n        if self.include_poi:\n            base_features.extend([\n                'dist_to_center',\n                'dist_to_bazaars', 'dist_to_parks', 'dist_to_malls',\n                'dist_to_universities', 'dist_to_hospitals',\n                'dist_to_transport', 'dist_to_admin',\n                'dist_to_premium', 'is_premium_zone',\n            ])\n\n        # Target encoded\n        for col in self.target_encodings.keys():\n            base_features.append(f'{col}_encoded')\n\n        # Label encoded\n        for col in self.label_encoders.keys():\n            base_features.append(f'{col}_encoded')\n\n        return base_features\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T18:48:04.873905Z","iopub.execute_input":"2026-01-10T18:48:04.874283Z","iopub.status.idle":"2026-01-10T18:48:04.897550Z","shell.execute_reply.started":"2026-01-10T18:48:04.874238Z","shell.execute_reply":"2026-01-10T18:48:04.896780Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Image Feature Extractor (CV)\n\n","metadata":{}},{"cell_type":"code","source":"class ImageFeatureExtractor:\n    \"\"\"\n    Extract image embeddings using pretrained ResNet-50.\n\n    Based on research:\n    - MHPP (arXiv 2024): ResNet-50 + mean pooling\n    - PLOS One 2025: ResNet-101 + PCA\n    \"\"\"\n\n    def __init__(self, embedding_dim=64, batch_size=32):\n        self.embedding_dim = embedding_dim\n        self.batch_size = batch_size\n        self.pca = None\n        self.model = None\n        self.transform = None\n\n        if TORCH_AVAILABLE:\n            # Load pretrained ResNet-50, remove classification head\n            resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n            self.model = torch.nn.Sequential(*list(resnet.children())[:-1])\n            self.model.eval()\n            self.model.to(DEVICE)\n\n            # Image preprocessing (ImageNet standard)\n            self.transform = transforms.Compose([\n                transforms.Resize(256),\n                transforms.CenterCrop(224),\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    mean=[0.485, 0.456, 0.406],\n                    std=[0.229, 0.224, 0.225]\n                )\n            ])\n\n    def extract_single_image(self, image_path):\n        \"\"\"Extract 2048-dim embedding from single image\"\"\"\n        if not TORCH_AVAILABLE or self.model is None:\n            return None\n\n        try:\n            img = Image.open(image_path).convert('RGB')\n            img_tensor = self.transform(img).unsqueeze(0).to(DEVICE)\n            img.close()  # Prevent memory leak\n\n            with torch.no_grad():\n                embedding = self.model(img_tensor)\n\n            return embedding.squeeze().cpu().numpy()\n        except Exception as e:\n            return None\n\n    def extract_listing_embedding(self, image_paths, max_images=10):\n        \"\"\"Extract mean embedding from multiple images of a listing\"\"\"\n        embeddings = []\n\n        for path in image_paths[:max_images]:\n            emb = self.extract_single_image(path)\n            if emb is not None:\n                embeddings.append(emb)\n\n        if not embeddings:\n            return None\n\n        # Mean pooling (robust to varying image counts)\n        return np.mean(embeddings, axis=0)\n\n    def fit_pca(self, embeddings):\n        \"\"\"Fit PCA on training embeddings\"\"\"\n        valid = [e for e in embeddings if e is not None]\n        if len(valid) > self.embedding_dim:\n            self.pca = PCA(n_components=self.embedding_dim, random_state=42)\n            self.pca.fit(valid)\n            print(f\"PCA fitted: {len(valid)} samples, explained variance: {self.pca.explained_variance_ratio_.sum():.2%}\")\n        return self\n\n    def transform_embeddings(self, embeddings):\n        \"\"\"Apply PCA to embeddings\"\"\"\n        result = []\n        for emb in embeddings:\n            if emb is not None and self.pca is not None:\n                result.append(self.pca.transform([emb])[0])\n            else:\n                result.append(np.zeros(self.embedding_dim))\n        return np.array(result)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T18:48:04.898357Z","iopub.execute_input":"2026-01-10T18:48:04.898608Z","iopub.status.idle":"2026-01-10T18:48:04.912636Z","shell.execute_reply.started":"2026-01-10T18:48:04.898588Z","shell.execute_reply":"2026-01-10T18:48:04.912051Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def get_image_paths(listing_id, image_dir):\n    \"\"\"Get all image paths for a listing\"\"\"\n    listing_dir = Path(image_dir) / str(listing_id)\n    if listing_dir.exists():\n        return list(listing_dir.glob('*.jpg')) + list(listing_dir.glob('*.jpeg')) + list(listing_dir.glob('*.png'))\n    return []\n\ndef get_listing_id_from_url(url):\n    \"\"\"Extract listing ID from URL like /details/33889236900a242685078-39865021\"\"\"\n    if pd.isna(url):\n        return None\n    import re\n    match = re.search(r'/details/([^?]+)', str(url))\n    if match:\n        return match.group(1).split('?')[0]\n    return None\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T18:48:04.913410Z","iopub.execute_input":"2026-01-10T18:48:04.913694Z","iopub.status.idle":"2026-01-10T18:48:04.926045Z","shell.execute_reply.started":"2026-01-10T18:48:04.913672Z","shell.execute_reply":"2026-01-10T18:48:04.925460Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Metrics\n\n","metadata":{}},{"cell_type":"code","source":"def calculate_metrics(y_true, y_pred):\n    \"\"\"Calculate regression metrics\"\"\"\n    mae = mean_absolute_error(y_true, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n    medape = np.median(np.abs((y_true - y_pred) / y_true)) * 100\n    r2 = r2_score(y_true, y_pred)\n    within_10 = np.mean(np.abs((y_true - y_pred) / y_true) <= 0.1) * 100\n    return {\n        'MAE': mae, 'RMSE': rmse, 'MAPE': mape,\n        'MedAPE': medape, 'R2': r2, 'Within10%': within_10\n    }\n\ndef print_metrics(metrics, prefix=\"\"):\n    \"\"\"Print metrics nicely\"\"\"\n    print(f\"{prefix}MAE: ${metrics['MAE']:.2f}/m²\")\n    print(f\"{prefix}RMSE: ${metrics['RMSE']:.2f}/m²\")\n    print(f\"{prefix}MAPE: {metrics['MAPE']:.2f}%\")\n    print(f\"{prefix}MedAPE: {metrics['MedAPE']:.2f}%\")\n    print(f\"{prefix}R²: {metrics['R2']:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T18:48:04.928096Z","iopub.execute_input":"2026-01-10T18:48:04.928402Z","iopub.status.idle":"2026-01-10T18:48:04.937073Z","shell.execute_reply.started":"2026-01-10T18:48:04.928372Z","shell.execute_reply":"2026-01-10T18:48:04.936421Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Prepare Data\n\n","metadata":{}},{"cell_type":"code","source":"# Prepare data\nTARGET = 'price_per_m2'\n\n# Remove outliers (SAME AS v3)\ndf_clean = df[\n    (df[TARGET] >= 500) &\n    (df[TARGET] <= 5000) &\n    (df['area'] >= 15) &\n    (df['area'] <= 300)\n].copy()\n\nprint(f\"After outlier removal: {len(df_clean)} apartments\")\n\n# Extract listing IDs from URL\ndf_clean['listing_id'] = df_clean['url'].apply(get_listing_id_from_url)\n\n# Split data (RANDOM SPLIT - same as v3)\nX = df_clean.drop(columns=[TARGET, 'price_usd', 'price_local'], errors='ignore')\ny = df_clean[TARGET]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(f\"Train: {len(X_train)}, Test: {len(X_test)}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T18:48:04.937873Z","iopub.execute_input":"2026-01-10T18:48:04.938327Z","iopub.status.idle":"2026-01-10T18:48:04.987632Z","shell.execute_reply.started":"2026-01-10T18:48:04.938300Z","shell.execute_reply":"2026-01-10T18:48:04.987090Z"}},"outputs":[{"name":"stdout","text":"After outlier removal: 8727 apartments\nTrain: 6981, Test: 1746\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Feature Engineering\n\n","metadata":{}},{"cell_type":"code","source":"# Feature engineering\nfe = FeatureEngineer(n_district_clusters=30)\nX_train_fe = fe.fit_transform(X_train, y_train)\nX_test_fe = fe.transform(X_test)\n\nfeature_cols = fe.get_feature_columns()\n# Filter to existing columns\nfeature_cols = [c for c in feature_cols if c in X_train_fe.columns]\n\nprint(f\"Tabular features: {len(feature_cols)}\")\nprint(feature_cols)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T18:48:04.988515Z","iopub.execute_input":"2026-01-10T18:48:04.988958Z","iopub.status.idle":"2026-01-10T18:48:09.691598Z","shell.execute_reply.started":"2026-01-10T18:48:04.988936Z","shell.execute_reply":"2026-01-10T18:48:09.690919Z"}},"outputs":[{"name":"stdout","text":"Tabular features: 39\n['latitude', 'longitude', 'area', 'rooms', 'floor', 'total_floors', 'year_built', 'ceiling_height', 'floor_ratio', 'is_first_floor', 'is_last_floor', 'building_age', 'is_new_building', 'area_per_room', 'is_highrise', 'has_balcony', 'has_parking', 'has_furniture', 'is_monolith', 'is_brick', 'is_panel', 'district_cluster', 'dist_to_center', 'dist_to_bazaars', 'dist_to_parks', 'dist_to_malls', 'dist_to_universities', 'dist_to_hospitals', 'dist_to_transport', 'dist_to_admin', 'dist_to_premium', 'is_premium_zone', 'jk_name_encoded', 'district_encoded', 'building_series_encoded', 'house_type_encoded', 'condition_encoded', 'heating_encoded', 'bathroom_encoded']\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Image Features (CV)\n\n","metadata":{}},{"cell_type":"code","source":"# Setup images directory\n# Kaggle auto-extracts zips to folders like bishkek_p1, bishkek_p2, etc.\nIMAGES_BASE = Path('/kaggle/input/bishkek-real-estate-images')\nMERGED_IMAGES = Path('/kaggle/working/images')\n\n# Check what's available\nprint(f\"Images base exists: {IMAGES_BASE.exists()}\")\nif IMAGES_BASE.exists():\n    contents = list(IMAGES_BASE.iterdir())\n    print(f\"Contents: {[c.name for c in contents[:10]]}\")\n\n    # Merge all parts into single directory\n    image_parts = sorted(IMAGES_BASE.glob(\"bishkek_p*\"))\n    if image_parts:\n        print(f\"\\nFound {len(image_parts)} image parts, merging...\")\n        MERGED_IMAGES.mkdir(parents=True, exist_ok=True)\n\n        total_listings = 0\n        for part_dir in image_parts:\n            if part_dir.is_dir():\n                for listing_dir in part_dir.iterdir():\n                    if listing_dir.is_dir():\n                        dest = MERGED_IMAGES / listing_dir.name\n                        if not dest.exists():\n                            shutil.copytree(listing_dir, dest)\n                            total_listings += 1\n\n        print(f\"Merged {total_listings} listing directories\")\n        IMAGE_DIR = MERGED_IMAGES\n    else:\n        # Maybe images are directly in base\n        IMAGE_DIR = IMAGES_BASE\nelse:\n    IMAGE_DIR = None\n    print(\"No images available\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T18:48:09.692696Z","iopub.execute_input":"2026-01-10T18:48:09.692990Z","iopub.status.idle":"2026-01-10T18:53:34.799374Z","shell.execute_reply.started":"2026-01-10T18:48:09.692960Z","shell.execute_reply":"2026-01-10T18:53:34.798640Z"}},"outputs":[{"name":"stdout","text":"Images base exists: True\nContents: ['bishkek_p1', 'bishkek_p2', 'bishkek_p3', 'bishkek_p4']\n\nFound 4 image parts, merging...\nMerged 7869 listing directories\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Extract image embeddings\nUSE_CV = TORCH_AVAILABLE and IMAGE_DIR is not None and IMAGE_DIR.exists()\n\nif USE_CV:\n    print(\"Extracting image embeddings with ResNet-50...\")\n    print(f\"Image directory: {IMAGE_DIR}\")\n\n    img_extractor = ImageFeatureExtractor(embedding_dim=64)\n\n    # Get train listing IDs and their image paths\n    train_ids = X_train_fe['listing_id'].tolist() if 'listing_id' in X_train_fe.columns else X_train['listing_id'].tolist()\n    test_ids = X_test_fe['listing_id'].tolist() if 'listing_id' in X_test_fe.columns else X_test['listing_id'].tolist()\n\n    # Check how many listings have images\n    train_with_images = sum(1 for lid in train_ids if lid and get_image_paths(lid, IMAGE_DIR))\n    test_with_images = sum(1 for lid in test_ids if lid and get_image_paths(lid, IMAGE_DIR))\n    print(f\"Listings with images - Train: {train_with_images}/{len(train_ids)}, Test: {test_with_images}/{len(test_ids)}\")\n\n    if train_with_images > 100:  # Need enough images to train\n        # Extract train embeddings\n        print(\"\\nExtracting train embeddings...\")\n        train_embeddings = []\n        for i, lid in enumerate(train_ids):\n            if lid:\n                paths = get_image_paths(lid, IMAGE_DIR)\n                emb = img_extractor.extract_listing_embedding(paths) if paths else None\n            else:\n                emb = None\n            train_embeddings.append(emb)\n\n            if (i + 1) % 500 == 0:\n                print(f\"  Processed {i+1}/{len(train_ids)}\")\n                gc.collect()\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n\n        # Fit PCA on train embeddings\n        img_extractor.fit_pca(train_embeddings)\n\n        # Transform train embeddings\n        train_img_features = img_extractor.transform_embeddings(train_embeddings)\n\n        # Extract and transform test embeddings\n        print(\"\\nExtracting test embeddings...\")\n        test_embeddings = []\n        for i, lid in enumerate(test_ids):\n            if lid:\n                paths = get_image_paths(lid, IMAGE_DIR)\n                emb = img_extractor.extract_listing_embedding(paths) if paths else None\n            else:\n                emb = None\n            test_embeddings.append(emb)\n\n            if (i + 1) % 500 == 0:\n                print(f\"  Processed {i+1}/{len(test_ids)}\")\n\n        test_img_features = img_extractor.transform_embeddings(test_embeddings)\n\n        print(f\"\\nImage features shape: train={train_img_features.shape}, test={test_img_features.shape}\")\n        CV_FEATURES_AVAILABLE = True\n    else:\n        print(\"Not enough listings with images, skipping CV features\")\n        CV_FEATURES_AVAILABLE = False\n        train_img_features = None\n        test_img_features = None\nelse:\n    print(\"CV features disabled (no PyTorch or no images)\")\n    CV_FEATURES_AVAILABLE = False\n    train_img_features = None\n    test_img_features = None\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T18:53:34.800384Z","iopub.execute_input":"2026-01-10T18:53:34.800627Z","iopub.status.idle":"2026-01-10T19:14:38.128015Z","shell.execute_reply.started":"2026-01-10T18:53:34.800604Z","shell.execute_reply":"2026-01-10T19:14:38.127093Z"}},"outputs":[{"name":"stdout","text":"Extracting image embeddings with ResNet-50...\nImage directory: /kaggle/working/images\nDownloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 97.8M/97.8M [00:00<00:00, 193MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Listings with images - Train: 6246/6981, Test: 1565/1746\n\nExtracting train embeddings...\n  Processed 500/6981\n  Processed 1000/6981\n  Processed 1500/6981\n  Processed 2000/6981\n  Processed 2500/6981\n  Processed 3000/6981\n  Processed 3500/6981\n  Processed 4000/6981\n  Processed 4500/6981\n  Processed 5000/6981\n  Processed 5500/6981\n  Processed 6000/6981\n  Processed 6500/6981\nPCA fitted: 6246 samples, explained variance: 81.36%\n\nExtracting test embeddings...\n  Processed 500/1746\n  Processed 1000/1746\n  Processed 1500/1746\n\nImage features shape: train=(6981, 64), test=(1746, 64)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Prepare Final Features\n\n","metadata":{}},{"cell_type":"code","source":"# Prepare final feature matrices\nX_train_final = X_train_fe[feature_cols].fillna(0)\nX_test_final = X_test_fe[feature_cols].fillna(0)\n\n# Add image features if available\nif CV_FEATURES_AVAILABLE and train_img_features is not None:\n    img_cols = [f'img_{i}' for i in range(train_img_features.shape[1])]\n\n    train_img_df = pd.DataFrame(train_img_features, columns=img_cols, index=X_train_final.index)\n    test_img_df = pd.DataFrame(test_img_features, columns=img_cols, index=X_test_final.index)\n\n    X_train_final = pd.concat([X_train_final, train_img_df], axis=1)\n    X_test_final = pd.concat([X_test_final, test_img_df], axis=1)\n\n    print(f\"Combined features: {X_train_final.shape[1]} (tabular: {len(feature_cols)}, image: {len(img_cols)})\")\nelse:\n    print(f\"Tabular features only: {X_train_final.shape[1]}\")\n\n# Scale features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_final)\nX_test_scaled = scaler.transform(X_test_final)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T19:14:38.129241Z","iopub.execute_input":"2026-01-10T19:14:38.129553Z","iopub.status.idle":"2026-01-10T19:14:38.163387Z","shell.execute_reply.started":"2026-01-10T19:14:38.129525Z","shell.execute_reply":"2026-01-10T19:14:38.162722Z"}},"outputs":[{"name":"stdout","text":"Combined features: 103 (tabular: 39, image: 64)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## GPU Detection\n\n","metadata":{}},{"cell_type":"code","source":"# Detect GPU availability for boosting models\nUSE_GPU_BOOSTING = False\nUSE_GPU_XGB = False\nUSE_GPU_LGB = False\nUSE_GPU_CAT = False\n\n# Check if GPU is available (on Kaggle)\ntry:\n    import subprocess\n    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n    if result.returncode == 0:\n        USE_GPU_BOOSTING = True\n        USE_GPU_XGB = True  # XGBoost 2.0+ with CUDA\n        USE_GPU_LGB = True  # LightGBM with GPU\n        USE_GPU_CAT = True  # CatBoost with GPU\n        print(\"GPU detected! Enabling GPU acceleration for boosting models.\")\n        print(result.stdout.split('\\n')[8] if len(result.stdout.split('\\n')) > 8 else \"\")\nexcept:\n    pass\n\nif not USE_GPU_BOOSTING:\n    print(\"No GPU detected or not available. Using CPU for boosting models.\")\n    print(\"Tip: On Kaggle, enable GPU accelerator in Settings for faster training.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T19:14:38.164179Z","iopub.execute_input":"2026-01-10T19:14:38.164809Z","iopub.status.idle":"2026-01-10T19:14:38.280149Z","shell.execute_reply.started":"2026-01-10T19:14:38.164786Z","shell.execute_reply":"2026-01-10T19:14:38.279575Z"}},"outputs":[{"name":"stdout","text":"GPU detected! Enabling GPU acceleration for boosting models.\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Train Base Models\n\n","metadata":{}},{"cell_type":"code","source":"# Define base models with GPU support\nxgb_params = {\n    'n_estimators': 500,\n    'max_depth': 6,\n    'learning_rate': 0.05,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'min_child_weight': 3,\n    'reg_alpha': 0.1,\n    'reg_lambda': 1.0,\n    'random_state': 42,\n}\n\n# XGBoost GPU settings\nif USE_GPU_XGB:\n    xgb_params['tree_method'] = 'hist'\n    xgb_params['device'] = 'cuda'\n\nlgb_params = {\n    'n_estimators': 500,\n    'max_depth': 6,\n    'learning_rate': 0.05,\n    'num_leaves': 31,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'random_state': 42,\n    'verbose': -1,\n}\n\nif USE_GPU_LGB:\n    lgb_params['device'] = 'gpu'\n\ncat_params = {\n    'iterations': 500,\n    'depth': 6,\n    'learning_rate': 0.05,\n    'random_state': 42,\n    'verbose': 0,\n}\n\nif USE_GPU_CAT:\n    cat_params['task_type'] = 'GPU'\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T19:14:38.280909Z","iopub.execute_input":"2026-01-10T19:14:38.281159Z","iopub.status.idle":"2026-01-10T19:14:38.286442Z","shell.execute_reply.started":"2026-01-10T19:14:38.281130Z","shell.execute_reply":"2026-01-10T19:14:38.285732Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Train individual models\nprint(\"Training XGBoost...\")\nxgb_model = XGBRegressor(**xgb_params)\nxgb_model.fit(X_train_scaled, y_train)\nxgb_pred = xgb_model.predict(X_test_scaled)\nprint_metrics(calculate_metrics(y_test, xgb_pred), \"XGBoost: \")\n\nprint(\"\\nTraining LightGBM...\")\nlgb_model = LGBMRegressor(**lgb_params)\nlgb_model.fit(X_train_scaled, y_train)\nlgb_pred = lgb_model.predict(X_test_scaled)\nprint_metrics(calculate_metrics(y_test, lgb_pred), \"LightGBM: \")\n\nprint(\"\\nTraining CatBoost...\")\ncat_model = CatBoostRegressor(**cat_params)\ncat_model.fit(X_train_scaled, y_train)\ncat_pred = cat_model.predict(X_test_scaled)\nprint_metrics(calculate_metrics(y_test, cat_pred), \"CatBoost: \")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T19:14:38.287301Z","iopub.execute_input":"2026-01-10T19:14:38.287758Z","iopub.status.idle":"2026-01-10T19:14:53.813985Z","shell.execute_reply.started":"2026-01-10T19:14:38.287737Z","shell.execute_reply":"2026-01-10T19:14:53.813178Z"}},"outputs":[{"name":"stdout","text":"Training XGBoost...\nXGBoost: MAE: $129.83/m²\nXGBoost: RMSE: $189.10/m²\nXGBoost: MAPE: 8.32%\nXGBoost: MedAPE: 6.08%\nXGBoost: R²: 0.7315\n\nTraining LightGBM...\n","output_type":"stream"},{"name":"stderr","text":"1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n","output_type":"stream"},{"name":"stdout","text":"LightGBM: MAE: $131.30/m²\nLightGBM: RMSE: $190.20/m²\nLightGBM: MAPE: 8.46%\nLightGBM: MedAPE: 6.09%\nLightGBM: R²: 0.7284\n\nTraining CatBoost...\nCatBoost: MAE: $140.67/m²\nCatBoost: RMSE: $200.97/m²\nCatBoost: MAPE: 9.11%\nCatBoost: MedAPE: 6.74%\nCatBoost: R²: 0.6967\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Optuna Hyperparameter Tuning\n\n","metadata":{}},{"cell_type":"code","source":"def run_optuna_tuning(X_train, y_train, n_trials=50, use_gpu_xgb=False, use_gpu_lgb=False, use_gpu_cat=False):\n    \"\"\"\n    Run Optuna hyperparameter optimization for XGBoost, LightGBM, and CatBoost.\n    Returns best parameters for each model.\n    \"\"\"\n    if not OPTUNA_AVAILABLE:\n        print(\"Optuna not available, using default parameters\")\n        return None, None, None\n\n    print(\"Starting Optuna hyperparameter tuning...\")\n    print(f\"GPU: XGBoost={use_gpu_xgb}, LightGBM={use_gpu_lgb}, CatBoost={use_gpu_cat}\")\n    optuna.logging.set_verbosity(optuna.logging.WARNING)\n\n    # XGBoost objective\n    def xgb_objective(trial):\n        params = {\n            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n            'max_depth': trial.suggest_int('max_depth', 3, 10),\n            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n            'reg_alpha': trial.suggest_float('reg_alpha', 1e-4, 1.0, log=True),\n            'reg_lambda': trial.suggest_float('reg_lambda', 1e-4, 1.0, log=True),\n            'random_state': 42,\n        }\n        if use_gpu_xgb:\n            params['tree_method'] = 'hist'\n            params['device'] = 'cuda'\n\n        model = XGBRegressor(**params)\n        scores = cross_val_score(model, X_train, y_train, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n        return -scores.mean()\n\n    # LightGBM objective\n    def lgb_objective(trial):\n        params = {\n            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n            'max_depth': trial.suggest_int('max_depth', 3, 12),\n            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n            'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n            'random_state': 42,\n            'verbose': -1,\n        }\n        if use_gpu_lgb:\n            params['device'] = 'gpu'\n\n        model = LGBMRegressor(**params)\n        scores = cross_val_score(model, X_train, y_train, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n        return -scores.mean()\n\n    # CatBoost objective\n    def cat_objective(trial):\n        params = {\n            'iterations': trial.suggest_int('iterations', 100, 1000),\n            'depth': trial.suggest_int('depth', 4, 10),\n            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.1, 10.0, log=True),\n            'random_state': 42,\n            'verbose': 0,\n        }\n        if use_gpu_cat:\n            params['task_type'] = 'GPU'\n\n        model = CatBoostRegressor(**params)\n        scores = cross_val_score(model, X_train, y_train, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n        return -scores.mean()\n\n    # Run optimization\n    print(\"\\n[1/3] Tuning XGBoost...\")\n    xgb_study = optuna.create_study(direction='minimize')\n    xgb_study.optimize(xgb_objective, n_trials=n_trials, show_progress_bar=False)\n    print(f\"  Best MAE: ${-xgb_study.best_value:.2f}/m²\")\n\n    print(\"\\n[2/3] Tuning LightGBM...\")\n    lgb_study = optuna.create_study(direction='minimize')\n    lgb_study.optimize(lgb_objective, n_trials=n_trials, show_progress_bar=False)\n    print(f\"  Best MAE: ${-lgb_study.best_value:.2f}/m²\")\n\n    print(\"\\n[3/3] Tuning CatBoost...\")\n    cat_study = optuna.create_study(direction='minimize')\n    cat_study.optimize(cat_objective, n_trials=n_trials, show_progress_bar=False)\n    print(f\"  Best MAE: ${-cat_study.best_value:.2f}/m²\")\n\n    # Get best params\n    xgb_best = xgb_study.best_params.copy()\n    xgb_best['random_state'] = 42\n    if use_gpu_xgb:\n        xgb_best['tree_method'] = 'hist'\n        xgb_best['device'] = 'cuda'\n\n    lgb_best = lgb_study.best_params.copy()\n    lgb_best['random_state'] = 42\n    lgb_best['verbose'] = -1\n    if use_gpu_lgb:\n        lgb_best['device'] = 'gpu'\n\n    cat_best = cat_study.best_params.copy()\n    cat_best['random_state'] = 42\n    cat_best['verbose'] = 0\n    if use_gpu_cat:\n        cat_best['task_type'] = 'GPU'\n\n    return xgb_best, lgb_best, cat_best\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T19:14:53.815035Z","iopub.execute_input":"2026-01-10T19:14:53.815427Z","iopub.status.idle":"2026-01-10T19:14:53.835097Z","shell.execute_reply.started":"2026-01-10T19:14:53.815398Z","shell.execute_reply":"2026-01-10T19:14:53.834391Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Run Optuna tuning\nUSE_OPTUNA = True\n\nif USE_OPTUNA and OPTUNA_AVAILABLE:\n    xgb_params_opt, lgb_params_opt, cat_params_opt = run_optuna_tuning(\n        X_train_scaled, y_train, n_trials=30,\n        use_gpu_xgb=False, use_gpu_lgb=False, use_gpu_cat=False\n    )\n\n    if xgb_params_opt:\n        print(\"\\n\" + \"=\"*50)\n        print(\"OPTIMIZED PARAMETERS:\")\n        print(\"=\"*50)\n        print(f\"\\nXGBoost best params: {xgb_params_opt}\")\n        print(f\"\\nLightGBM best params: {lgb_params_opt}\")\n        print(f\"\\nCatBoost best params: {cat_params_opt}\")\n\n        # Update params for ensemble\n        xgb_params = xgb_params_opt\n        lgb_params = lgb_params_opt\n        cat_params = cat_params_opt\n\n        # Retrain with optimized parameters\n        print(\"\\nRetraining with optimized parameters...\")\n        xgb_model = XGBRegressor(**xgb_params)\n        xgb_model.fit(X_train_scaled, y_train)\n        xgb_pred = xgb_model.predict(X_test_scaled)\n        print_metrics(calculate_metrics(y_test, xgb_pred), \"XGBoost (tuned): \")\n\n        lgb_model = LGBMRegressor(**lgb_params)\n        lgb_model.fit(X_train_scaled, y_train)\n        lgb_pred = lgb_model.predict(X_test_scaled)\n        print_metrics(calculate_metrics(y_test, lgb_pred), \"\\nLightGBM (tuned): \")\n\n        cat_model = CatBoostRegressor(**cat_params)\n        cat_model.fit(X_train_scaled, y_train)\n        cat_pred = cat_model.predict(X_test_scaled)\n        print_metrics(calculate_metrics(y_test, cat_pred), \"\\nCatBoost (tuned): \")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T19:36:02.424015Z","iopub.execute_input":"2026-01-10T19:36:02.424770Z","iopub.status.idle":"2026-01-10T20:42:06.467550Z","shell.execute_reply.started":"2026-01-10T19:36:02.424739Z","shell.execute_reply":"2026-01-10T20:42:06.466629Z"}},"outputs":[{"name":"stdout","text":"Starting Optuna hyperparameter tuning...\nGPU: XGBoost=False, LightGBM=False, CatBoost=False\n\n[1/3] Tuning XGBoost...\n  Best MAE: $-135.10/m²\n\n[2/3] Tuning LightGBM...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"  Best MAE: $-136.73/m²\n\n[3/3] Tuning CatBoost...\n  Best MAE: $-138.01/m²\n\n==================================================\nOPTIMIZED PARAMETERS:\n==================================================\n\nXGBoost best params: {'n_estimators': 980, 'max_depth': 8, 'learning_rate': 0.01429682190405642, 'subsample': 0.7453340306688304, 'colsample_bytree': 0.6525204072741028, 'min_child_weight': 2, 'reg_alpha': 0.0004203123283104735, 'reg_lambda': 0.008661281301109377, 'random_state': 42}\n\nLightGBM best params: {'n_estimators': 720, 'max_depth': 12, 'learning_rate': 0.03781509460184321, 'num_leaves': 78, 'subsample': 0.7589847094582766, 'colsample_bytree': 0.8423211583618249, 'random_state': 42, 'verbose': -1}\n\nCatBoost best params: {'iterations': 916, 'depth': 8, 'learning_rate': 0.07025183668627026, 'l2_leaf_reg': 0.16162769670807364, 'random_state': 42, 'verbose': 0}\n\nRetraining with optimized parameters...\nXGBoost (tuned): MAE: $126.06/m²\nXGBoost (tuned): RMSE: $185.15/m²\nXGBoost (tuned): MAPE: 8.10%\nXGBoost (tuned): MedAPE: 5.71%\nXGBoost (tuned): R²: 0.7426\n\nLightGBM (tuned): MAE: $128.70/m²\n\nLightGBM (tuned): RMSE: $188.61/m²\n\nLightGBM (tuned): MAPE: 8.26%\n\nLightGBM (tuned): MedAPE: 5.89%\n\nLightGBM (tuned): R²: 0.7329\n\nCatBoost (tuned): MAE: $130.89/m²\n\nCatBoost (tuned): RMSE: $190.77/m²\n\nCatBoost (tuned): MAPE: 8.45%\n\nCatBoost (tuned): MedAPE: 6.02%\n\nCatBoost (tuned): R²: 0.7267\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## Stacking Ensemble\n\n","metadata":{}},{"cell_type":"code","source":"# Ensemble with stacking\nprint(\"\\nTraining Stacking Ensemble...\")\n\nestimators = [\n    ('xgb', XGBRegressor(**xgb_params)),\n    ('lgb', LGBMRegressor(**lgb_params)),\n    ('cat', CatBoostRegressor(**cat_params))\n]\n\nstacking = StackingRegressor(\n    estimators=estimators,\n    final_estimator=Ridge(alpha=1.0),\n    cv=5,\n    n_jobs=-1\n)\n\nstacking.fit(X_train_scaled, y_train)\nensemble_pred = stacking.predict(X_test_scaled)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"ENSEMBLE RESULTS:\")\nprint(\"=\"*50)\nmetrics = calculate_metrics(y_test, ensemble_pred)\nprint(f\"MAE: ${metrics['MAE']:.2f}/m²\")\nprint(f\"RMSE: ${metrics['RMSE']:.2f}/m²\")\nprint(f\"MAPE: {metrics['MAPE']:.2f}%\")\nprint(f\"MedAPE: {metrics['MedAPE']:.2f}%\")\nprint(f\"R²: {metrics['R2']:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T20:47:36.891112Z","iopub.execute_input":"2026-01-10T20:47:36.891432Z","iopub.status.idle":"2026-01-10T20:53:10.246336Z","shell.execute_reply.started":"2026-01-10T20:47:36.891401Z","shell.execute_reply":"2026-01-10T20:53:10.245642Z"}},"outputs":[{"name":"stdout","text":"\nTraining Stacking Ensemble...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nENSEMBLE RESULTS:\n==================================================\nMAE: $125.28/m²\nRMSE: $183.62/m²\nMAPE: 8.02%\nMedAPE: 5.76%\nR²: 0.7468\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Summary\n\n","metadata":{}},{"cell_type":"code","source":"# Print summary\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL SUMMARY\")\nprint(\"=\"*60)\n\nprint(f\"\\nDataset: {len(df_clean)} apartments\")\nprint(f\"Train: {len(X_train)}, Test: {len(X_test)}\")\nprint(f\"Tabular features: {len(feature_cols)}\")\nif CV_FEATURES_AVAILABLE:\n    print(f\"Image features: 64 (ResNet-50 + PCA)\")\n    print(f\"Total features: {X_train_final.shape[1]}\")\nelse:\n    print(\"Image features: Not used\")\n\nprint(f\"\\nFinal Model Performance:\")\nprint(f\"  MAE: ${metrics['MAE']:.2f}/m²\")\nprint(f\"  MedAPE: {metrics['MedAPE']:.2f}%\")\nprint(f\"  R²: {metrics['R2']:.4f}\")\n\nif CV_FEATURES_AVAILABLE:\n    print(f\"\\nComparison with v3 baseline (tabular only):\")\n    print(f\"  v3 baseline: MAE ~$122, R² ~0.76\")\n    print(f\"  v3 + CV:     MAE ${metrics['MAE']:.2f}, R² {metrics['R2']:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T20:53:10.247744Z","iopub.execute_input":"2026-01-10T20:53:10.248043Z","iopub.status.idle":"2026-01-10T20:53:10.254121Z","shell.execute_reply.started":"2026-01-10T20:53:10.248021Z","shell.execute_reply":"2026-01-10T20:53:10.253430Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nFINAL SUMMARY\n============================================================\n\nDataset: 8727 apartments\nTrain: 6981, Test: 1746\nTabular features: 39\nImage features: 64 (ResNet-50 + PCA)\nTotal features: 103\n\nFinal Model Performance:\n  MAE: $125.28/m²\n  MedAPE: 5.76%\n  R²: 0.7468\n\nComparison with v3 baseline (tabular only):\n  v3 baseline: MAE ~$122, R² ~0.76\n  v3 + CV:     MAE $125.28, R² 0.7468\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Save predictions\nresults = pd.DataFrame({\n    'actual': y_test,\n    'predicted': ensemble_pred,\n    'error': y_test - ensemble_pred,\n    'error_pct': (y_test - ensemble_pred) / y_test * 100\n})\n\nresults.to_csv('predictions.csv', index=False)\nprint(\"Predictions saved to predictions.csv\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"DONE!\")\nprint(\"=\"*60)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T20:53:10.255780Z","iopub.execute_input":"2026-01-10T20:53:10.256095Z","iopub.status.idle":"2026-01-10T20:53:10.283948Z","shell.execute_reply.started":"2026-01-10T20:53:10.256065Z","shell.execute_reply":"2026-01-10T20:53:10.283300Z"}},"outputs":[{"name":"stdout","text":"Predictions saved to predictions.csv\n\n============================================================\nDONE!\n============================================================\n","output_type":"stream"}],"execution_count":22}]}