{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Astana Real Estate Price Prediction\n",
    "\n",
    "**Goal:** Predict apartment price per square meter (₸/m²) in Astana, Kazakhstan\n",
    "\n",
    "**Dataset:** 18,293 apartment listings from krisha.kz (January 2025)\n",
    "\n",
    "**Approach:**\n",
    "1. Feature Engineering\n",
    "2. Baseline Models Comparison\n",
    "3. Hyperparameter Tuning (Optuna)\n",
    "4. Final Evaluation & SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (for Kaggle)\n",
    "!pip install -q optuna shap catboost lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold, GroupKFold\nfrom sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nimport optuna\nimport shap\n\nprint(\"Libraries loaded successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from Kaggle dataset\n",
    "df = pd.read_csv('/kaggle/input/astana-real-estate-2025/astana_clean.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick overview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable statistics\n",
    "target = 'price_per_m2_kzt'\n",
    "\n",
    "print(\"Target variable statistics:\")\n",
    "print(f\"  Mean:   {df[target].mean():,.0f} ₸/m²\")\n",
    "print(f\"  Median: {df[target].median():,.0f} ₸/m²\")\n",
    "print(f\"  Std:    {df[target].std():,.0f} ₸/m²\")\n",
    "print(f\"  Min:    {df[target].min():,.0f} ₸/m²\")\n",
    "print(f\"  Max:    {df[target].max():,.0f} ₸/m²\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(df[target], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Price per m² (KZT)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Price Distribution')\n",
    "axes[0].axvline(df[target].median(), color='red', linestyle='--', label=f'Median: {df[target].median():,.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(np.log1p(df[target]), bins=50, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[1].set_xlabel('Log(Price per m²)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Log-transformed Price Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===================\n# POI (Points of Interest) - verified coordinates\n# ===================\n\nPOI = {\n    # Shopping malls\n    'khan_shatyr': (51.1260, 71.4023),\n    'mega_silk_way': (51.0881, 71.4088),\n    'asia_park': (51.1280, 71.4116),\n    'saryarka_mall': (51.1609, 71.4113),\n    'keruen_city': (51.14591, 71.414001),\n    'keruen': (51.128223, 71.424591),\n    'abu_dhabi_plaza': (51.12218, 71.426543),\n    \n    # Key landmarks\n    'baiterek': (51.1283, 71.4305),\n    'akorda': (51.1258, 71.4464),\n    'expo_nur_alem': (51.089487, 71.415327),\n    'nazarbayev_university': (51.0906, 71.3972),\n    'hazrat_sultan_mosque': (51.1250, 71.4722),\n    \n    # Transport\n    'nurly_zhol_station': (51.1124, 71.5318),\n    'astana_1_station': (51.1956, 71.4089),\n    \n    # Markets\n    'astanalyk_bazaar': (51.17283, 71.43662),\n}\n\n# ===================\n# Park polygons\n# ===================\n\nPARKS = {\n    'presidential_park': [\n        (51.138959, 71.435097), (51.133512, 71.434477), (51.132215, 71.440333),\n        (51.131091, 71.445017), (51.12119, 71.441642), (51.112633, 71.438886),\n        (51.103723, 71.453767), (51.100046, 71.468578), (51.100565, 71.486835),\n        (51.104372, 71.486284), (51.109779, 71.476639), (51.110125, 71.462034),\n        (51.114017, 71.455007), (51.120379, 71.455321), (51.119168, 71.473302),\n        (51.123882, 71.475437), (51.12773, 71.459041), (51.135064, 71.460771),\n        (51.138782, 71.452435), (51.140122, 71.444582)\n    ],\n    'central_park': [\n        (51.146409, 71.412506), (51.156925, 71.411967), (51.159836, 71.420565),\n        (51.15151, 71.427833), (51.14756, 71.422292)\n    ],\n    'botanical_garden': [\n        (51.100993, 71.42198), (51.109475, 71.425268), (51.111839, 71.410623),\n        (51.10302, 71.407754)\n    ],\n    'zhetisu_park': [\n        (51.1335, 71.434528), (51.138415, 71.434804), (51.139564, 71.440678),\n        (51.131704, 71.446176)\n    ],\n    'nurzhol_boulevard': [\n        (51.12767, 71.438654), (51.126423, 71.438118), (51.127407, 71.432151),\n        (51.123086, 71.4302), (51.123734, 71.426605), (51.128079, 71.428326),\n        (51.129911, 71.417377), (51.128182, 71.416497), (51.130991, 71.397792),\n        (51.135839, 71.399284), (51.132599, 71.418486), (51.130823, 71.417874),\n        (51.129287, 71.428775), (51.130919, 71.430038), (51.130919, 71.433442)\n    ],\n    'triathlon_park': [\n        (51.134808, 71.454883), (51.1387, 71.452378), (51.139723, 71.445062),\n        (51.138526, 71.444108), (51.132064, 71.448203), (51.132762, 71.454048)\n    ]\n}\n\n# ===================\n# Yesil (Ishim) River - line through Astana\n# ===================\nYESIL_RIVER = [\n    (51.097223, 71.586765), (51.103356, 71.525837), (51.101073, 71.515104),\n    (51.103691, 71.501088), (51.099736, 71.488939), (51.102744, 71.475066),\n    (51.105752, 71.471695), (51.106755, 71.454575), (51.109985, 71.450051),\n    (51.116166, 71.449519), (51.120747, 71.445462), (51.124199, 71.451228),\n    (51.128096, 71.450164), (51.133551, 71.445906), (51.14101, 71.441027),\n    (51.147465, 71.439785), (51.150858, 71.429495), (51.16015, 71.42231),\n    (51.157257, 71.408203), (51.160094, 71.397558), (51.162764, 71.395075),\n    (51.165156, 71.359681)\n]\n\ndef haversine_distance(lat1, lon1, lat2, lon2):\n    \"\"\"Calculate distance between two points in km using Haversine formula\"\"\"\n    R = 6371  # Earth's radius in km\n    \n    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    \n    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n    c = 2 * np.arcsin(np.sqrt(a))\n    \n    return R * c\n\ndef distance_to_polyline(lat, lon, polyline):\n    \"\"\"Calculate minimum distance from point to polyline (river)\"\"\"\n    min_dist = float('inf')\n    \n    for i in range(len(polyline) - 1):\n        p1_lat, p1_lon = polyline[i]\n        p2_lat, p2_lon = polyline[i + 1]\n        \n        # Project point onto line segment\n        # Simplified: check distance to both endpoints and midpoint\n        d1 = haversine_distance(lat, lon, p1_lat, p1_lon)\n        d2 = haversine_distance(lat, lon, p2_lat, p2_lon)\n        \n        # Midpoint\n        mid_lat = (p1_lat + p2_lat) / 2\n        mid_lon = (p1_lon + p2_lon) / 2\n        d_mid = haversine_distance(lat, lon, mid_lat, mid_lon)\n        \n        min_dist = min(min_dist, d1, d2, d_mid)\n    \n    return min_dist\n\ndef point_in_polygon(lat, lon, polygon):\n    \"\"\"Check if point is inside polygon using ray casting algorithm\"\"\"\n    n = len(polygon)\n    inside = False\n    \n    p1_lat, p1_lon = polygon[0]\n    for i in range(1, n + 1):\n        p2_lat, p2_lon = polygon[i % n]\n        if lon > min(p1_lon, p2_lon):\n            if lon <= max(p1_lon, p2_lon):\n                if lat <= max(p1_lat, p2_lat):\n                    if p1_lon != p2_lon:\n                        lat_inters = (lon - p1_lon) * (p2_lat - p1_lat) / (p2_lon - p1_lon) + p1_lat\n                    if p1_lat == p2_lat or lat <= lat_inters:\n                        inside = not inside\n        p1_lat, p1_lon = p2_lat, p2_lon\n    \n    return inside\n\ndef point_in_any_park(lat, lon):\n    \"\"\"Check if point is inside any park\"\"\"\n    for park_name, polygon in PARKS.items():\n        if point_in_polygon(lat, lon, polygon):\n            return True\n    return False\n\nprint(f\"Loaded {len(POI)} POI locations\")\nprint(f\"Loaded {len(PARKS)} park polygons\")\nprint(f\"Loaded Yesil river with {len(YESIL_RIVER)} points\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_features(df, fit_encoders=None, target_col=None):\n    \"\"\"\n    Create all features for the model.\n    \n    Args:\n        df: DataFrame with raw data\n        fit_encoders: Dict with fitted encoders (for test set). If None, creates new encoders.\n        target_col: Target column name (required if fit_encoders is None)\n    \n    Returns:\n        df: DataFrame with features\n        encoders: Dict with fitted encoders (for applying to test set)\n    \"\"\"\n    df = df.copy()\n    encoders = fit_encoders or {}\n    \n    # ===================\n    # Floor features (simplified - remove multicollinear)\n    # ===================\n    df['floor_ratio'] = df['floor'] / df['total_floors']\n    # Removed: is_first_floor, is_last_floor, is_middle_floor (multicollinear with floor_ratio)\n    \n    # ===================\n    # Building features\n    # ===================\n    df['building_age'] = 2025 - df['year_built']\n    df['is_new_building'] = (df['year_built'] >= 2020).astype(int)\n    df['is_highrise'] = (df['total_floors'] >= 10).astype(int)\n    # Removed: is_lowrise (inverse of is_highrise)\n    \n    # ===================\n    # Area features\n    # ===================\n    df['area_per_room'] = df['area'] / df['rooms'].replace(0, 1)\n    df['is_large_apartment'] = (df['area'] >= 100).astype(int)\n    \n    # Kitchen with missing indicator\n    df['kitchen_area_clean'] = pd.to_numeric(df['kitchen_area'], errors='coerce')\n    df['kitchen_ratio'] = df['kitchen_area_clean'] / df['area']\n    df['kitchen_ratio'] = df['kitchen_ratio'].clip(0, 0.5)\n    df['kitchen_missing'] = df['kitchen_ratio'].isna().astype(int)\n    df['kitchen_ratio'] = df['kitchen_ratio'].fillna(df['kitchen_ratio'].median() if fit_encoders is None else encoders.get('kitchen_median', 0.15))\n    if fit_encoders is None:\n        encoders['kitchen_median'] = df['kitchen_ratio'].median()\n    \n    # ===================\n    # Ceiling height with missing indicator\n    # ===================\n    def parse_ceiling(val):\n        if pd.isna(val):\n            return np.nan\n        val = str(val).replace('м', '').replace(',', '.').strip()\n        try:\n            return float(val)\n        except:\n            return np.nan\n    \n    df['ceiling_height_m'] = df['ceiling_height'].apply(parse_ceiling)\n    df['ceiling_missing'] = df['ceiling_height_m'].isna().astype(int)\n    ceiling_median = df['ceiling_height_m'].median() if fit_encoders is None else encoders.get('ceiling_median', 2.7)\n    df['ceiling_height_m'] = df['ceiling_height_m'].fillna(ceiling_median)\n    if fit_encoders is None:\n        encoders['ceiling_median'] = ceiling_median\n    \n    # ===================\n    # Condition - use score only (remove redundant binary flags)\n    # ===================\n    df['condition_score'] = df['condition'].map({\n        'свежий ремонт': 4,\n        'не новый, но аккуратный ремонт': 3,\n        'свободная планировка': 2,\n        'черновая отделка': 1,\n        'требует ремонта': 0\n    })\n    df['condition_missing'] = df['condition_score'].isna().astype(int)\n    df['condition_score'] = df['condition_score'].fillna(2)  # Unknown = средний\n    \n    # ===================\n    # Bathroom - simplified\n    # ===================\n    df['has_2plus_bathrooms'] = (df['bathroom'] == '2 с/у и более').astype(int)\n    df['has_separate_bathroom'] = (df['bathroom'] == 'раздельный').astype(int)\n    \n    # ===================\n    # Balcony - simplified\n    # ===================\n    df['has_balcony'] = df['balcony'].notna().astype(int)\n    df['has_multiple_balconies'] = df['balcony'].str.contains('несколько|и лоджия', na=False).astype(int)\n    \n    # ===================\n    # Parking - simplified\n    # ===================\n    df['has_parking'] = df['parking'].notna().astype(int)\n    \n    # ===================\n    # Floor type - keep only parquet (premium indicator)\n    # ===================\n    df['is_parquet'] = df['floor_type'].str.contains('паркет', na=False).astype(int)\n    \n    # ===================\n    # Security - aggregate score\n    # ===================\n    df['security_score'] = (\n        df['security'].str.contains('охрана', na=False).astype(int) * 2 +\n        df['security'].str.contains('видео', na=False).astype(int) +\n        df['security'].str.contains('домофон', na=False).astype(int)\n    )\n    \n    # ===================\n    # Other features - keep important only\n    # ===================\n    df['is_dormitory'] = (df['former_dormitory'] == 'да').astype(int)\n    df['has_furniture'] = df['furniture'].isin(['полностью', 'частично']).astype(int) if 'furniture' in df.columns else 0\n    \n    # ===================\n    # House type - simplified\n    # ===================\n    df['is_monolith'] = (df['house_type'] == 'монолитный').astype(int)\n    df['is_panel'] = (df['house_type'] == 'панельный').astype(int)\n    \n    # ===================\n    # Residential complex\n    # ===================\n    elite_complexes = [\n        'Хайвил Астана', 'Гранд Астана', 'Abu Dhabi Plaza', 'Абу-Даби Плаза',\n        'Northern Lights', 'Северное сияние', 'Изумрудный квартал',\n        'Millennium Park', 'Premium Tower', 'D Tower', 'Talan Towers'\n    ]\n    df['is_elite_complex'] = df['raw_жилой_комплекс'].isin(elite_complexes).astype(int)\n    df['has_complex_name'] = df['raw_жилой_комплекс'].notna().astype(int)\n    \n    # ===================\n    # Location\n    # ===================\n    df['is_left_bank'] = (df['district'] == 'Есильский р-н').astype(int)\n    \n    # ===================\n    # Distance to Yesil River\n    # ===================\n    df['dist_river'] = df.apply(\n        lambda row: distance_to_polyline(row['latitude'], row['longitude'], YESIL_RIVER),\n        axis=1\n    )\n    df['near_river'] = (df['dist_river'] <= 0.5).astype(int)\n    \n    # ===================\n    # POI distances - keep only aggregates to reduce multicollinearity\n    # ===================\n    lats = df['latitude'].values\n    lons = df['longitude'].values\n    \n    # Key POIs only\n    for poi_name, (poi_lat, poi_lon) in POI.items():\n        df[f'dist_{poi_name}'] = haversine_distance(lats, lons, poi_lat, poi_lon)\n    \n    # Aggregated distances (main features)\n    mall_pois = ['khan_shatyr', 'mega_silk_way', 'asia_park', 'saryarka_mall', 'keruen_city', 'keruen', 'abu_dhabi_plaza']\n    df['dist_nearest_mall'] = df[[f'dist_{p}' for p in mall_pois]].min(axis=1)\n    \n    transport_pois = ['nurly_zhol_station', 'astana_1_station']\n    df['dist_nearest_station'] = df[[f'dist_{p}' for p in transport_pois]].min(axis=1)\n    \n    df['dist_center'] = df['dist_baiterek']\n    \n    # ===================\n    # Park features - aggregated only\n    # ===================\n    df['near_park'] = df.apply(\n        lambda row: point_in_any_park(row['latitude'], row['longitude']), \n        axis=1\n    ).astype(int)\n    \n    for park_name, polygon in PARKS.items():\n        centroid_lat = np.mean([p[0] for p in polygon])\n        centroid_lon = np.mean([p[1] for p in polygon])\n        df[f'dist_{park_name}'] = haversine_distance(lats, lons, centroid_lat, centroid_lon)\n    \n    park_dist_cols = [f'dist_{p}' for p in PARKS.keys()]\n    df['dist_nearest_park'] = df[park_dist_cols].min(axis=1)\n    \n    return df, encoders"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Apply feature engineering (without target encoding - that comes after split)\ndf_features, feature_encoders = create_features(df)\nprint(f\"Features created. New shape: {df_features.shape}\")\n\n# Show distance feature statistics\npoi_dist_cols = [c for c in df_features.columns if c.startswith('dist_')]\nprint(f\"\\nDistance features: {len(poi_dist_cols)}\")\nprint(df_features[poi_dist_cols[:5]].describe().round(2))"
  },
  {
   "cell_type": "code",
   "source": "def create_target_encoders(train_df, cols, target_col, min_samples=5):\n    \"\"\"\n    Create target encoders from training data only (no leakage).\n    Returns dict of {col: {category: encoded_value}}\n    \"\"\"\n    encoders = {}\n    global_mean = train_df[target_col].mean()\n    \n    for col in cols:\n        agg = train_df.groupby(col)[target_col].agg(['mean', 'count'])\n        \n        # Smoothing: blend with global mean based on sample size\n        smoothing_factor = agg['count'] / (agg['count'] + min_samples)\n        smoothed_mean = smoothing_factor * agg['mean'] + (1 - smoothing_factor) * global_mean\n        \n        encoders[col] = {\n            'mapping': smoothed_mean.to_dict(),\n            'global_mean': global_mean\n        }\n    \n    return encoders\n\n\ndef apply_target_encoding(df, encoders):\n    \"\"\"Apply target encoding using pre-fitted encoders (safe for train/test)\"\"\"\n    df = df.copy()\n    \n    for col, encoder in encoders.items():\n        col_mean = f'{col}_price_mean'\n        df[col_mean] = df[col].map(encoder['mapping'])\n        df[col_mean] = df[col_mean].fillna(encoder['global_mean'])\n    \n    return df\n\n\nprint(\"Target encoding functions defined (will apply after train/test split)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===================\n# FEATURE LIST - matches create_features() output\n# ===================\n\n# Core numeric\nnumeric_features = [\n    'rooms', 'area', 'floor', 'total_floors', 'year_built',\n    'latitude', 'longitude'\n]\n\n# Floor & Building (simplified - no multicollinearity)\nfloor_building_features = [\n    'floor_ratio',  # Single feature instead of is_first/last/middle\n    'building_age', 'is_new_building', \n    'is_highrise',  # Removed is_lowrise (inverse)\n]\n\n# Area & Kitchen\narea_features = [\n    'area_per_room', 'is_large_apartment',\n    'kitchen_ratio', 'kitchen_missing',  # Added missing indicator\n]\n\n# Ceiling\nceiling_features = [\n    'ceiling_height_m', 'ceiling_missing',  # Added missing indicator\n]\n\n# Condition (single score + missing indicator)\ncondition_features = [\n    'condition_score', 'condition_missing',  # Simplified from 5 binary flags\n]\n\n# Bathroom (simplified)\nbathroom_features = [\n    'has_separate_bathroom', 'has_2plus_bathrooms',\n]\n\n# Balcony (simplified)\nbalcony_features = [\n    'has_balcony', 'has_multiple_balconies',\n]\n\n# Parking (simplified)\nparking_features = ['has_parking']\n\n# Floor type (only premium indicator)\nfloor_type_features = ['is_parquet']\n\n# Security (aggregate score instead of 4 binary flags)\nsecurity_features = ['security_score']\n\n# Other\nother_features = [\n    'is_dormitory', 'has_furniture',\n]\n\n# House type\nhouse_type_features = ['is_monolith', 'is_panel']\n\n# Residential complex\ncomplex_features = ['is_elite_complex', 'has_complex_name']\n\n# Target encoding (applied after split)\ntarget_encoding_features = [\n    'district_price_mean',\n    'raw_жилой_комплекс_price_mean',\n]\n\n# Location & River\nlocation_features = [\n    'is_left_bank',\n    'dist_river', 'near_river',\n]\n\n# POI distances\npoi_features = [f'dist_{poi}' for poi in POI.keys()] + [\n    'dist_nearest_mall', 'dist_nearest_station', 'dist_center',\n]\n\n# Park features\npark_features = [f'dist_{park}' for park in PARKS.keys()] + [\n    'dist_nearest_park', 'near_park',\n]\n\n# ===================\n# COMBINE ALL FEATURES\n# ===================\nall_features = (\n    numeric_features + \n    floor_building_features + \n    area_features +\n    ceiling_features +\n    condition_features +\n    bathroom_features +\n    balcony_features +\n    parking_features +\n    floor_type_features +\n    security_features +\n    other_features +\n    house_type_features +\n    complex_features +\n    target_encoding_features +\n    location_features +\n    poi_features + \n    park_features\n)\n\nprint(f\"=\" * 50)\nprint(f\"TOTAL FEATURES: {len(all_features)}\")\nprint(f\"=\" * 50)\nprint(f\"\\nBy category:\")\nprint(f\"  Numeric:          {len(numeric_features)}\")\nprint(f\"  Floor/Building:   {len(floor_building_features)}\")\nprint(f\"  Area/Kitchen:     {len(area_features)}\")\nprint(f\"  Ceiling:          {len(ceiling_features)}\")\nprint(f\"  Condition:        {len(condition_features)}\")\nprint(f\"  Bathroom:         {len(bathroom_features)}\")\nprint(f\"  Balcony:          {len(balcony_features)}\")\nprint(f\"  Parking:          {len(parking_features)}\")\nprint(f\"  Floor type:       {len(floor_type_features)}\")\nprint(f\"  Security:         {len(security_features)}\")\nprint(f\"  Other:            {len(other_features)}\")\nprint(f\"  House type:       {len(house_type_features)}\")\nprint(f\"  Complex:          {len(complex_features)}\")\nprint(f\"  Target encoding:  {len(target_encoding_features)}\")\nprint(f\"  Location/River:   {len(location_features)}\")\nprint(f\"  POI:              {len(poi_features)}\")\nprint(f\"  Parks:            {len(park_features)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===================\n# TRAIN/TEST SPLIT FIRST (before target encoding!)\n# ===================\ntarget = 'price_per_m2_kzt'\n\n# Split indices\ntrain_idx, test_idx = train_test_split(\n    df_features.index, test_size=0.2, random_state=42\n)\n\ntrain_df = df_features.loc[train_idx].copy()\ntest_df = df_features.loc[test_idx].copy()\n\nprint(f\"Train set: {len(train_df)} samples\")\nprint(f\"Test set:  {len(test_df)} samples\")\n\n# ===================\n# TARGET ENCODING (fit on train only!)\n# ===================\ntarget_cols = ['district', 'raw_жилой_комплекс']\ntarget_encoders = create_target_encoders(\n    train_df, target_cols, target, min_samples=10\n)\n\n# Apply to both sets\ntrain_df = apply_target_encoding(train_df, target_encoders)\ntest_df = apply_target_encoding(test_df, target_encoders)\n\nprint(\"\\nTarget encoding applied (fitted on train only - no leakage)\")\n\n# Show district price stats from training data\nprint(\"\\nDistrict price statistics (from training data):\")\ndistrict_stats = train_df.groupby('district')[target].agg(['mean', 'median', 'count'])\ndistrict_stats = district_stats.sort_values('median', ascending=False)\nprint(district_stats.round(0))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===================\n# PREPARE FINAL FEATURES\n# ===================\n\n# Check which features are available\navailable_features = [f for f in all_features if f in train_df.columns]\nmissing_features = [f for f in all_features if f not in train_df.columns]\n\nif missing_features:\n    print(f\"Warning: {len(missing_features)} features not found:\")\n    print(f\"  {missing_features}\")\n\nprint(f\"\\nUsing {len(available_features)} features\")\n\n# Create X and y\nX_train = train_df[available_features].values\nX_test = test_df[available_features].values\ny_train = train_df[target].values\ny_test = test_df[target].values\n\n# Replace any remaining NaN with 0\nX_train = np.nan_to_num(X_train, nan=0.0)\nX_test = np.nan_to_num(X_test, nan=0.0)\n\nprint(f\"\\nX_train shape: {X_train.shape}\")\nprint(f\"X_test shape:  {X_test.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"y_test shape:  {y_test.shape}\")"
  },
  {
   "cell_type": "code",
   "source": "# ===================\n# CREATE GROUP LABELS for GroupKFold\n# ===================\n# Apartments in the same residential complex should not be split across train/validation\n# This prevents information leakage during cross-validation\n\n# Create group ID from residential complex (NaN = unique group per sample)\ncomplex_col = 'raw_жилой_комплекс'\ntrain_groups = train_df[complex_col].fillna(\n    'unknown_' + train_df.index.astype(str)\n).values\n\n# Convert to numeric group IDs\nfrom sklearn.preprocessing import LabelEncoder\ngroup_encoder = LabelEncoder()\ntrain_group_ids = group_encoder.fit_transform(train_groups)\n\nn_complexes = len(np.unique(train_group_ids))\nprint(f\"Created {n_complexes} unique groups for GroupKFold\")\nprint(f\"  - Named complexes: {train_df[complex_col].notna().sum()}\")\nprint(f\"  - Unnamed (individual): {train_df[complex_col].isna().sum()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Models Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"Train and evaluate a model, return metrics\"\"\"\n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Train MAE': mean_absolute_error(y_train, y_pred_train),\n",
    "        'Test MAE': mean_absolute_error(y_test, y_pred_test),\n",
    "        'Train R²': r2_score(y_train, y_pred_train),\n",
    "        'Test R²': r2_score(y_test, y_pred_test),\n",
    "        'MAPE (%)': np.mean(np.abs((y_test - y_pred_test) / y_test)) * 100\n",
    "    }\n",
    "    \n",
    "    return metrics, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline models\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(\n",
    "        n_estimators=200, max_depth=15, random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    'XGBoost': XGBRegressor(\n",
    "        n_estimators=300, max_depth=10, learning_rate=0.05, random_state=42\n",
    "    ),\n",
    "    'LightGBM': LGBMRegressor(\n",
    "        n_estimators=300, max_depth=10, learning_rate=0.05, random_state=42, verbose=-1\n",
    "    ),\n",
    "    'CatBoost': CatBoostRegressor(\n",
    "        n_estimators=300, max_depth=10, learning_rate=0.05, random_state=42, verbose=0\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate all models\n",
    "results = []\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    metrics, trained_model = evaluate_model(model, X_train, X_test, y_train, y_test, name)\n",
    "    results.append(metrics)\n",
    "    trained_models[name] = trained_model\n",
    "    print(f\"  Test MAE: {metrics['Test MAE']:,.0f} ₸/m² | Test R²: {metrics['Test R²']:.3f} | MAPE: {metrics['MAPE (%)']:.1f}%\")\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results comparison\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('Test MAE')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BASELINE MODELS COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# MAE comparison\n",
    "colors = ['#2ecc71' if m == results_df['Test MAE'].min() else '#3498db' for m in results_df['Test MAE']]\n",
    "axes[0].barh(results_df['Model'], results_df['Test MAE'], color=colors)\n",
    "axes[0].set_xlabel('MAE (₸/m²)')\n",
    "axes[0].set_title('Test MAE by Model (lower is better)')\n",
    "for i, v in enumerate(results_df['Test MAE']):\n",
    "    axes[0].text(v + 500, i, f'{v:,.0f}', va='center')\n",
    "\n",
    "# R² comparison\n",
    "colors = ['#2ecc71' if r == results_df['Test R²'].max() else '#3498db' for r in results_df['Test R²']]\n",
    "axes[1].barh(results_df['Model'], results_df['Test R²'], color=colors)\n",
    "axes[1].set_xlabel('R²')\n",
    "axes[1].set_title('Test R² by Model (higher is better)')\n",
    "for i, v in enumerate(results_df['Test R²']):\n",
    "    axes[1].text(v + 0.01, i, f'{v:.3f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Feature Selection\n\nRemove low-importance features to reduce overfitting and improve interpretability.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Train a quick LightGBM to get feature importances\nlgbm_selector = LGBMRegressor(n_estimators=100, max_depth=8, random_state=42, verbose=-1)\nlgbm_selector.fit(X_train, y_train)\n\n# Get feature importances\nimportance_df_selection = pd.DataFrame({\n    'feature': available_features,\n    'importance': lgbm_selector.feature_importances_\n}).sort_values('importance', ascending=False)\n\n# Set threshold - keep features with importance > 0.5% of total\nimportance_threshold = 0.005\nimportance_df_selection['importance_pct'] = importance_df_selection['importance'] / importance_df_selection['importance'].sum()\nselected_features = importance_df_selection[importance_df_selection['importance_pct'] >= importance_threshold]['feature'].tolist()\n\nprint(f\"Feature Selection Results:\")\nprint(f\"  Original features: {len(available_features)}\")\nprint(f\"  Selected features: {len(selected_features)} (importance >= {importance_threshold*100}%)\")\nprint(f\"  Removed features:  {len(available_features) - len(selected_features)}\")\n\n# Show removed features\nremoved_features = [f for f in available_features if f not in selected_features]\nif removed_features:\n    print(f\"\\nRemoved low-importance features:\")\n    for f in removed_features:\n        imp = importance_df_selection[importance_df_selection['feature'] == f]['importance_pct'].values[0]\n        print(f\"  - {f}: {imp*100:.2f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Update features with selected subset\nselected_indices = [available_features.index(f) for f in selected_features]\nX_train_selected = X_train[:, selected_indices]\nX_test_selected = X_test[:, selected_indices]\n\nprint(f\"X_train shape: {X_train.shape} -> {X_train_selected.shape}\")\nprint(f\"X_test shape:  {X_test.shape} -> {X_test_selected.shape}\")\n\n# Use selected features for training\nX_train = X_train_selected\nX_test = X_test_selected\navailable_features = selected_features\n\nprint(f\"\\nProceeding with {len(available_features)} selected features\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning (Optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best baseline model for tuning\n",
    "best_baseline = results_df.iloc[0]['Model']\n",
    "print(f\"Best baseline model: {best_baseline}\")\n",
    "print(f\"Proceeding with XGBoost tuning (most stable in practice)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def objective(trial):\n    \"\"\"Optuna objective function for XGBoost with GroupKFold\"\"\"\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n        'max_depth': trial.suggest_int('max_depth', 5, 15),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n        'subsample': trial.suggest_float('subsample', 0.5, 0.9),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.9),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-4, 1.0, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-4, 10.0, log=True),\n        'random_state': 42\n    }\n    \n    model = XGBRegressor(**params)\n    \n    # GroupKFold - keeps same residential complex in same fold (no leakage)\n    gkf = GroupKFold(n_splits=5)\n    \n    scores = []\n    for train_idx, val_idx in gkf.split(X_train, y_train, groups=train_group_ids):\n        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n        \n        model.fit(X_tr, y_tr)\n        y_pred = model.predict(X_val)\n        scores.append(mean_absolute_error(y_val, y_pred))\n    \n    return np.mean(scores)\n\nprint(\"Objective function defined with GroupKFold (no same-complex leakage)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Optuna optimization\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest trial:\")\n",
    "print(f\"  MAE (CV): {study.best_trial.value:,.0f} ₸/m²\")\n",
    "print(f\"\\nBest parameters:\")\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Optimization history\n",
    "trials = [t.value for t in study.trials]\n",
    "best_so_far = [min(trials[:i+1]) for i in range(len(trials))]\n",
    "axes[0].plot(trials, 'o-', alpha=0.5, label='Trial MAE')\n",
    "axes[0].plot(best_so_far, 'r-', linewidth=2, label='Best so far')\n",
    "axes[0].set_xlabel('Trial')\n",
    "axes[0].set_ylabel('MAE (₸/m²)')\n",
    "axes[0].set_title('Optimization History')\n",
    "axes[0].legend()\n",
    "\n",
    "# Parameter importance\n",
    "importance = optuna.importance.get_param_importances(study)\n",
    "params = list(importance.keys())\n",
    "values = list(importance.values())\n",
    "axes[1].barh(params, values, color='steelblue')\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_title('Hyperparameter Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best parameters\n",
    "best_params = study.best_trial.params\n",
    "best_params['random_state'] = 42\n",
    "\n",
    "final_model = XGBRegressor(**best_params)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = final_model.predict(X_train)\n",
    "y_pred_test = final_model.predict(X_test)\n",
    "\n",
    "# Final metrics\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL MODEL RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTrain Set:\")\n",
    "print(f\"  MAE:  {mean_absolute_error(y_train, y_pred_train):,.0f} ₸/m²\")\n",
    "print(f\"  R²:   {r2_score(y_train, y_pred_train):.4f}\")\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  MAE:  {mean_absolute_error(y_test, y_pred_test):,.0f} ₸/m²\")\n",
    "print(f\"  R²:   {r2_score(y_test, y_pred_test):.4f}\")\n",
    "print(f\"  MAPE: {np.mean(np.abs((y_test - y_pred_test) / y_test)) * 100:.2f}%\")\n",
    "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):,.0f} ₸/m²\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction vs Actual plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(y_test, y_pred_test, alpha=0.3, s=10)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)\n",
    "axes[0].set_xlabel('Actual Price (₸/m²)')\n",
    "axes[0].set_ylabel('Predicted Price (₸/m²)')\n",
    "axes[0].set_title('Predicted vs Actual')\n",
    "\n",
    "# Residuals\n",
    "residuals = y_test - y_pred_test\n",
    "axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(0, color='red', linestyle='--')\n",
    "axes[1].set_xlabel('Residual (₸/m²)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title(f'Residuals Distribution (Mean: {residuals.mean():,.0f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance & SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# XGBoost feature importance\nimportance_df = pd.DataFrame({\n    'feature': available_features,\n    'importance': final_model.feature_importances_\n}).sort_values('importance', ascending=False)\n\n# Plot top 15 features\nplt.figure(figsize=(10, 8))\ntop_n = 15\nplt.barh(importance_df['feature'][:top_n][::-1], \n         importance_df['importance'][:top_n][::-1], \n         color='steelblue')\nplt.xlabel('Feature Importance')\nplt.title(f'Top {top_n} Feature Importances (XGBoost)')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nTop 15 features:\")\nfor idx, row in importance_df.head(15).iterrows():\n    print(f\"  {row['feature']:30s}: {row['importance']:.4f} ({row['importance']*100:.1f}%)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP values\n",
    "print(\"Computing SHAP values (this may take a minute)...\")\n",
    "explainer = shap.TreeExplainer(final_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# SHAP summary plot\nplt.figure(figsize=(10, 8))\nshap.summary_plot(shap_values, X_test, feature_names=available_features, show=False)\nplt.title('SHAP Feature Impact on Price Prediction')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# SHAP bar plot (mean absolute impact)\nplt.figure(figsize=(10, 8))\nshap.summary_plot(shap_values, X_test, feature_names=available_features, plot_type='bar', show=False)\nplt.title('Mean Absolute SHAP Values')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Analysis by Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create test dataframe with predictions\ntest_analysis = test_df.copy()\ntest_analysis['predicted'] = y_pred_test\ntest_analysis['error'] = test_analysis[target] - test_analysis['predicted']\ntest_analysis['abs_error'] = np.abs(test_analysis['error'])\ntest_analysis['pct_error'] = test_analysis['abs_error'] / test_analysis[target] * 100"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Error by district\ndistrict_error = test_analysis.groupby('district').agg({\n    'abs_error': 'mean',\n    'pct_error': 'mean',\n    target: 'count'\n}).rename(columns={target: 'count'})\ndistrict_error = district_error.sort_values('abs_error')\n\nprint(\"Error by District:\")\nprint(district_error.round(0))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Error by room count\nroom_error = test_analysis.groupby('rooms').agg({\n    'abs_error': 'mean',\n    'pct_error': 'mean',\n    target: 'count'\n}).rename(columns={target: 'count'})\n\nprint(\"\\nError by Room Count:\")\nprint(room_error.round(0))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Error by price segment\ntest_analysis['price_segment'] = pd.cut(\n    test_analysis[target], \n    bins=[0, 400000, 600000, 800000, 1000000, 2000000],\n    labels=['<400k', '400-600k', '600-800k', '800k-1M', '>1M']\n)\n\nsegment_error = test_analysis.groupby('price_segment').agg({\n    'abs_error': 'mean',\n    'pct_error': 'mean',\n    target: 'count'\n}).rename(columns={target: 'count'})\n\nprint(\"\\nError by Price Segment:\")\nprint(segment_error.round(0))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import joblib\nimport json\n\n# Save model\njoblib.dump(final_model, 'astana_price_model.joblib')\nprint(\"Model saved to: astana_price_model.joblib\")\n\n# Save feature list and encoders\nwith open('model_config.json', 'w', encoding='utf-8') as f:\n    json.dump({\n        'features': available_features,\n        'best_params': best_params,\n        'target_encoders': {\n            col: {\n                'mapping': {str(k): float(v) for k, v in enc['mapping'].items()},\n                'global_mean': float(enc['global_mean'])\n            } for col, enc in target_encoders.items()\n        },\n        'feature_encoders': {\n            'kitchen_median': float(feature_encoders.get('kitchen_median', 0.15)),\n            'ceiling_median': float(feature_encoders.get('ceiling_median', 2.7))\n        },\n        'metrics': {\n            'test_mae': float(mean_absolute_error(y_test, y_pred_test)),\n            'test_r2': float(r2_score(y_test, y_pred_test)),\n            'test_mape': float(np.mean(np.abs((y_test - y_pred_test) / y_test)) * 100)\n        }\n    }, f, indent=2, ensure_ascii=False)\nprint(\"Model config saved to: model_config.json\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*70)\nprint(\"ASTANA REAL ESTATE PRICE PREDICTION - SUMMARY\")\nprint(\"=\"*70)\nprint(f\"\\nDataset: {len(df):,} apartments\")\nprint(f\"Features: {len(available_features)}\")\nprint(f\"Train/Test split: 80/20\")\nprint(f\"\\nBest Model: XGBoost (tuned with Optuna, 50 trials)\")\nprint(f\"\\nFinal Results:\")\nprint(f\"  MAE:  {mean_absolute_error(y_test, y_pred_test):,.0f} KZT/m2\")\nprint(f\"  MAPE: {np.mean(np.abs((y_test - y_pred_test) / y_test)) * 100:.1f}%\")\nprint(f\"  R2:   {r2_score(y_test, y_pred_test):.3f}\")\nprint(f\"\\nInterpretation:\")\nprint(f\"  For an average apartment (60m2, ~35M KZT):\")\nprint(f\"  Expected prediction error: ~{mean_absolute_error(y_test, y_pred_test) * 60 / 1e6:.1f} million KZT\")\nprint(f\"\\nTop 5 Most Important Features:\")\nfor idx, row in importance_df.head(5).iterrows():\n    print(f\"  - {row['feature']} ({row['importance']*100:.1f}%)\")\nprint(f\"\\nKey Improvements:\")\nprint(f\"  - No data leakage (target encoding fit on train only)\")\nprint(f\"  - Reduced multicollinearity (simplified features)\")\nprint(f\"  - Missing value indicators for kitchen/ceiling/condition\")\nprint(\"\\n\" + \"=\"*70)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}