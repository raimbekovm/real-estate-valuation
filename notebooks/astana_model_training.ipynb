{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Astana Real Estate Price Prediction\n",
    "\n",
    "**Goal:** Predict apartment price per square meter (₸/m²) in Astana, Kazakhstan\n",
    "\n",
    "**Dataset:** 18,293 apartment listings from krisha.kz (January 2025)\n",
    "\n",
    "**Approach:**\n",
    "1. Feature Engineering\n",
    "2. Baseline Models Comparison\n",
    "3. Hyperparameter Tuning (Optuna)\n",
    "4. Final Evaluation & SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (for Kaggle)\n",
    "!pip install -q optuna shap catboost lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import optuna\n",
    "import shap\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from Kaggle dataset\n",
    "df = pd.read_csv('/kaggle/input/astana-real-estate-2025/astana_clean.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick overview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable statistics\n",
    "target = 'price_per_m2_kzt'\n",
    "\n",
    "print(\"Target variable statistics:\")\n",
    "print(f\"  Mean:   {df[target].mean():,.0f} ₸/m²\")\n",
    "print(f\"  Median: {df[target].median():,.0f} ₸/m²\")\n",
    "print(f\"  Std:    {df[target].std():,.0f} ₸/m²\")\n",
    "print(f\"  Min:    {df[target].min():,.0f} ₸/m²\")\n",
    "print(f\"  Max:    {df[target].max():,.0f} ₸/m²\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(df[target], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Price per m² (KZT)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Price Distribution')\n",
    "axes[0].axvline(df[target].median(), color='red', linestyle='--', label=f'Median: {df[target].median():,.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(np.log1p(df[target]), bins=50, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[1].set_xlabel('Log(Price per m²)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Log-transformed Price Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===================\n# POI (Points of Interest) - verified coordinates\n# ===================\n\nPOI = {\n    # Shopping malls\n    'khan_shatyr': (51.1260, 71.4023),\n    'mega_silk_way': (51.0881, 71.4088),\n    'asia_park': (51.1280, 71.4116),\n    'saryarka_mall': (51.1609, 71.4113),\n    'keruen_city': (51.14591, 71.414001),\n    'keruen': (51.128223, 71.424591),\n    'abu_dhabi_plaza': (51.12218, 71.426543),\n    \n    # Key landmarks\n    'baiterek': (51.1283, 71.4305),\n    'akorda': (51.1258, 71.4464),\n    'expo_nur_alem': (51.089487, 71.415327),\n    'nazarbayev_university': (51.0906, 71.3972),\n    'hazrat_sultan_mosque': (51.1250, 71.4722),\n    \n    # Transport\n    'nurly_zhol_station': (51.1124, 71.5318),\n    'astana_1_station': (51.1956, 71.4089),\n    \n    # Markets\n    'astanalyk_bazaar': (51.17283, 71.43662),\n}\n\n# ===================\n# Park polygons\n# ===================\n\nPARKS = {\n    'presidential_park': [\n        (51.138959, 71.435097), (51.133512, 71.434477), (51.132215, 71.440333),\n        (51.131091, 71.445017), (51.12119, 71.441642), (51.112633, 71.438886),\n        (51.103723, 71.453767), (51.100046, 71.468578), (51.100565, 71.486835),\n        (51.104372, 71.486284), (51.109779, 71.476639), (51.110125, 71.462034),\n        (51.114017, 71.455007), (51.120379, 71.455321), (51.119168, 71.473302),\n        (51.123882, 71.475437), (51.12773, 71.459041), (51.135064, 71.460771),\n        (51.138782, 71.452435), (51.140122, 71.444582)\n    ],\n    'central_park': [\n        (51.146409, 71.412506), (51.156925, 71.411967), (51.159836, 71.420565),\n        (51.15151, 71.427833), (51.14756, 71.422292)\n    ],\n    'botanical_garden': [\n        (51.100993, 71.42198), (51.109475, 71.425268), (51.111839, 71.410623),\n        (51.10302, 71.407754)\n    ],\n    'zhetisu_park': [\n        (51.1335, 71.434528), (51.138415, 71.434804), (51.139564, 71.440678),\n        (51.131704, 71.446176)\n    ],\n    'nurzhol_boulevard': [\n        (51.12767, 71.438654), (51.126423, 71.438118), (51.127407, 71.432151),\n        (51.123086, 71.4302), (51.123734, 71.426605), (51.128079, 71.428326),\n        (51.129911, 71.417377), (51.128182, 71.416497), (51.130991, 71.397792),\n        (51.135839, 71.399284), (51.132599, 71.418486), (51.130823, 71.417874),\n        (51.129287, 71.428775), (51.130919, 71.430038), (51.130919, 71.433442)\n    ],\n    'triathlon_park': [\n        (51.134808, 71.454883), (51.1387, 71.452378), (51.139723, 71.445062),\n        (51.138526, 71.444108), (51.132064, 71.448203), (51.132762, 71.454048)\n    ]\n}\n\n# ===================\n# Yesil (Ishim) River - line through Astana\n# ===================\nYESIL_RIVER = [\n    (51.097223, 71.586765), (51.103356, 71.525837), (51.101073, 71.515104),\n    (51.103691, 71.501088), (51.099736, 71.488939), (51.102744, 71.475066),\n    (51.105752, 71.471695), (51.106755, 71.454575), (51.109985, 71.450051),\n    (51.116166, 71.449519), (51.120747, 71.445462), (51.124199, 71.451228),\n    (51.128096, 71.450164), (51.133551, 71.445906), (51.14101, 71.441027),\n    (51.147465, 71.439785), (51.150858, 71.429495), (51.16015, 71.42231),\n    (51.157257, 71.408203), (51.160094, 71.397558), (51.162764, 71.395075),\n    (51.165156, 71.359681)\n]\n\ndef haversine_distance(lat1, lon1, lat2, lon2):\n    \"\"\"Calculate distance between two points in km using Haversine formula\"\"\"\n    R = 6371  # Earth's radius in km\n    \n    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    \n    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n    c = 2 * np.arcsin(np.sqrt(a))\n    \n    return R * c\n\ndef distance_to_polyline(lat, lon, polyline):\n    \"\"\"Calculate minimum distance from point to polyline (river)\"\"\"\n    min_dist = float('inf')\n    \n    for i in range(len(polyline) - 1):\n        p1_lat, p1_lon = polyline[i]\n        p2_lat, p2_lon = polyline[i + 1]\n        \n        # Project point onto line segment\n        # Simplified: check distance to both endpoints and midpoint\n        d1 = haversine_distance(lat, lon, p1_lat, p1_lon)\n        d2 = haversine_distance(lat, lon, p2_lat, p2_lon)\n        \n        # Midpoint\n        mid_lat = (p1_lat + p2_lat) / 2\n        mid_lon = (p1_lon + p2_lon) / 2\n        d_mid = haversine_distance(lat, lon, mid_lat, mid_lon)\n        \n        min_dist = min(min_dist, d1, d2, d_mid)\n    \n    return min_dist\n\ndef point_in_polygon(lat, lon, polygon):\n    \"\"\"Check if point is inside polygon using ray casting algorithm\"\"\"\n    n = len(polygon)\n    inside = False\n    \n    p1_lat, p1_lon = polygon[0]\n    for i in range(1, n + 1):\n        p2_lat, p2_lon = polygon[i % n]\n        if lon > min(p1_lon, p2_lon):\n            if lon <= max(p1_lon, p2_lon):\n                if lat <= max(p1_lat, p2_lat):\n                    if p1_lon != p2_lon:\n                        lat_inters = (lon - p1_lon) * (p2_lat - p1_lat) / (p2_lon - p1_lon) + p1_lat\n                    if p1_lat == p2_lat or lat <= lat_inters:\n                        inside = not inside\n        p1_lat, p1_lon = p2_lat, p2_lon\n    \n    return inside\n\ndef point_in_any_park(lat, lon):\n    \"\"\"Check if point is inside any park\"\"\"\n    for park_name, polygon in PARKS.items():\n        if point_in_polygon(lat, lon, polygon):\n            return True\n    return False\n\nprint(f\"Loaded {len(POI)} POI locations\")\nprint(f\"Loaded {len(PARKS)} park polygons\")\nprint(f\"Loaded Yesil river with {len(YESIL_RIVER)} points\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_features(df):\n    \"\"\"Create all features for the model\"\"\"\n    df = df.copy()\n    \n    # ===================\n    # Floor features\n    # ===================\n    df['floor_ratio'] = df['floor'] / df['total_floors']\n    df['is_first_floor'] = (df['floor'] == 1).astype(int)\n    df['is_last_floor'] = (df['floor'] == df['total_floors']).astype(int)\n    df['is_middle_floor'] = ((df['floor'] > 1) & (df['floor'] < df['total_floors'])).astype(int)\n    \n    # ===================\n    # Building features\n    # ===================\n    df['building_age'] = 2025 - df['year_built']\n    df['is_new_building'] = (df['year_built'] >= 2020).astype(int)\n    df['is_highrise'] = (df['total_floors'] >= 10).astype(int)\n    df['is_lowrise'] = (df['total_floors'] <= 5).astype(int)\n    df['building_decade'] = (df['year_built'] // 10) * 10\n    \n    # ===================\n    # Area features\n    # ===================\n    df['area_per_room'] = df['area'] / df['rooms'].replace(0, 1)\n    df['is_large_apartment'] = (df['area'] >= 100).astype(int)\n    df['is_studio'] = (df['rooms'] == 1).astype(int)\n    \n    # Kitchen area\n    df['kitchen_area_clean'] = pd.to_numeric(df['kitchen_area'], errors='coerce')\n    df['kitchen_ratio'] = df['kitchen_area_clean'] / df['area']\n    df['kitchen_ratio'] = df['kitchen_ratio'].clip(0, 0.5).fillna(0)\n    \n    # ===================\n    # Ceiling height\n    # ===================\n    def parse_ceiling(val):\n        if pd.isna(val):\n            return np.nan\n        val = str(val).replace('м', '').replace(',', '.').strip()\n        try:\n            return float(val)\n        except:\n            return np.nan\n    \n    df['ceiling_height_m'] = df['ceiling_height'].apply(parse_ceiling)\n    df['is_high_ceiling'] = (df['ceiling_height_m'] >= 3.0).astype(int)\n    \n    # ===================\n    # Condition (важно!)\n    # ===================\n    df['is_fresh_repair'] = (df['condition'] == 'свежий ремонт').astype(int)\n    df['is_good_condition'] = (df['condition'] == 'не новый, но аккуратный ремонт').astype(int)\n    df['is_shell_finish'] = (df['condition'] == 'черновая отделка').astype(int)\n    df['needs_repair'] = (df['condition'] == 'требует ремонта').astype(int)\n    df['condition_score'] = df['condition'].map({\n        'свежий ремонт': 4,\n        'не новый, но аккуратный ремонт': 3,\n        'свободная планировка': 2,\n        'черновая отделка': 1,\n        'требует ремонта': 0\n    }).fillna(2)\n    \n    # ===================\n    # Bathroom features\n    # ===================\n    df['has_separate_bathroom'] = (df['bathroom'] == 'раздельный').astype(int)\n    df['has_2plus_bathrooms'] = (df['bathroom'] == '2 с/у и более').astype(int)\n    df['has_combined_bathroom'] = (df['bathroom'] == 'совмещенный').astype(int)\n    \n    # ===================\n    # Balcony features\n    # ===================\n    df['has_balcony'] = df['balcony'].notna().astype(int)\n    df['has_loggia'] = df['balcony'].str.contains('лоджия', na=False).astype(int)\n    df['has_multiple_balconies'] = df['balcony'].str.contains('несколько|и лоджия', na=False).astype(int)\n    df['is_balcony_glazed'] = (df['balcony_glazed'] == 'да').astype(int)\n    \n    # ===================\n    # Parking features\n    # ===================\n    df['has_parking'] = df['parking'].notna().astype(int)\n    df['has_underground_parking'] = df['parking'].str.contains('паркинг', na=False).astype(int)\n    df['has_garage'] = df['parking'].str.contains('гараж', na=False).astype(int)\n    \n    # ===================\n    # Floor type\n    # ===================\n    df['is_laminate'] = df['floor_type'].str.contains('ламинат', na=False).astype(int)\n    df['is_parquet'] = df['floor_type'].str.contains('паркет', na=False).astype(int)\n    df['is_linoleum'] = df['floor_type'].str.contains('линолеум', na=False).astype(int)\n    \n    # ===================\n    # Security features\n    # ===================\n    df['has_security'] = df['security'].str.contains('охрана', na=False).astype(int)\n    df['has_video'] = df['security'].str.contains('видео', na=False).astype(int)\n    df['has_intercom'] = df['security'].str.contains('домофон', na=False).astype(int)\n    df['has_code_lock'] = df['security'].str.contains('кодовый', na=False).astype(int)\n    \n    # ===================\n    # Door features\n    # ===================\n    df['is_armored_door'] = df['entrance_door'].str.contains('бронированная', na=False).astype(int)\n    df['is_metal_door'] = df['entrance_door'].str.contains('металлическая', na=False).astype(int)\n    \n    # ===================\n    # Other features\n    # ===================\n    df['is_dormitory'] = (df['former_dormitory'] == 'да').astype(int)\n    df['exchange_possible'] = (df['exchange_possible'] == 'Да').astype(int)\n    \n    if 'furniture' in df.columns:\n        df['has_furniture'] = df['furniture'].isin(['полностью', 'частично']).astype(int)\n        df['has_full_furniture'] = (df['furniture'] == 'полностью').astype(int)\n    else:\n        df['has_furniture'] = 0\n        df['has_full_furniture'] = 0\n    \n    # ===================\n    # House type encoding\n    # ===================\n    df['is_monolith'] = (df['house_type'] == 'монолитный').astype(int)\n    df['is_brick'] = (df['house_type'] == 'кирпичный').astype(int)\n    df['is_panel'] = (df['house_type'] == 'панельный').astype(int)\n    \n    # ===================\n    # Elite residential complexes\n    # ===================\n    elite_complexes = [\n        'Хайвил Астана', 'Гранд Астана', 'Abu Dhabi Plaza', 'Абу-Даби Плаза',\n        'Northern Lights', 'Северное сияние', 'Изумрудный квартал',\n        'Millennium Park', 'Premium Tower', 'D Tower', 'Talan Towers'\n    ]\n    df['is_elite_complex'] = df['raw_жилой_комплекс'].isin(elite_complexes).astype(int)\n    df['has_complex_name'] = df['raw_жилой_комплекс'].notna().astype(int)\n    \n    # ===================\n    # Location features\n    # ===================\n    df['is_left_bank'] = (df['district'] == 'Есильский р-н').astype(int)\n    \n    # ===================\n    # Distance to Yesil River (embankment = +price)\n    # ===================\n    df['dist_river'] = df.apply(\n        lambda row: distance_to_polyline(row['latitude'], row['longitude'], YESIL_RIVER),\n        axis=1\n    )\n    df['near_river'] = (df['dist_river'] <= 0.5).astype(int)  # Within 500m\n    \n    # ===================\n    # POI distance features\n    # ===================\n    lats = df['latitude'].values\n    lons = df['longitude'].values\n    \n    for poi_name, (poi_lat, poi_lon) in POI.items():\n        df[f'dist_{poi_name}'] = haversine_distance(lats, lons, poi_lat, poi_lon)\n    \n    mall_pois = ['khan_shatyr', 'mega_silk_way', 'asia_park', 'saryarka_mall', 'keruen_city', 'keruen', 'abu_dhabi_plaza']\n    df['dist_nearest_mall'] = df[[f'dist_{p}' for p in mall_pois]].min(axis=1)\n    \n    transport_pois = ['nurly_zhol_station', 'astana_1_station']\n    df['dist_nearest_station'] = df[[f'dist_{p}' for p in transport_pois]].min(axis=1)\n    \n    # ===================\n    # Park features\n    # ===================\n    df['near_park'] = df.apply(\n        lambda row: point_in_any_park(row['latitude'], row['longitude']), \n        axis=1\n    ).astype(int)\n    \n    for park_name, polygon in PARKS.items():\n        centroid_lat = np.mean([p[0] for p in polygon])\n        centroid_lon = np.mean([p[1] for p in polygon])\n        df[f'dist_{park_name}'] = haversine_distance(lats, lons, centroid_lat, centroid_lon)\n    \n    park_dist_cols = [f'dist_{p}' for p in PARKS.keys()]\n    df['dist_nearest_park'] = df[park_dist_cols].min(axis=1)\n    \n    df['distance_to_center'] = df['dist_baiterek']\n    \n    return df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Apply feature engineering\ndf_features = create_features(df)\nprint(f\"Features created. New shape: {df_features.shape}\")\n\n# Show POI distance statistics\npoi_dist_cols = [c for c in df_features.columns if c.startswith('dist_')]\nprint(f\"\\nPOI distance features: {len(poi_dist_cols)}\")\nprint(df_features[poi_dist_cols[:5]].describe().round(2))"
  },
  {
   "cell_type": "code",
   "source": "def target_encode_cv(df, col, target_col, n_splits=5, min_samples=5):\n    \"\"\"\n    Target encoding with cross-validation to prevent data leakage.\n    Returns mean price per category with smoothing.\n    \"\"\"\n    df = df.copy()\n    col_mean = f'{col}_price_mean'\n    df[col_mean] = np.nan\n    \n    # Global mean for smoothing\n    global_mean = df[target_col].mean()\n    \n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    \n    for train_idx, val_idx in kf.split(df):\n        train_df = df.iloc[train_idx]\n        \n        # Calculate means and counts\n        agg = train_df.groupby(col)[target_col].agg(['mean', 'count'])\n        \n        # Smoothing: blend with global mean based on sample size\n        smoothing_factor = agg['count'] / (agg['count'] + min_samples)\n        smoothed_mean = smoothing_factor * agg['mean'] + (1 - smoothing_factor) * global_mean\n        \n        df.loc[df.index[val_idx], col_mean] = df.iloc[val_idx][col].map(smoothed_mean)\n    \n    # Fill NaN with global mean\n    df[col_mean] = df[col_mean].fillna(global_mean)\n    \n    return df",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define feature list\nnumeric_features = [\n    'rooms', 'area', 'floor', 'total_floors', 'year_built',\n    'latitude', 'longitude'\n]\n\n# Floor & Building\nfloor_building_features = [\n    'floor_ratio', 'is_first_floor', 'is_last_floor', 'is_middle_floor',\n    'building_age', 'is_new_building', 'is_highrise', 'is_lowrise',\n]\n\n# Area & Kitchen\narea_features = [\n    'area_per_room', 'is_large_apartment', 'is_studio',\n    'kitchen_ratio',\n]\n\n# Ceiling\nceiling_features = [\n    'ceiling_height_m', 'is_high_ceiling',\n]\n\n# Condition (важно!)\ncondition_features = [\n    'is_fresh_repair', 'is_good_condition', 'is_shell_finish', \n    'needs_repair', 'condition_score',\n]\n\n# Bathroom\nbathroom_features = [\n    'has_separate_bathroom', 'has_2plus_bathrooms', 'has_combined_bathroom',\n]\n\n# Balcony\nbalcony_features = [\n    'has_balcony', 'has_loggia', 'has_multiple_balconies', 'is_balcony_glazed',\n]\n\n# Parking\nparking_features = [\n    'has_parking', 'has_underground_parking', 'has_garage',\n]\n\n# Floor type\nfloor_type_features = [\n    'is_laminate', 'is_parquet', 'is_linoleum',\n]\n\n# Security\nsecurity_features = [\n    'has_security', 'has_video', 'has_intercom', 'has_code_lock',\n]\n\n# Door & Other\nother_features = [\n    'is_armored_door', 'is_metal_door',\n    'is_dormitory', 'exchange_possible',\n    'has_furniture', 'has_full_furniture',\n]\n\n# House type\nhouse_type_features = [\n    'is_monolith', 'is_brick', 'is_panel',\n]\n\n# Elite complex\ncomplex_features = [\n    'is_elite_complex', 'has_complex_name',\n]\n\n# Location\nlocation_features = [\n    'is_left_bank', 'distance_to_center', 'district_price_mean',\n]\n\n# POI distances\npoi_features = [f'dist_{poi}' for poi in POI.keys()] + [\n    'dist_nearest_mall', 'dist_nearest_station',\n]\n\n# Park features\npark_features = [f'dist_{park}' for park in PARKS.keys()] + [\n    'dist_nearest_park', 'near_park',\n]\n\n# Combine all\nall_features = (\n    numeric_features + \n    floor_building_features + \n    area_features +\n    ceiling_features +\n    condition_features +\n    bathroom_features +\n    balcony_features +\n    parking_features +\n    floor_type_features +\n    security_features +\n    other_features +\n    house_type_features +\n    complex_features +\n    location_features +\n    poi_features + \n    park_features\n)\n\nprint(f\"Total features: {len(all_features)}\")\nprint(f\"\\nBy category:\")\nprint(f\"  Numeric:        {len(numeric_features)}\")\nprint(f\"  Floor/Building: {len(floor_building_features)}\")\nprint(f\"  Area/Kitchen:   {len(area_features)}\")\nprint(f\"  Ceiling:        {len(ceiling_features)}\")\nprint(f\"  Condition:      {len(condition_features)}\")\nprint(f\"  Bathroom:       {len(bathroom_features)}\")\nprint(f\"  Balcony:        {len(balcony_features)}\")\nprint(f\"  Parking:        {len(parking_features)}\")\nprint(f\"  Floor type:     {len(floor_type_features)}\")\nprint(f\"  Security:       {len(security_features)}\")\nprint(f\"  Other:          {len(other_features)}\")\nprint(f\"  House type:     {len(house_type_features)}\")\nprint(f\"  Complex:        {len(complex_features)}\")\nprint(f\"  Location:       {len(location_features)}\")\nprint(f\"  POI:            {len(poi_features)}\")\nprint(f\"  Parks:          {len(park_features)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Apply target encoding for district\ndf_features = target_encode_cv(df_features, 'district', 'price_per_m2_kzt')\nprint(\"Target encoding applied for district\")\n\n# Apply target encoding for residential complex (ЖК)\ndf_features = target_encode_cv(df_features, 'raw_жилой_комплекс', 'price_per_m2_kzt', min_samples=10)\nprint(\"Target encoding applied for residential complex\")\n\n# Show district price means\nprint(\"\\nDistrict price statistics:\")\ndistrict_stats = df_features.groupby('district')['price_per_m2_kzt'].agg(['mean', 'median', 'count'])\ndistrict_stats = district_stats.sort_values('median', ascending=False)\nprint(district_stats.round(0))\n\n# Show top residential complexes by price\nprint(\"\\nTop 10 residential complexes by median price:\")\ncomplex_stats = df_features.groupby('raw_жилой_комплекс')['price_per_m2_kzt'].agg(['median', 'count'])\ncomplex_stats = complex_stats[complex_stats['count'] >= 10].sort_values('median', ascending=False)\nprint(complex_stats.head(10).round(0))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define feature list\nnumeric_features = [\n    'rooms', 'area', 'floor', 'total_floors', 'year_built',\n    'latitude', 'longitude'\n]\n\nfloor_building_features = [\n    'floor_ratio', 'is_first_floor', 'is_last_floor', 'is_middle_floor',\n    'building_age', 'is_new_building', 'is_highrise', 'is_lowrise',\n]\n\narea_features = [\n    'area_per_room', 'is_large_apartment', 'is_studio', 'kitchen_ratio',\n]\n\nceiling_features = ['ceiling_height_m', 'is_high_ceiling']\n\ncondition_features = [\n    'is_fresh_repair', 'is_good_condition', 'is_shell_finish', \n    'needs_repair', 'condition_score',\n]\n\nbathroom_features = ['has_separate_bathroom', 'has_2plus_bathrooms', 'has_combined_bathroom']\n\nbalcony_features = ['has_balcony', 'has_loggia', 'has_multiple_balconies', 'is_balcony_glazed']\n\nparking_features = ['has_parking', 'has_underground_parking', 'has_garage']\n\nfloor_type_features = ['is_laminate', 'is_parquet', 'is_linoleum']\n\nsecurity_features = ['has_security', 'has_video', 'has_intercom', 'has_code_lock']\n\nother_features = [\n    'is_armored_door', 'is_metal_door', 'is_dormitory', 'exchange_possible',\n    'has_furniture', 'has_full_furniture',\n]\n\nhouse_type_features = ['is_monolith', 'is_brick', 'is_panel']\n\ncomplex_features = ['is_elite_complex', 'has_complex_name']\n\n# Target encoding features\ntarget_encoding_features = [\n    'district_price_mean',\n    'raw_жилой_комплекс_price_mean',  # ЖК target encoding\n]\n\n# Location & River\nlocation_features = [\n    'is_left_bank', 'distance_to_center',\n    'dist_river', 'near_river',  # River features\n]\n\n# POI distances\npoi_features = [f'dist_{poi}' for poi in POI.keys()] + [\n    'dist_nearest_mall', 'dist_nearest_station',\n]\n\n# Park features\npark_features = [f'dist_{park}' for park in PARKS.keys()] + [\n    'dist_nearest_park', 'near_park',\n]\n\n# Combine all\nall_features = (\n    numeric_features + \n    floor_building_features + \n    area_features +\n    ceiling_features +\n    condition_features +\n    bathroom_features +\n    balcony_features +\n    parking_features +\n    floor_type_features +\n    security_features +\n    other_features +\n    house_type_features +\n    complex_features +\n    target_encoding_features +\n    location_features +\n    poi_features + \n    park_features\n)\n\nprint(f\"=\" * 50)\nprint(f\"TOTAL FEATURES: {len(all_features)}\")\nprint(f\"=\" * 50)\nprint(f\"\\nBy category:\")\nprint(f\"  Numeric:          {len(numeric_features)}\")\nprint(f\"  Floor/Building:   {len(floor_building_features)}\")\nprint(f\"  Area/Kitchen:     {len(area_features)}\")\nprint(f\"  Ceiling:          {len(ceiling_features)}\")\nprint(f\"  Condition:        {len(condition_features)}\")\nprint(f\"  Bathroom:         {len(bathroom_features)}\")\nprint(f\"  Balcony:          {len(balcony_features)}\")\nprint(f\"  Parking:          {len(parking_features)}\")\nprint(f\"  Floor type:       {len(floor_type_features)}\")\nprint(f\"  Security:         {len(security_features)}\")\nprint(f\"  Other:            {len(other_features)}\")\nprint(f\"  House type:       {len(house_type_features)}\")\nprint(f\"  Complex:          {len(complex_features)}\")\nprint(f\"  Target encoding:  {len(target_encoding_features)}\")\nprint(f\"  Location/River:   {len(location_features)}\")\nprint(f\"  POI:              {len(poi_features)}\")\nprint(f\"  Parks:            {len(park_features)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set:  {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Models Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"Train and evaluate a model, return metrics\"\"\"\n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Train MAE': mean_absolute_error(y_train, y_pred_train),\n",
    "        'Test MAE': mean_absolute_error(y_test, y_pred_test),\n",
    "        'Train R²': r2_score(y_train, y_pred_train),\n",
    "        'Test R²': r2_score(y_test, y_pred_test),\n",
    "        'MAPE (%)': np.mean(np.abs((y_test - y_pred_test) / y_test)) * 100\n",
    "    }\n",
    "    \n",
    "    return metrics, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline models\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(\n",
    "        n_estimators=200, max_depth=15, random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    'XGBoost': XGBRegressor(\n",
    "        n_estimators=300, max_depth=10, learning_rate=0.05, random_state=42\n",
    "    ),\n",
    "    'LightGBM': LGBMRegressor(\n",
    "        n_estimators=300, max_depth=10, learning_rate=0.05, random_state=42, verbose=-1\n",
    "    ),\n",
    "    'CatBoost': CatBoostRegressor(\n",
    "        n_estimators=300, max_depth=10, learning_rate=0.05, random_state=42, verbose=0\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate all models\n",
    "results = []\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    metrics, trained_model = evaluate_model(model, X_train, X_test, y_train, y_test, name)\n",
    "    results.append(metrics)\n",
    "    trained_models[name] = trained_model\n",
    "    print(f\"  Test MAE: {metrics['Test MAE']:,.0f} ₸/m² | Test R²: {metrics['Test R²']:.3f} | MAPE: {metrics['MAPE (%)']:.1f}%\")\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results comparison\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('Test MAE')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BASELINE MODELS COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# MAE comparison\n",
    "colors = ['#2ecc71' if m == results_df['Test MAE'].min() else '#3498db' for m in results_df['Test MAE']]\n",
    "axes[0].barh(results_df['Model'], results_df['Test MAE'], color=colors)\n",
    "axes[0].set_xlabel('MAE (₸/m²)')\n",
    "axes[0].set_title('Test MAE by Model (lower is better)')\n",
    "for i, v in enumerate(results_df['Test MAE']):\n",
    "    axes[0].text(v + 500, i, f'{v:,.0f}', va='center')\n",
    "\n",
    "# R² comparison\n",
    "colors = ['#2ecc71' if r == results_df['Test R²'].max() else '#3498db' for r in results_df['Test R²']]\n",
    "axes[1].barh(results_df['Model'], results_df['Test R²'], color=colors)\n",
    "axes[1].set_xlabel('R²')\n",
    "axes[1].set_title('Test R² by Model (higher is better)')\n",
    "for i, v in enumerate(results_df['Test R²']):\n",
    "    axes[1].text(v + 0.01, i, f'{v:.3f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning (Optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best baseline model for tuning\n",
    "best_baseline = results_df.iloc[0]['Model']\n",
    "print(f\"Best baseline model: {best_baseline}\")\n",
    "print(f\"Proceeding with XGBoost tuning (most stable in practice)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"Optuna objective function for XGBoost\"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 0.9),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.9),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-4, 1.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-4, 10.0, log=True),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    model = XGBRegressor(**params)\n",
    "    \n",
    "    # 5-Fold Cross-validation\n",
    "    scores = cross_val_score(\n",
    "        model, X_train, y_train, \n",
    "        cv=5, scoring='neg_mean_absolute_error', n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    return -scores.mean()  # Return positive MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Optuna optimization\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest trial:\")\n",
    "print(f\"  MAE (CV): {study.best_trial.value:,.0f} ₸/m²\")\n",
    "print(f\"\\nBest parameters:\")\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Optimization history\n",
    "trials = [t.value for t in study.trials]\n",
    "best_so_far = [min(trials[:i+1]) for i in range(len(trials))]\n",
    "axes[0].plot(trials, 'o-', alpha=0.5, label='Trial MAE')\n",
    "axes[0].plot(best_so_far, 'r-', linewidth=2, label='Best so far')\n",
    "axes[0].set_xlabel('Trial')\n",
    "axes[0].set_ylabel('MAE (₸/m²)')\n",
    "axes[0].set_title('Optimization History')\n",
    "axes[0].legend()\n",
    "\n",
    "# Parameter importance\n",
    "importance = optuna.importance.get_param_importances(study)\n",
    "params = list(importance.keys())\n",
    "values = list(importance.values())\n",
    "axes[1].barh(params, values, color='steelblue')\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_title('Hyperparameter Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best parameters\n",
    "best_params = study.best_trial.params\n",
    "best_params['random_state'] = 42\n",
    "\n",
    "final_model = XGBRegressor(**best_params)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = final_model.predict(X_train)\n",
    "y_pred_test = final_model.predict(X_test)\n",
    "\n",
    "# Final metrics\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL MODEL RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTrain Set:\")\n",
    "print(f\"  MAE:  {mean_absolute_error(y_train, y_pred_train):,.0f} ₸/m²\")\n",
    "print(f\"  R²:   {r2_score(y_train, y_pred_train):.4f}\")\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  MAE:  {mean_absolute_error(y_test, y_pred_test):,.0f} ₸/m²\")\n",
    "print(f\"  R²:   {r2_score(y_test, y_pred_test):.4f}\")\n",
    "print(f\"  MAPE: {np.mean(np.abs((y_test - y_pred_test) / y_test)) * 100:.2f}%\")\n",
    "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):,.0f} ₸/m²\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction vs Actual plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(y_test, y_pred_test, alpha=0.3, s=10)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)\n",
    "axes[0].set_xlabel('Actual Price (₸/m²)')\n",
    "axes[0].set_ylabel('Predicted Price (₸/m²)')\n",
    "axes[0].set_title('Predicted vs Actual')\n",
    "\n",
    "# Residuals\n",
    "residuals = y_test - y_pred_test\n",
    "axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(0, color='red', linestyle='--')\n",
    "axes[1].set_xlabel('Residual (₸/m²)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title(f'Residuals Distribution (Mean: {residuals.mean():,.0f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance & SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': all_features,\n",
    "    'importance': final_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 15 features\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_n = 15\n",
    "plt.barh(importance_df['feature'][:top_n][::-1], \n",
    "         importance_df['importance'][:top_n][::-1], \n",
    "         color='steelblue')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title(f'Top {top_n} Feature Importances (XGBoost)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 15 features:\")\n",
    "for i, row in importance_df.head(15).iterrows():\n",
    "    print(f\"  {row['feature']:25s}: {row['importance']:.4f} ({row['importance']*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP values\n",
    "print(\"Computing SHAP values (this may take a minute)...\")\n",
    "explainer = shap.TreeExplainer(final_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP summary plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X_test, feature_names=all_features, show=False)\n",
    "plt.title('SHAP Feature Impact on Price Prediction')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP bar plot (mean absolute impact)\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X_test, feature_names=all_features, plot_type='bar', show=False)\n",
    "plt.title('Mean Absolute SHAP Values')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Analysis by Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataframe with predictions\n",
    "test_analysis = df_features.iloc[X_test.index].copy()\n",
    "test_analysis['predicted'] = y_pred_test\n",
    "test_analysis['error'] = test_analysis['price_per_m2_kzt'] - test_analysis['predicted']\n",
    "test_analysis['abs_error'] = np.abs(test_analysis['error'])\n",
    "test_analysis['pct_error'] = test_analysis['abs_error'] / test_analysis['price_per_m2_kzt'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error by district\n",
    "district_error = test_analysis.groupby('district').agg({\n",
    "    'abs_error': 'mean',\n",
    "    'pct_error': 'mean',\n",
    "    'price_per_m2_kzt': 'count'\n",
    "}).rename(columns={'price_per_m2_kzt': 'count'})\n",
    "district_error = district_error.sort_values('abs_error')\n",
    "\n",
    "print(\"Error by District:\")\n",
    "print(district_error.round(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error by room count\n",
    "room_error = test_analysis.groupby('rooms').agg({\n",
    "    'abs_error': 'mean',\n",
    "    'pct_error': 'mean',\n",
    "    'price_per_m2_kzt': 'count'\n",
    "}).rename(columns={'price_per_m2_kzt': 'count'})\n",
    "\n",
    "print(\"\\nError by Room Count:\")\n",
    "print(room_error.round(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error by price segment\n",
    "test_analysis['price_segment'] = pd.cut(\n",
    "    test_analysis['price_per_m2_kzt'], \n",
    "    bins=[0, 400000, 600000, 800000, 1000000, 2000000],\n",
    "    labels=['<400k', '400-600k', '600-800k', '800k-1M', '>1M']\n",
    ")\n",
    "\n",
    "segment_error = test_analysis.groupby('price_segment').agg({\n",
    "    'abs_error': 'mean',\n",
    "    'pct_error': 'mean',\n",
    "    'price_per_m2_kzt': 'count'\n",
    "}).rename(columns={'price_per_m2_kzt': 'count'})\n",
    "\n",
    "print(\"\\nError by Price Segment:\")\n",
    "print(segment_error.round(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "\n",
    "# Save model\n",
    "joblib.dump(final_model, 'astana_price_model.joblib')\n",
    "print(\"Model saved to: astana_price_model.joblib\")\n",
    "\n",
    "# Save feature list\n",
    "with open('model_features.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'features': all_features,\n",
    "        'best_params': best_params,\n",
    "        'metrics': {\n",
    "            'test_mae': float(mean_absolute_error(y_test, y_pred_test)),\n",
    "            'test_r2': float(r2_score(y_test, y_pred_test)),\n",
    "            'test_mape': float(np.mean(np.abs((y_test - y_pred_test) / y_test)) * 100)\n",
    "        }\n",
    "    }, f, indent=2)\n",
    "print(\"Feature list saved to: model_features.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ASTANA REAL ESTATE PRICE PREDICTION - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDataset: {len(df):,} apartments\")\n",
    "print(f\"Features: {len(all_features)}\")\n",
    "print(f\"Train/Test split: 80/20\")\n",
    "print(f\"\\nBest Model: XGBoost (tuned with Optuna, 50 trials)\")\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"  MAE:  {mean_absolute_error(y_test, y_pred_test):,.0f} ₸/m²\")\n",
    "print(f\"  MAPE: {np.mean(np.abs((y_test - y_pred_test) / y_test)) * 100:.1f}%\")\n",
    "print(f\"  R²:   {r2_score(y_test, y_pred_test):.3f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  For an average apartment (60m², ~35M ₸):\")\n",
    "print(f\"  Expected prediction error: ~{mean_absolute_error(y_test, y_pred_test) * 60 / 1e6:.1f} million ₸\")\n",
    "print(f\"\\nTop 5 Most Important Features:\")\n",
    "for i, row in importance_df.head(5).iterrows():\n",
    "    print(f\"  {i+1}. {row['feature']} ({row['importance']*100:.1f}%)\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}